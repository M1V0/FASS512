[
  {
    "objectID": "Week10/Week10Data.html",
    "href": "Week10/Week10Data.html",
    "title": "Statistics for Social Scientists",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome\nWelcome to FASS512, Quantitative Research Methods for 2023/2024. In the sidebar you will see each week’s worksheets. This is a one-stop shop for all your worksheets, and each week the previous content will have answers incorporated into the content for easier revision.\nSee Moodle for course slides and recordings.\n\n\nHelpful links:\n\nInstalling R and RStudio\nThe Tidyverse Style guide\nHow to search for help on the internet\nAnything you think should be here? Let me know!\n\nContent written by Matthew Ivory, built on Patrick Rebuschat’s previous content; website developed and maintained by Matthew Ivory.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Worksheets/Worksheet_wk1.html",
    "href": "Worksheets/Worksheet_wk1.html",
    "title": "1. Introduction to quantitative research methods using R",
    "section": "",
    "text": "This week, we will do our first steps in R. Please work through the following handout at your own pace.\nThree important things to remember:"
  },
  {
    "objectID": "Worksheets/Worksheet_wk1.html#step-1-using-the-r-console",
    "href": "Worksheets/Worksheet_wk1.html#step-1-using-the-r-console",
    "title": "1. Introduction to quantitative research methods using R",
    "section": "Step 1: Using the R console",
    "text": "Step 1: Using the R console\nR is a command-based system. This means: You type commands (such as the ones highlighted below), R translates the commands into machine instructions, which your computer then executes.\nYou can type the R commands directly into the console. Or, as you will see later, you can also type commands into the script editor and run the sequence of commands as a batch or collection of lines.\nIn the next section, we will try out writing commands in the Console pane.\nCommands are typed at the command prompt &gt;. We then press Return (Mac) / Enter (Windows), and our command is executed. The output of the command, if there is any output, will be displayed on the next line(s)."
  },
  {
    "objectID": "Worksheets/Worksheet_wk1.html#step-2-using-r-as-a-calculator",
    "href": "Worksheets/Worksheet_wk1.html#step-2-using-r-as-a-calculator",
    "title": "1. Introduction to quantitative research methods using R",
    "section": "Step 2: Using R as a Calculator",
    "text": "Step 2: Using R as a Calculator\nMost textbooks recommend familiarizing yourself with R and RStudio by first using R as a calculator. Let’s try this out.\nFor example, if you type 10 + 10 in the command prompt and press Return (Mac) / Return (Mac) / Enter (Windows), the result will be displayed underneath:\n\n10 + 10\n\n[1] 20\n\n\nPlease try out the following commands.\nIn the task below, just type the commands in the shaded area, then press Return (Mac) / Enter (Windows).\nNote: You don’t have the leave spaces around the operators (+, -, *, /, etc.), but the lines are more readable if you do.\nSo it’s better if you get used to writing 10 + 10 rather than 10+10, even though the output is the same.\nThe exception are negative values.\nHere, it is recommended not to leave a space between the minus sign - and the value we are negating. So, we would write -3, not - 3, even though it’s the same.\nPlease try out all of the commands on your computer now.\n\nThis is how we do addition\n\n\n10 + 10\n\n\nSubtraction\n\n\n8 - 2\n\n\nMultiplication\n\n\n10 * 14\n\n\nDivision\n\n\n112/8\n\n\nWe can also use exponents, i.e. raising a number to the power of another number, e.g., 2^8, as in the example below\n\n\n2 ^ 8\n\n\nSquare root. This is done via a function called sqrt(). We will discuss functions later. For now, just add a numerical value in the parentheses of sqrt()\n\n\nsqrt(64)\n\n\nYou can also combine +, -, *, /, ^ operators in your commands. By default, the precedence order of operations will be ^ followed by * or /, followed by + or -, just like in a calculator.\n\n\n2+3-4/2\n\nor\n\n3+9/3*2^8\n\n\nBut: You can use brackets () to overcome the default order to operations: Test the effect of bracketing on the precedence order of operations in the two examples below.\n\n\n#example a\n2 + 3 - 4 / 2\n\n#example b\n(2 + 3 - 4) / 2\n\nor\n\n#example a\n3 + 9 / 3 * 2 ^ 8\n\n#Example b\n(3 + 9) / 3 * 2 ^ 8"
  },
  {
    "objectID": "Worksheets/Worksheet_wk1.html#step-3-history",
    "href": "Worksheets/Worksheet_wk1.html#step-3-history",
    "title": "1. Introduction to quantitative research methods using R",
    "section": "Step 3: History",
    "text": "Step 3: History\nIn the Console pane, have you noticed that pressing the up and down arrows does not allow you to go through the different lines (as would happen in a text file, e.g. Word)? Instead, when you’re at the command prompt &gt;, pressing the up and down arrows allows you to move through the history of executed commands. This can save you a lot of time if you want to re- run one of the previous commands.\nGo ahead and rerun the last line using this method, but change the last number to a 4 (instead of an 8)\nIn the Environment, History, etc. pane, you can use the History tab to see your entire command history. If you click on any line in the History tab, it will re-run the command. Again, very helpful as it saves you a lot of typing. Let’s try this out.\nGo into the History pane and find the line where you ran sqrt(64) and rerun it"
  },
  {
    "objectID": "Worksheets/Worksheet_wk1.html#step-4-incomplete-commands-and-escaping",
    "href": "Worksheets/Worksheet_wk1.html#step-4-incomplete-commands-and-escaping",
    "title": "1. Introduction to quantitative research methods using R",
    "section": "Step 4: Incomplete commands, and escaping",
    "text": "Step 4: Incomplete commands, and escaping\nWhat happens if you try to execute an incomplete command such as the one below?\n\n\n\n\n10 +\n\nYou will notice there is something missing (another number for the addition). Rather than giving us the result of the addition, R will just display a plus sign + in the console instead of the usual &gt;. This means that the command is incomplete. And: If you keep hitting Return (Mac) / Enter (Windows), you will just get more plus signs…\nYou can either complete the command on the next line (try adding a number to complete the addition), or you can just press Esc to exit the command"
  },
  {
    "objectID": "Worksheets/Worksheet_wk1.html#step-5-variables",
    "href": "Worksheets/Worksheet_wk1.html#step-5-variables",
    "title": "1. Introduction to quantitative research methods using R",
    "section": "Step 5: Variables",
    "text": "Step 5: Variables\nThe next important step is to learn how to create variables and assign values to variables. In general, the assignment rule is:\nname &lt;- expression/value\nExpression - An expression is any R code that returns some value. This can be a single number, the result of a calculation, or a complex statistical analysis.\nAssignment operator - The &lt;- is called the assignment operator. This assigns everything that is to its right (the expression) to the variable on its left. Rather than typing &lt; and the minus sign, you can also simply press the shortcut Option+- (Mac) or Alt+- (Windows).\nName - The name is simply the name of the variable.\n\nrun the following\n\n\nx &lt;- 4 * 8\n\nIt appears that nothing happened; after all, there seems to be no output in the command prompt. However, something did happen after we executed the command x &lt;- 4 * 8.\nYou will see this when you execute the following command.\n\nx\n\nAs you see, the previous command x &lt;- 4 * 8 assigned the multiplication 4 * 8 to the variable x.\nBy typing the name of the variable x in the command line and running it, the value of the variable will be displayed, in this case 32 (being the product of 4 * 8).\nThere is another way to confirm that you created a variable. If you check the Environment tab in the Environment, History, Connections, etc. pane at the top right of your RStudio window, you will now see that new variable listed.\nOnce you have created a variable, you can use it for other calculations just like you would with any other number\n\n\n\n\nx * 36\n\nWe can also assign any of these values to new variables.\n\n\n\n\ny&lt;-x*28\n\ny"
  },
  {
    "objectID": "Worksheets/Worksheet_wk1.html#step-6.-naming-your-variables",
    "href": "Worksheets/Worksheet_wk1.html#step-6.-naming-your-variables",
    "title": "1. Introduction to quantitative research methods using R",
    "section": "Step 6. Naming your variables",
    "text": "Step 6. Naming your variables\nThe name has to follow certain naming conventions. The variable name can consist of letters (upper or lower case), numbers, dots, and underscores. However, it must begin with a letter, or a dot that is not followed by a number. That is, a dot not followed by a number, OR a letter. If it is a dot and then number it will think of it as a decimal.\n\nWhich of the following would be okay?\n\n\ny157 &lt;- 2\nx_y_z &lt;- 2\nabc_123 &lt;- 2\n_x &lt;- 2\n.1px &lt;- 2\na_b_c &lt;- 2\nx-y-z &lt;- 2\n123 &lt;- 2\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nthese variable names would work:\n\ny157\nx_y_z\nabc_123\na_b_c\n\nthese will not:\n\n_x starts with an underscore\n.1px starts with a number\nx-y-z is trying to subtract z from y from x (it thinks you want to run a function)\n123 is just a number\n\n\n\n\nEven though you can use nonsense sequences such as the ones above, it is good practice to select variable names that are meaningful, short, without dots (use underscore _ instead), and ideally in lowercase characters. For example:\n\nage &lt;- 56\nincome &lt;- 101034\nis_married &lt;- TRUE\nyears_married &lt;- 27"
  },
  {
    "objectID": "Worksheets/Worksheet_wk1.html#step-7-data-types",
    "href": "Worksheets/Worksheet_wk1.html#step-7-data-types",
    "title": "1. Introduction to quantitative research methods using R",
    "section": "Step 7: Data Types",
    "text": "Step 7: Data Types\nData types are variables that refer to collections of values. There are different types of data types (e.g., lists, matrices, arrays), but we will focus on two particularly important ones: vectors and data frames.\n\nVectors\nVectors are one-dimensional sequences of values. They are very simple but fundamental data structures, as you will see below when we talk about data frames. For this reason, it’s worth learning about vectors, how to create and manipulate them. Below we will cover numeric and character vectors.\nYou can create vectors by using the c() function. The c stands for combine. All the items within brackets, separated by commas, will be assigned to the variable on the left of the assignment operator.\nIf you type the following command, you will create a vector with seven elements. This is called a numeric vector as the elements within the brackets are numbers.\n\n\n\n\nnumbers &lt;- c(2, 3, 5, 7, 11, 13, 17)\n\nWe can now use this vector to perform all kinds of operations. Try out the following, for example.\n\n\n\n\nnumbers + 1\n\nnumbers / 2\n\nnumbers^2\n\nNote how the operations are applied to each number in the vector.\nFor any vector (number sequence), we can also refer to individual numbers that form the vector. We do this by means of indexing operations []. For example, to get the first element of the vector, we can type the following. This will display the first element in the vector.\n\nnumbers[1]\n\nYou can also extract more than one element from the vector, and even specify the order. For example:\nNote the difference in the example above and the one below. Above, we just used [1] and that was fine, but because of how indexing works (and it gets more complex when handling different datatypes, such as 2-dimensional tables), we need to use the c() function to demonstrate what we are after.\n\nnumbers[c(4,2,1)]\n\nThis will retrieve the fourth element in the vector (the number 7), the first element (2) and the second (3).\n\n\n\n\n\n\nWhy is it different?\n\n\n\n\n\nWhat happens if you don’t use the c() function? Try it out and see what happens.\n\nnumbers[4,2,1]\n\nError in numbers[4, 2, 1]: incorrect number of dimensions\n\n\nYou get an error! Now, in this particular situation, the error message isn’t the most helpful, but it is important to get familiar with errors (you will be seeing a lot of them in the next 10 weeks!), and note that they are useful. R cannot carry out operations that don’t make sense, and as it happens, the above command is telling it to look for the item in three dimensions (imagine a cube made of vectors). When we get errors it is useful to check our code to see what might be wrong.\n\n\n\nOr you can refer to consecutive set of elements in the vector, such as the fourth to the seventh elements. For this, simply type:\n\nnumbers[4:7]\n\nWe can also retrieve all elements with the exception of one. For this, we use the minus sign to exclude the vector element that we want to exclude, as in the following example.\n\nnumbers [-1]\n\nFinally, we can also exclude a sequence of elements by adding the minus sign before the c() function. This will exclude from the output all elements that are specified within the brackets.\n\nnumbers [-c(1, 3, 5, 7)]\n\nThe numbers vector is a sequence of numbers. We can find out the type of vector by using the function class(). This will confirm that numbers is a numeric vector.\n\nclass(numbers)\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nthe class() command is really useful, and one worth remembering for future troubleshooting. Sometimes commands don’t run as you want them to, and checking the class is what you want is a great sanity check. It has saved me on many occasions.\n\n\n\nWe can also create vectors with elements that are character strings. Here, each element needs to be surrounded by quotation marks (single or double), as in the example below\n\ncolleges &lt;- c('bowland', 'cartmel', 'county', 'furness', 'fylde', 'graduate', 'grizedale', 'lonsdale', 'pendle')\n\ncolleges\n\nThis type of vector is character, how would you verify this?\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nclass(colleges)\n\n\n\n\nYou can index character vectors, just like numeric vectors.\n\ncolleges[3]\n\nBut you cannot perform arithmetic functions on character vectors, of course.\n\ncolleges*2\n\n\nCoercing vectors\nIn vectors, all of the elements must be of the same type. For example, you cannot have a vector that has both numbers and character strings as elements. If you try to create a vector with both numbers and character strings, then some of your elements will coerced into other types.\nIf you type the following, you will see that the attempt to create a mixed vector with character strings (bowland, cartmel, county, fylde) and numbers (1, 2, 5, 7, 8) converted the numbers into character strings, as evidenced by the quotation marks. You cannot perform calculations on these numbers as R interprets them as text strings.\n\nc('bowland', 'cartmel', 'county', 'furness', 'fylde', 1, 2, 5, 7, 8)\n\nFinally, you can also combine vectors using thec()function. For example, we can create a new vector called new_numbers by combining the original numbers vectors and adding the squares and cubes of numbers.\n\nnew_numbers &lt;- c(numbers, numbers ^ 2, numbers ^ 3)\n\nnew_numbers\n\nWe have now produced a new vector new_numbers with 21 elements. These don’t fit all in a line and so are wrapped over two lines. The first row displays elements 1 to 12, and the second row begins with element 13. Now you can also see the meaning of the [1] on the output. The [1] is just the index of the first element of the vector show on the corresponding line. [1] refers to the first element in our vector (the number 2), and [13] refers to the 13th element in our vector (169).\n\n\nNaming Vectors\nThe elements of a vector can be named, too. Each element in our vector can have a distinct label, which can be useful.\n\nages &lt;- c(bob = 27, bill = 34, charles = 76)\n\nages\n\nWe can now access the values of the vector by the label or by the index as before.\n\nages['bill']\n\nages[2]\n\nWe can also add names to existing vectors by using the names() function. In the following example, we first assign new values to the vector ages. This will delete the previous numbers and labels. We then assign names to this vector using the names() function.\n\nages &lt;- c(23, 54, 8)\nnames(ages) &lt;- c(\"michaela\", \"jane\", \"jacques\")\nages\n\n\n\nMissing Values\nLast but not least. Sometimes, we have missing values in our data. In R, missing values are denoted by NA. This is not treated as a character string but as a special symbol.\nYou can insert a placeholder for a missing value into a numeric or character vector by simply typing NA in the list of elements.\n\na&lt;-c(1,5,7,NA,11,14)\na\n\n\nb &lt;- c('michaela', 'bill', NA, 'jane')\nb\n\n\n\n\nDataframes\nData frames are probably the most important data structure in R. They are the default form for representing data sets for statistical analyses.\nData frames have a certain number of columns, and each column has the same number of rows. Each column is a vector, and so data frames are essentially collections of equal-length vectors. (This is also why we spent so much time on vectors above…)\nThere are two ways of creating data frames. We can create a data frame in R by importing a data file, usually in .csv or .xlsx format. (We will discuss how to import files next week.)\nOr we can use the data.frame() function to create a data frame from scratch, as in the following example.\n\ndata_df &lt;- data.frame(name = c('bill', 'jane', 'jacques'), age = c(23, 54, 8))\n\ndata_df\n\nAs you can see, we have now created a data frame with two columns (name, age) and three rows (displaying the name and age of each person, Bill, Jane and Jacques). The columns are our variables and the rows are our observations of these variables.\nWe can refer to specific elements of the data frame by using indices. In the example below, there are two elements within the index [ ], one for the rows, one for the columns. These are separated by a comma [ , ].\nFor example, to refer to the element that is in the second row, first column, you would type the following.\n\ndata_df[2 ,1]\n\nYou can also retrieve multiple elements. One way of doing this is by leaving one of the indices blank.\nIf you leave the first index blank (e.g., [ , 1]), then you are telling R that you want the information from all the rows. In the example below, you retrieve the information that is in all the rows that are in column 1.\n\ndata_df[ ,1]\n\nIn contrast, if you leave the second index blank ([2, ], you will retrieve the information found in the second row across the two columns.\n\ndata_df[2 , ]\n\nWe can also be more specific. For example, you can refer to the first and third row of the second column, we would type:\n\ndata_df[c(1, 3), 2]\n\nOn the other hand, we can also refer to a column by name. To do this, we need to use the $ notation.\n\ndata_df$name\n\ndata_df$age\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nDid you notice that RStudio will automatically suggest the variable names after you typed the $? The code completion feature in RStudio makes writing and executing code much easier!\nIt will do this for pretty much everything that is either a function, a variable, or even an argument (we come to those later), provided that you have typed at least three characters (or press Tab to get there faster)"
  },
  {
    "objectID": "Worksheets/Worksheet_wk1.html#step-8-functions",
    "href": "Worksheets/Worksheet_wk1.html#step-8-functions",
    "title": "1. Introduction to quantitative research methods using R",
    "section": "Step 8: Functions",
    "text": "Step 8: Functions\nWhile data structures hold data in R, functions are used to do things with the data. You have already encountered a few functions above, namely sqrt(), c(), and data.frame().\nAcross all R packages and R’s standard library, there are tens of thousands of functions available for you to use. However, most of our analyses in this course will require only a relatively small number of functions.\nBelow is a general introduction to functions; we will cover functions in more detail as we progress through this course.\nFunctions tend to have the following structure:\nfunction(argument 1, argument 2, argument 3, ...)\nWe can think of functions as actions and arguments as the inputs, i.e. something the functions act on. Most functions require at least one argument. If they have more than one argument, these are separated by commas as in the example above.\nFor example, see what happens when you type the following two commands.\n\nsqrt()\nsqrt(4)\n\nThe function sqrt() requires an argument; if you fail to supply an argument you will get an error message (Error in sqrt()…)\nHere are another few functions that are helpful for our analyses.\nWe can count the number of elements in a given vector by using the length() function.\n\nlength(new_numbers)\n\nWe can calculate sum, mean, median, and standard deviation as follows. (More on this in future sessions when we discuss exploratory data analysis.)\n\nsum(new_numbers)\n\nmean(new_numbers)\n\nmedian(new_numbers)\n\nsd(new_numbers)\n\nYou can also nest functions inside each other, such as:\n\nround(sqrt(mean(new_numbers)))\n\nHere, we use three functions, each nested within the other. In the example, we first calculated the mean of the vector new_numbers (460.381), then the square root of this value and finally rounded it. We could have done the same calculation in three steps, as in the example below, but nesting the functions in a single command allows us to be more efficient.\nNow, I don’t like nesting functions and neither should you. Next session I will cover this more clearly, and introduce you to a set of functions and stylistic choices that are the gold standard in psychology and have a huge user base (meaning there’s plenty of help out there). For now, just be in awe of the complexity of carrying out multiple functions in R.\n\nOptional arguments\nIn some cases, functions can also take an additional argument. A good example is the mean() function, which can an additional argument called trim. If we add the trim argument to the mean function, the command will first remove a certain proportion of the extreme values of the vector and then calculate the mean. This is very useful when we are dealing with outliers in our data, for example. (We will talk more about outliers in future sessions.)\nIn order to trim observations, we need to specify a value between 0 to 0.5. This will trim the highest and the lowest values before calculating the mean. Naturally, a trim value of 0 means you’re not trimming anything, so the value assigned to trim should be greater than 0. For example, a value of 0.1 means you’re trimming the 10% highest and lowest observations, 0.2 means you’re trimming the 20% highest and lowest, and so forth.\n\nmean(new_numbers)\n\nmean(new_numbers, trim=0.0)\n\nmean(new_numbers, trim=0.1)\n\nmean(new_numbers, trim=0.2)\n\nmean(new_numbers, trim=0.3)\n\nmean(new_numbers, trim=0.4)\n\nmean(new_numbers, trim=0.5)"
  },
  {
    "objectID": "Worksheets/Worksheet_wk1.html#step-9-help-pages",
    "href": "Worksheets/Worksheet_wk1.html#step-9-help-pages",
    "title": "1. Introduction to quantitative research methods using R",
    "section": "Step 9: Help Pages",
    "text": "Step 9: Help Pages\nRStudio has very helpful pages for the available functions. This is useful when you’re not sure if a function requires an argument, or if you’re in doubt about the use of arguments such as trim.\nYou can access the help page for a given function in different ways.\nThe most efficient one is by typing a question mark and the function name in the command line. If you now press Return (Mac) / Enter (Windows), this command will also open the help page for the function. Alternatively, you can use thehelp()function in the command line, as below.\n\n?mean()\n\nhelp('mean')\n\nThen there is my preferred option (and again, when we start using the script pane, I will make this clear):\nYou can also go to the search line of the Help tab in the Files, Plots, Packages, Help pane (bottom right of the screen) and type the name of the function there (mean). There is a useful shortcut to search R Help, namely Ctrl+Option+F1 (Mac) and Ctrl+Alt+F1 (Windows).\nIn each of the cases above, RStudio will open the help page for the function in question in the Help tab."
  },
  {
    "objectID": "Worksheets/Worksheet_wk1.html#take-home-task",
    "href": "Worksheets/Worksheet_wk1.html#take-home-task",
    "title": "1. Introduction to quantitative research methods using R",
    "section": "Take home task",
    "text": "Take home task\nThe following table displays the scores of students in two foreign language exams, one administered at the beginning of term, the other at the end of term.\nCreate a data frame called language_exams with the information provided in the table, then answer the questions below using R. To save you the headache of tediously typing it all out, you can highlight and paste the code below as-is into your own script.\n\nlanguage_exams &lt;- data.frame(\n  student_id = c('Elin', 'Spencer', 'Crystal', 'Arun', 'Lina', 'Maximilian', 'Leyton', 'Alexandra', 'Valentina', 'Lola', 'Garfield', 'Lucy', 'Shania', 'Arnold', 'Julie', 'Michaela', 'Nicholas'), \n  exam_1 = c(93, 89, 75, 52, 34, 50, 46, 62, 84, 68, 74, 51, 84, 34, 57, 25, 72), \n  exam_2 = c(98, 96, 94, 65, 50, 68, 58, 77, 95, 86, 89, 70, 90, 50, 67, 37, 90))\n\n\nWhat are the mean scores for exam 1 and exam 2?\n\n\nWhat is the difference between the two means?\n\n\nWhat are the mean scores for the two exams if you remove extreme values (the top and bottom 20%) from each?\n\n\nBased on the previous step (with outliers removed): What is the difference between the two means now? Please round the value before reporting the result.\n\n\nCan you do steps 3 and 4 in a single command?"
  },
  {
    "objectID": "Worksheets/Worksheet_wk2.html",
    "href": "Worksheets/Worksheet_wk2.html",
    "title": "2. Data management and data wrangling",
    "section": "",
    "text": "Three important things to remember:"
  },
  {
    "objectID": "Worksheets/Worksheet_wk2.html#step-0-installing-tidyverse",
    "href": "Worksheets/Worksheet_wk2.html#step-0-installing-tidyverse",
    "title": "2. Data management and data wrangling",
    "section": "Step 0: Installing tidyverse",
    "text": "Step 0: Installing tidyverse\nI mentioned tidyverse in the lecture, and now we will intall and load it, before using it (mainly for pipes!)\nAs a one-off (on a per machine basis) the first command only needs to be run when we first want a package. As noted before, there are thousands of functions available to R, having them all pre-packaged would break your computer and you don’t need every single one.\n\ninstall.packages(\"tidyverse\")\n\nThere is little to know about this at this stage, as the function does a lot of the legwork for us. It looks on the CRAN (The Comprehensive R Archive Network) which contains the R approved packages. It downloads it so you can use all the functions contained in a spacific package.\nAs stated before, tidyverse is a collection of packages, and we will need to understand that later on, but for now: no need.\nNow to load the package so we can use the functions:\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.4     ✔ purrr   1.0.2\n✔ tibble  3.2.1     ✔ dplyr   1.1.2\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nAs above, you will get a bunch of messages in the console, we can reasonably ignore these for now.\n\n\n\n\n\n\nNote\n\n\n\nAs a general point, we would run install.packages() in the console, and place library() at the top of a script. The next section should make this a bit clearer as to the difference"
  },
  {
    "objectID": "Worksheets/Worksheet_wk2.html#step-1-scripts",
    "href": "Worksheets/Worksheet_wk2.html#step-1-scripts",
    "title": "2. Data management and data wrangling",
    "section": "Step 1: Scripts",
    "text": "Step 1: Scripts\nA script is essentially a sequence of commands that we want R to execute. As Winter (2019) points out, we can think of our R script as the recipe and the R console as the kitchen that cooks according to this recipe. Let’s try out the script editor and write our first script. Typing commands in the console is good for one off commands (maybe to check the class() or to install.packages()), but the script is better for keeping the steps in order.\nWhen working in R, try to work as much as possible in the script. This will be a summary of all of your analyses, which can then be shared with other researchers, together with your data. This way, others can reproduce your analyses.\nThus far, you have typed your command lines in the console. This was useful to illustrate the functioning of our R, but in most of your analyses you won’t type much in the console. Instead, we will use the script editor.\nThe script editor is the pane on the top left of your window. If you don’t see it, you need to open a new script first. For this, press Cmd+Shift+N (Mac) or Ctrl+Shift+N (Windows). Alternatively, in the menu, click File &gt; New File &gt; RScript.)\nIn the script editor (not the console), type the following command in line 1 press Return (Mac) / Enter (Windows).\n\n2 + 3\n\nAs you can see, nothing happened. There is no output in the Console pane; the cursor just moved to the next line in the script editor (line 2). This is because you did not execute the script.\nTo execute a command in the script editor, you need to place your cursor anywhere on the line you wish to execute and then click the Run icon in the Script editor pane. If you do this, then the following output will appear in your Console.\nYou can also run the current command line or selection in the script by pressing Cmd+Return (Mac) or Ctrl+Enter (Windows). This will also send your command from the script editor to the console. (I suggest using the shortcut, it’s much more efficient.)\nIn the script, you can have as many lines of code as you wish. For example, you can add the following three commands to your script.\n\nscores &lt;- c(145, 234, 653, 876, 456) \n\nmean(scores)\n\nsd(scores)\n\nTo execute each one separately, just go to the line in question and click the Run icon or, even better, press the keyboard shortcut.\nYou can also run multiple commands in one go. For this, you either highlight several lines and then press the Run icon (or keyboard shortcut). Try it with the above three lines.\nTo execute all commands in the script, you click the Source icon (next to the Run icon) in the Script editor pane. Or just use the shortcut Cmd+Option+R (Mac) or Ctrl+Alt+R (Windows).\n\nMultiline commands\nUsing the script editor is particularly useful when we write long and complex commands. The example below illustrates this nicely.\nThis is a fairly long command, written in the console in one line.\n\ndf &lt;- data.frame(name = c('jane', 'michaela', 'laurel', 'jaques'), age = c(23, 25, 46, 19), occupation = c('doctor', 'director', 'student', 'spy'))\n\nbut in a multiline format:\n\ndf &lt;- data.frame(name = c('jane', 'michaela', 'laurel', 'jaques'), \n                 age = c(23, 25, 46, 19), \n                 occupation = c('doctor', 'director', 'student', 'spy'))\n\nNote the indentations, this is done automatically by RStudio as it recognises what is grouped according to parentheses.\n\n\nComments\nAn important feature of R (and other programming languages) is the option to write comments in the code files. Comments are notes, written around the code, that are ignored when the script is executed. In R, anything followed by the # symbol on any line is treated as a comment. This means that a line starting with # is ignored when the code is being run. And if we place a # at any point in a line, anything after the hash tag is also ignored. The following code illustrates this.\nComments are really useful for writing explanatory notes to ourselves or others.\n\n# Here is data frame with three variables.\n# The variables refer to the names, ages, and occupations of the participants.\ndf &lt;- data.frame(name = c('jane', 'michaela', 'laurel', 'jaques'), \n                 age = c(23, 25, 46, 19),\n                 occupation = c('doctor', 'director', 'student', 'spy'))\n\nor\n\n2 + 3 #This is addition in R.\n\n\n\nCode sections\nTo make your script even clearer, you can use code sections. These divide up your script into sections as in the example below. To create a code section, go the line in the script editor where you would like to create the new section, then press Cmd+Shift+R (Mac) or Ctrl+Shift+R (Windows). Alternatively, in the Menu, select Code &gt; Insert Section.\nThe lines with the many hypens create the sections\n\n# Create vectors ---------------------------------------------------\n\nscores_test1 &lt;- c(1, 5, 6, 8, 10) # These are the scores on the pre-test.\nscores_test2 &lt;- c(25, 23, 52, 63) # These are the scores on the post-test.\n\n# A few calculations -----------------------------------------------\n\nmean_test1 &lt;- mean(scores_test1)\nmean_test2 &lt;- mean(scores_test2)\n\nround(mean_test1 - mean_test2) # The difference between pre and post-tests.\n\nOnce you have created a section, you can ask R to run only the code in a specific region. This is because R recognizes script sections as distinct regions of code.\nTo run the code in a specific section, first go to the section in question (e.g., the section called # A few calculations ————) and then either press Cmd+Option+T (Mac) or Ctrl+Alt+T (Windows). You can also use the menu, Code &gt; Run Region &gt; Run Section. Have a go to see if this works out well.\n\n\nSaving scripts\nFinally, you can also save your script. To do this, just click the Save icon in the Script editor pane or press Cmd+S (Mac) or Ctrl+S (Windows). The script can be named anything, but it is often recommended to use lowercase letters, numbers and underscores only. (That is, no spaces, hyphens, dots, etc.)\nThe script is saved in the .R format in your directory. If you later double click it, the file will open in RStudio by default, but you can also view and edit the file in Word and similar programs."
  },
  {
    "objectID": "Worksheets/Worksheet_wk2.html#step-2-a-bit-more-on-packages",
    "href": "Worksheets/Worksheet_wk2.html#step-2-a-bit-more-on-packages",
    "title": "2. Data management and data wrangling",
    "section": "Step 2: A bit more on packages",
    "text": "Step 2: A bit more on packages\nIt’s important to acknowledge the important work done by the developers who make R packages available for free and open source. When you use a package for your analyses (e.g., tidyverse or lme4), you should acknowledge their work by citing them in your output (dissertation, presentation, articles, etc.). You can find the reference for each package via the citation() function, as in the examples below.\n\ncitation(\"tidyverse\")\n\ncitation(\"lme4\")\n\nYou can also install packages by using the Packages tab in the Files, Plots, Packages, etc. pane. As you see in the figure below, the base package is already installed. You can install more packages by scrolling through the list (or using the search option to narrow down the choices) and then selecting the tick box to the left of the package. If you do this, you will see that the click will run the install.packages() command in the console.\nAs I mentioned above, run install.packages() in the console as a one-off command, you do not need to run this every time you want to use a package. Everytime we want to use a package in a given session, we need to tell R to load it up, which is why we put library() at the top of the script, so we can use the functions."
  },
  {
    "objectID": "Worksheets/Worksheet_wk2.html#step-3-working-directories-and-clean-workspaces",
    "href": "Worksheets/Worksheet_wk2.html#step-3-working-directories-and-clean-workspaces",
    "title": "2. Data management and data wrangling",
    "section": "Step 3: Working directories and clean workspaces",
    "text": "Step 3: Working directories and clean workspaces\nEvery R session has a working directory. This is essentially the directory or folder from which files are read and to which files are written.\nYou can find out your working directory by typing the following command. Your output will obviously look different from the one below, which refers to my machine\n\ngetwd()\n\n[1] \"/Users/ivorym/Documents/PhD/Teaching/23_24/FASS512/Worksheets\"\n\n\nYou can also use a command to list the content in the working directory. (Alternatively, you can see your direct by using the Files tab in the Files, Packages, Plot, etc. pane.)\n\nlist.files()\n\nI suggest you create a new working directory on your computer desktop and then use it for the entire course. Important files related to your R tasks (scripts, data, etc.) should later be downloaded to this folder.\nThe first step is for you to create a folder called FASS512 (or similar) in a sensible place on your computer. You can do this by going to the Files tab (in the Files, Packages, etc. pane) and clicking the “Create a new folder” icon. Place each weekly set of weekly files in their own weekly folders.\nOnce you have created the “statistics” folder on the desktop, go to the menu to set the default working directory to the new “statistics” folder. The easiest way is to go to the menu, RStudio &gt; Preferences. This should call up the following window.\nIn the window, click the Browse button and set the default working directory to the “statistics” folder in the desktop."
  },
  {
    "objectID": "Worksheets/Worksheet_wk2.html#step-4-loading-data",
    "href": "Worksheets/Worksheet_wk2.html#step-4-loading-data",
    "title": "2. Data management and data wrangling",
    "section": "Step 4: Loading data",
    "text": "Step 4: Loading data\nWhen we are dealing with data in our analyses, we usually begin by importing a data file. R allows you to important data files in many different formats, but the most likely ones are .csv and .xlsx.\nI have uploaded several data files to our Moodle page. Please go to folder called “Data sets to download for this session” in the section for today’s session, then download the files in the folder and place them in your working directory (the statistics folder you just created). The files are from Winter (2019) and Fogarty (2019).\nLet’s try out loading data files. In the examples below, you will import three types of files: .csv, .txt, and .xlsx. Remember: You need to download the data files from our Moodle page and place them in our working directory first. Otherwise, you cannot import the files from our directory into R.\n\nCSV\nWe can use the read_csv() function from dplyr (part of the tidyverse) to load data that is in .csv format. The command below will load the data set (‘nettle_1999_climate.csv’) and create a new label for this data set (languages). There exists a read.csv() function in base, but it is slower and not as ‘smart’ as read_csv().\n\nlanguages &lt;- read.csv('nettle_1999_climate.csv')\n\nAlternatively, you can load data files by clicking File &gt; Import Dataset &gt; From Text (readr). In the dialogue window, then click browse and select the file nettle_1999_climate.csv. You can change the name of the data set in the text box at the bottom left, below Import Options, where it says Name.\n\n\n\n\n\n\nNote\n\n\n\nI am giving you these alternative GUI-based methods for carrying out the same steps as what is written in the script. I offer these to highlight how things can be done in many ways, but preferably you will use the script for pretty much everything. This creates a record of the commands needed to reproduce your analysis, which is better for future researchers (which includes you in a week’s time)\n\n\n\n\nTXT\nThe data file you just imported is in the .csv format. You can important data from files in other formats, too. If the data is in .txt format, you can simply use the following command.\n\ntext_file &lt;- read_table('example_file.txt') #(Note: Ignore warning message in the console.)\n\nThe command creates a new data set called text_file.\n\n\nxlsx\nIf the data is an Excel spreadsheet (e.g., .xlsx format), you can proceed as follows. Ideally it shouldn’t be, as csv are a universal file format that can be read across many machines. As a general rule, it is important to use these universal filetypes (csv, txt, pdf, html…) for better reproducibility and data management (Towse et al., 2021)1\n\nlibrary(readxl) #you may need to run install.packages(\"readxl\") first\n\nspreadsheet_exl &lt;- read_excel('simd.xlsx', sheet = 'simd')\n\nFirst, you need to install the readxl package. Then, you create a new data set called spreadsheet_exl by using the read_excel() function.\nNote: Since spreadsheets have multiple sheets, you need to specify the name of the sheet you would like to import by using the sheet argument. In our case, the sheet is called simd, hence sheet = ‘simd’.\nRStudio can handle many other file extensions to import datasets. You can find out information on how to import other file types by using the R help function (or by searching on Google)."
  },
  {
    "objectID": "Worksheets/Worksheet_wk2.html#step-5-examining-datasets",
    "href": "Worksheets/Worksheet_wk2.html#step-5-examining-datasets",
    "title": "2. Data management and data wrangling",
    "section": "Step 5: Examining datasets",
    "text": "Step 5: Examining datasets\nIf you have followed the steps above, you will have imported three data sets, languages, spreadsheet_exl, and text_file. You can now start exploring the data. We will focus on languages as an example.\nEvery time you import data, it’s good to check the content, just to make sure you imported the correct file.\nThe easiest way to do this is by using the View() function. This allows you to inspect the data set in the script editor. Note: The function requires a capital V. If you have tidyverse loaded, which we do, then there is a view() function as well. These are functionally equivalent. Use whichever, but View() will always work\nIf you run the command below, you will see that this shows the data (a table) in a tab of the script editor. It will also be displayed in the console.\n\n\n\n\n\n\nNote\n\n\n\nRemember what I have said previously about some content being better off in the console rather than the script? This is another example of what to put in the console instead (like class() or install.packages().\nWhy? Great question, because it’s a one-off command that we don’t need in our script. It’s a sanity check, like class(), and it doesn’t add anything of value to the script. The script should be the minimum series of commands that are required to go from one stage to another. Taking a visual look at a dataframe is superfluous to the actual analysis\n\n\n\nView(spreadsheet_exl) \nView(languages)\n\nYou can also inspect your data by visiting the Environment tab in the Environment, History, Connections, etc. pane. As you can see in the figure below, this will tell you thatlanguageshas 74 observations (rows) and five variables (columns).\nIf you would like to examine variables, you can start by using the str() function (str for structure), as in the example below.\n\nstr(languages)\n\nAs you can see above, the str() function will tell you many useful things about your dataset. For example, it will reveal the number of observations (rows, 74) and variables (columns, 5), and then list the variables (Country, Population, Area, MGS, Langs). For each variable, it will also indicate the variable type (chr = character strings, num = numeric, intd = integer). The str() function will also display the first observations of each variable (Algeria, Angola, Australia, Bangladesh, etc.).\nYou can also check the names of variables separately by using the names() function, or check the variable type by checking the class() function, but it’s easier to just use the str() function as in the example above.\nIf you prefer, you can restrict your inspection of to the first or final rows of the data set. You can do this by using the head() and tail() function. This is helpful if your tables has lots of rows. It complements str() as it shows you a sample of the actual data, not just the structure.\n\nhead(languages) #default is six rows to display\ntail(languages, n = 5) #show last five rows\n\nHow could you show the first 10 rows?\nThere is also a very helpful function called summary(). As you can see in the example below, this function will provide you with summary information for each of your variables.\nFor numeric/integer variables such as Populations, Area, MGS, and Langs, this command will calculate the minimum and maximum values, quartiles, median and mean. (We will discuss summary statistics in more detail later.)\nFor character variables, as in Country, the command will simply provide you with the number of observations (length) for this variable.\n\nsummary(languages)\n\nIn large datasets, you might want to examine only a specific variable. You can do this by using the $ as an index. For example, if you would just like to examine the variable Population in the languages dataset, you could proceed as follows.\n\nstr(languages)\n\nstr(languages$Population)\n\nclass(languages$Population)\n\nhead(languages$Population)\n\ntail(languages$Population)\n\nsummary(languages$Population)\n\nWhich of the above six commands are best placed in the script or console?\n\n\n\n\n\n\nNote\n\n\n\n\n\nUltimately, there is no right or wrong answer. Personally,\nstr() belongs in the console because it should just be a quick check that it is the expected shape. It could go in the script if it was part of a more formal test. A sanity check is something that makes you go “oh, I should just make sure”, whereas a test is more in line with thoughts of “if it isn’t have an identical shape to dataframe2, none of this works” - a nuanced difference that we may perhaps explore in later sessions.\nclass() goes in the console - it is very much a sanity check. If it transpires the class isn’t what you wanted, we can coerce them into different classes, which we would include as a step in the script, but we don’t need to run the check everytime in the script if we are just going to coerce it anyway…\nhead() and tails() depends. If you’re just having a little look, then console. If it is something you are then using in the analysis, the script. Most likely the console though. If you can exit RStudio and reopen the script and it runs without errors, then it’s fine to leave in the console. If it fails, maybe you need things in the script?\nsummary() is one I usually keep in the script - particularly if I am reporting the summary of statistics (see a later session) because it is meaningful content that I need."
  },
  {
    "objectID": "Worksheets/Worksheet_wk2.html#step-6-closing-your-r-session",
    "href": "Worksheets/Worksheet_wk2.html#step-6-closing-your-r-session",
    "title": "2. Data management and data wrangling",
    "section": "Step 6: Closing your R session",
    "text": "Step 6: Closing your R session\nThe last step is to close your R session. When you quit RStudio, a prompt will ask whether you want to save the content of your workspace. It is better to NOT save the workspace. When you start RStudio again, you will have a clean workspace. You then just re-run your scripts.\nIf you have written your scripts well, upon re-open, you should be able to produce the exact same steps without error and without odd additional windows opening (because we put View() in a script…).\nSo, I would save R scripts (especially if these are very long and are relevant to your analyses), but I would not the workspace contents."
  },
  {
    "objectID": "Worksheets/Worksheet_wk2.html#take-home-task",
    "href": "Worksheets/Worksheet_wk2.html#take-home-task",
    "title": "2. Data management and data wrangling",
    "section": "Take home task",
    "text": "Take home task\nTo complete this homework task, you will need to download the language_exams data file from our Moodle page into your working directory.\nIn the file, you will find the (fictional) scores and ages of 475 students who took an intermediate Portuguese language course at university. Students were tested three times: first in September to check their Portuguese proficiency at the beginning of the course, then again in January as part of their mid-term examination, and finally in June as part of their final examination. On each occasion, students had to complete three subtests to respectively assess their Portuguese vocabulary, grammar and pronunciation. The scores for exams 1, 2 and 3 are composite scores, i.e. each combines the results of the three subtests.\nYour task is to run a basic analysis of the exam data using an R script.\nIn your script, please include all the steps, including the command that loaded the data.\nPlease also include sections to make your script very clear, as well as comments.\n\nHow many observations and columns does the datafile contain?\n\n\nRun commands to display the first and the last five lines of the table.\n\n\nWhat is the average age of participants? Report this as a whole number\n\n\nWhat type of variable is student_id?\n\n\nWhat is the rounded mean score on exam 3 to 2 decimal places?\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nNot sure how? Type ?round() into the console and read the help page. Specifically look under the Arguments section and the examples (the second to last is the best one)\n\n\n\n\nWhat is the difference between the mean scores on exams1 and 2?\n\nPlease save the script to discuss at the next session."
  },
  {
    "objectID": "Worksheets/Worksheet_wk2.html#footnotes",
    "href": "Worksheets/Worksheet_wk2.html#footnotes",
    "title": "2. Data management and data wrangling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTowse, A. S., Ellis, D. A., & Towse, J. (2021). Making data meaningful: Guidelines for good quality open data. The Journal of Social Psychology, 161(4), 395–402. https://doi.org/10.1080/00224545.2021.1938811↩︎"
  },
  {
    "objectID": "Worksheets/Worksheet_wk7.html",
    "href": "Worksheets/Worksheet_wk7.html",
    "title": "7. Tests for discrete variables",
    "section": "",
    "text": "Three important things to remember:"
  },
  {
    "objectID": "Worksheets/Worksheet_wk7.html#task-0-setting-up-our-environment",
    "href": "Worksheets/Worksheet_wk7.html#task-0-setting-up-our-environment",
    "title": "7. Tests for discrete variables",
    "section": "Task 0: Setting up our environment",
    "text": "Task 0: Setting up our environment\nYou should be able to do all these things, if not - check back on previous week’s content as a reminder\n\nCreate a new script and call it Week 7.\nLoad in the tidyverse library at the top of the script"
  },
  {
    "objectID": "Worksheets/Worksheet_wk7.html#task-1-chi-squared-and-cramers-v",
    "href": "Worksheets/Worksheet_wk7.html#task-1-chi-squared-and-cramers-v",
    "title": "7. Tests for discrete variables",
    "section": "Task 1: Chi-squared and Cramer’s V",
    "text": "Task 1: Chi-squared and Cramer’s V\nLet’s now have a look at running Chi-squared and Cramer’s V tests in R. download this week’s data moodle. Read titanic.csv into an object called “titanic”. It is a list of all the passengers aboard the titanic and their survival outcome.\nCan you remember the code you would need to read the csv file TitanicSurvival.csv and assign it to an object called titanic?\n\n\n\n\n\n\nNeed a reminder?\n\n\n\n\n\n\ntitanic &lt;- read_csv(\"titanic.csv\")\n\nRows: 1309 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): name, survived, sex, passengerClass\ndbl (1): age\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nOnce we’ve read that in, we need to do some wrangling to get the data into a usable format. Below I have given you the code, but can you identify what each part of the code does? Use the ?group_by notation in the console to get the help pages, or google the functions.\n\ntitanic_summary &lt;- titanic |&gt; group_by(survived, passengerClass) |&gt; \n  summarise(count = n()) |&gt;\n  ungroup() \n\n`summarise()` has grouped output by 'survived'. You can override using the\n`.groups` argument.\n\n\nLet us view the output - what do we have in this tibble?\n\ntitanic_summary\n\n# A tibble: 6 × 3\n  survived passengerClass count\n  &lt;chr&gt;    &lt;chr&gt;          &lt;int&gt;\n1 no       1st              123\n2 no       2nd              158\n3 no       3rd              528\n4 yes      1st              200\n5 yes      2nd              119\n6 yes      3rd              181\n\n\nThe summary table can be useful to look at or use as a report table.\nMake a bar graph to count the numbers of survived and died by class. We will be using ggplot() for which it does a lot of the hard work of counting occurrences etc. so if we want a plot with passenger class on the x-axis and the number of people who survived or died in each class, we need to specify this in the code.\nAs is, the code below won’t run because the aes() section is empty. We need to specify the x-axis (class) as well as the fill of the bars (survival). Replace the NAs with the right variable names\n\ntitanic |&gt; \n  ggplot(aes(NA, fill = NA)) +\n  geom_bar(position = \"dodge\")\n\nNow let’s see if there is a significant relation between class and survival using Chi-squared:\nWhat are the two variables we are interested in?\nWhat is the Dependent Variable? passengerClassnamesexagesurvived\nWhat is the Independent Variable? passengerClassnamesexagesurvived\nGreat job! Now we can use the function chisq.test that takes two arguments, x and y where these are the columns we want to test. We need to use the dataframe$column format for this test. Can you complete the code for this?\n\nchisq.test(x = , y = )\n\n\n\n\n\n\n\nNeed the answer?\n\n\n\n\n\n\nchisq.test(x = titanic$passengerClass, y = titanic$survived)\n\n\n\n\nThe results give the chi-squared value, the number of degrees of freedom, and the p-value. P = 2.2e-16 means p = .0000000000000022. That’s highly significant!\nThat means the observations are divided across the categories in a way that is very unlikely to be due to chance (for this number (P = 2.2e-16), it means there’s a 2 in a quadrillion chance that titanic survival was not related to class). In a report, you would write: \\(\\chi^2\\)(2, 1309) = 127.86, p &lt; .001.\nNow, let’s compute Cramer’s V. First, we need to make sure we have the package lsr. You might need to install this first - I’m sure you remember how. If not, look back at previous worksheets.\n\nlibrary(lsr)\n\nThen run the test:\n\ncramersV(x = titanic$passengerClass, y = titanic$survived)\n\n\nRunning the analysis for other variables\nIn our data, we have five variables in total, we have the dependent variable set as survived, but what else can we apply the \\(\\chi^2\\) test to? Out of the remaining variables of name, sex, and age, can you pick the one that is suitable for this analysis? That is, it is in the right data format\nThe suitable variable is: namesexage\nNow, using the code we have used above and should be neatly set up in your script, reproduce the steps to test this variable and make inferences about its effect on survival rates.\nThe last message that was relayed from Royal Mail Steamer Titanic was - “Sinking by the head. Have cleared boats and filled them with women and children.”\nSuppose we are wanting to investigate whether it really was “women and children first!” during the sinking of the Titanic. If this is truly what was called out, what might we hypothesise about the survival rates of the passengers based upon their recorded sex? Can you construct a directional hypothesis that includes both the IV, DV, and expected direction?\n\n\n\n\n\n\nMy example\n\n\n\n\n\nMale passengers were less likely to survive the sinking of the Titanic than Female passengers.\nWe have our IV - male/female\nWe have our DV - survival\nWe have everything here. It is testable with our data, and it clearly states our intentions\n\n\n\n\nWe want to first visualise the data, so make a bar plot\nWe then want to carry out the chi squared test\nThen finally Cramer’s V\nThen to test that you got everything right, can you fill in the blanks below with the correct information? Round all the values to 2dp EXCEPT for p-values, which are reported to 3 (unless it is less than .001 in which case we write “p &lt; .001”. We do not include the leading zero, so it is .001 not 0.001\n\nTo test the hypothesis “Male passengers were less likely to survive the sinking of the Titanic than Female passengers”, an examination of the deaths was tested between the male/female categorisation of the passengers. A significantnon-significant effect was detected, \\(\\chi^2\\)(2, 1309) = , p =&lt; , V = .\nSo what can we say about our hypothesis? Do we think that there may be some credibility to the notion that men stayed behind on the Titanic? Yes, I think we can say with some statistical certainty that the observed number of men who died compared to women upon the Titanic was significantly different enough to suggest the claims of “Women and Children first!” is credible.\nPause for a moment, and reflect on what you’ve just done (twice). We had some data, we had an idea that we turned into a hypothesis and from which we conducted a statistical analysis and detected a significant effect on class and sex upon whether Titanic passengers died. No longer do these questions need to keep you awake at night, you can sleep easy."
  },
  {
    "objectID": "Worksheets/Worksheet_wk7.html#task-2-repetition-repetition-repetition",
    "href": "Worksheets/Worksheet_wk7.html#task-2-repetition-repetition-repetition",
    "title": "7. Tests for discrete variables",
    "section": "Task 2: Repetition, repetition, repetition",
    "text": "Task 2: Repetition, repetition, repetition\nThis section is far more bare bones, chiefly because all the content has been listed out for you above, we’re just applying it to different contexts.\nRead in the csv file named 2023-12-lancashire-stop-and-search_simple.csv. This data is a simplified version of the data downloaded from https://data.police.uk/data/ for December 2023 for the Lancashire Constabulary including all three data sets. You can download it yourself with different forces or dates, just know that your values will be different to any I publish. I have then applied the following code to the raw data, just to keep it a bit more simplistic for our purposes. I show you the code for transparency in the accordion below, but if you are using the simple dataset I provide, then you don’t need to run the code.\nThe dataset is the stop and search reports for December 2023, and whether they result in an arrest or not, where we can separate by gender.\n\n\n\n\n\n\nData Tidying\n\n\n\n\n\n\ncrime &lt;- read_csv(\"2023-12-lancashire-stop-and-search.csv\")\n\ncrime |&gt; \n  select(Outcome, Gender) |&gt; \n  na.omit() |&gt;  #remove any rows without data present\n  mutate(Outcome = if_else(Outcome == \"Arrest\", \"Arrest\", \"No Arrest\")) |&gt; write_csv(\"2023-12-lancashire-stop-and-search_simple.csv\")\n\n\n\n\nOtherwise we just load in the data\n\ncrime &lt;- read_csv(\"2023-12-lancashire-stop-and-search_simple.csv\")\n\nRows: 2672 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Outcome, Gender\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nCreate a summary table\n\n\nVisualise the differences in a bar plot\n\n\nConstruct your hypothesis and assert what you think may be an effect here\nCarry out Chi-squared test and Cramer’s V.\n\n\nIs there a significant effect here? What does this mean for your hypothesis, can you reject the null or fail to reject the null? How would you write this in a report?\n\nThat’s all for this worksheet - feel free to continue exploring the crime dataset, download a broader time range? Is there an effect outside of the month of January? What about other police forces? Is one maybe “arrest-heavy”? Caveat, I don’t know - I only looked at the dataset I provided."
  },
  {
    "objectID": "Worksheets/Worksheet_wk4.html",
    "href": "Worksheets/Worksheet_wk4.html",
    "title": "4. Data visualization",
    "section": "",
    "text": "Three important things to remember:"
  },
  {
    "objectID": "Worksheets/Worksheet_wk4.html#step-0-preparing-your-environment",
    "href": "Worksheets/Worksheet_wk4.html#step-0-preparing-your-environment",
    "title": "4. Data visualization",
    "section": "Step 0: Preparing your Environment",
    "text": "Step 0: Preparing your Environment\nFirt things first, open up a new R script and load in the Tidyverse library\n\n\n\n\n\n\nNeed a reminder?\n\n\n\n\n\n\nlibrary(tidyverse)\n\n\n\n\nIn addition, please download the following data files from Moodle and place them in your working directory.\n\nlanguage_exams_shorter.csv\nnettle_1999_climate.csv\ntitanic.csv\ncarprice.csv\n\nYou will also notice when completing the handout, we will also use a “built-in” dataset. These are datasets that come with R (or are loaded with different packages). They are a nice way to illustrate some of the features of R.\nTo see the list of pre-loaded data, execute the function data(). This will display the available datasets in the script editor.\n\ndata()\n\nThe data is already installed, you don’t need to load it with the data() function. You can just use the built-in datasets in your commands. Often, these datasets come with packages and are used to demonstrate examples in these packages."
  },
  {
    "objectID": "Worksheets/Worksheet_wk4.html#step-1-two-plotting-systems-in-r-base-r-and-ggplot",
    "href": "Worksheets/Worksheet_wk4.html#step-1-two-plotting-systems-in-r-base-r-and-ggplot",
    "title": "4. Data visualization",
    "section": "Step 1: Two plotting systems in R: Base R and ggplot",
    "text": "Step 1: Two plotting systems in R: Base R and ggplot\nThere are fundamentally two methods for plotting in R, one that we touched on last week which was base R, and the other is via a library that comes bundled in the tidyverse called ggplot2. ggplot is currently the most widely used plotting system in R, and it can do lots of really cool things.\nBase R is called as such because it only uses functions that come with R in its most basic form. These are plots we can create without any additional packages required. You may come across examples of base plots in online resources such as StackOverflow when you go looking for help, so it is worthwhile understanding how they are created. Ultimately, you will be creating almost all your plots and graphics using ggplot, so I won’t spend much time on base R."
  },
  {
    "objectID": "Worksheets/Worksheet_wk4.html#step-2-base-r",
    "href": "Worksheets/Worksheet_wk4.html#step-2-base-r",
    "title": "4. Data visualization",
    "section": "Step 2: Base R",
    "text": "Step 2: Base R\nFirst, load in the data for this task and have a quick look at it\n\nlanguage_exams &lt;- read_csv(\"language_exams_shorter.csv\")\n\nRows: 10 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): age_cohort, student\ndbl (5): exam_1, exam_2, exam_3, level, age\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(language_exams)\n\n# A tibble: 6 × 7\n  age_cohort exam_1 exam_2 exam_3 level student    age\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1 15-17          57     52     59     1 Alesha      16\n2 15-17          92     76     81     1 Chiara      15\n3 15-17          63     60     66     1 Davina      17\n4 15-17          40     24     30     1 Sallie      16\n5 18-22          25     13     19     1 Roxanne     18\n6 18-22          84     76     82     1 Maariyah    21\n\n\nNext, we want to get the mean scores of the variables exam_1, exam_2, and exam_3. We can do this using the summarise() function that comes from dplyr (part of Tidyverse). Take a look at what the help tab has to say about this function by typing ?summarise, or by going to the Help tab in the top-right, and using the search bar to look for summarise.\nIt tells us that “summarise() creates a new data frame. It returns one row for each combination of grouping variables; if there are no grouping variables, the output will have a single row summarising all observations in the input. It will contain one column for each grouping variable and one column for each of the summary statistics that you have specified.”\nIf we then scroll to the bottom of the help pane, we get to the examples - have a look at the very first one, which I paste below. Make sure you know where the examples are, they are invaluable for understanding how new code works.\nNote: mtcars is one of these preloaded datasets I just mentioned, and see how it lets you run example code very easily.\n\nmtcars %&gt;%\n  summarise(mean = mean(disp), n = n())\n\n      mean  n\n1 230.7219 32\n\n\nSo we can run example code, and it gives us an output. How do we go about understanding what it has just done? In this section of code, we have four components, and only one of which we have seen before. Let me break it down for you:\n\nmtcars - a prepackaged dataset, and was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models). So it’s just a small “toy” datatset we can test ideas on.\nsummarise() - a new function that creates summary tables from larger datasets\nmean = mean(disp) - we have used mean() before. Here, we are telling summarise() to get the mean of the variable disp, and show it in a column called mean.\nn = n() - this is asking for the Number of observations (rows) in the dataset. n for number.\n\nCan you use this new knowledge to construct a summary table that will give you an output of the mean of exam_1, exam_2, and exam_3?\n\n\n\n\n\n\nanswer\n\n\n\n\n\n\nmean_scores &lt;- language_exams |&gt; \n  summarise(mean_1 = mean(exam_1),\n            mean_2 = mean(exam_2),\n            mean_3 = mean(exam_3))\n\nmean_scores\n\n# A tibble: 1 × 3\n  mean_1 mean_2 mean_3\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   63.9   53.8   60.9\n\n\nNotice how after each comma I start a new line, this is for ease of reading, and R knows to look at the next line if it seems a comma at the end of a line.\n\n\n\nOkay, so we have our summary table made from summarise() but it isn’t quite in the right format to use in plots, so let’s coerce the mean_scores object to a numeric vector:\n\nclass(mean_scores) # to show it's a tibble\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nmean_scores &lt;- mean_scores |&gt; \n  as.numeric() #quite literally, telling R to read it as numbers and not a tibble.\n\nclass(mean_scores) #now it's numeric!\n\n[1] \"numeric\"\n\n\n\nBarplots\nNow let’s use it in a barplot\n\nbarplot(mean_scores)\n\n\n\n\nYou should now see the bar chart in the Plot tab of the Files, Plots, etc. pane, bottom right.\nBy default, the plot is vertical, but we can change this.\nCompare the following output to the output produced by the previous command.\n\nbarplot(mean_scores, horiz = TRUE) \n\n\n\n\nThis is all very good, but we need to add names to our columns, of course. We can do this by adding the names.arg() argument as below.\n\nbarplot(mean_scores, names.arg = c(\"Exam 1\", \"Exam 2\", \"Exam 3\"))\n\n\n\n\nWe can go on and add titles and axis labels like so. Or change colours and make it look fancy, but really Base plots should be used for very fast, basic plots for you to check things quick. We will cover these ideas properly in the ggplot section\n\nbarplot(\n  mean_scores,\n  names.arg = c(\"1\", \"2\", \"3\"),\n  # Note: We change the column names to avoid duplication\n  main = \"Performance on the exams\", # title\n  xlab = \"Exams\", #x axis\n  ylab = \"Scores\" #y axis\n)\n\n\n\nbarplot(mean_scores, col = \"turquoise\", border = \"steelblue\")\n\n\n\nbarplot(\n  mean_scores,\n  col = \"turquoise\",\n  border = \"steelblue\",\n  names.arg = c(\"1\", \"2\", \"3\"),\n  # Note: We change the column names to avoid duplication\n  main = \"Performance on the exams\",\n  xlab = \"Exams\",\n  ylab = \"Scores\"\n)\n\n\n\n\n\n\nHistograms\nLet’s quickly look at histograms in Base R. We start by creating two new objects, exam_1 and ages, both extracted from our language_exams data.\n\nexam_1 &lt;- language_exams$exam_1\n\nages &lt;- language_exams$age\n\nA histogram can be created using the hist() function. This plots the frequency of the variable ages in the our language exams dataset. You can play with colours if you want here, it’s the same as with barplot\n\nhist(ages)\n\n\n\n\nWe can change the number of breaks, i.e. the breakpoints between histogram cells. This is useful as sometimes the default breakpoint obscures the data.\n\nhist(ages, breaks = 20)"
  },
  {
    "objectID": "Worksheets/Worksheet_wk4.html#lineplots",
    "href": "Worksheets/Worksheet_wk4.html#lineplots",
    "title": "4. Data visualization",
    "section": "Lineplots",
    "text": "Lineplots\nTo illustrate line plots in Base R, let’s start by generating some data and creating three variables. We will create our own tibble from ideas we used in previous weeks. I’m leaving this code intentionally uncommented and a bit barebones - I would like for you to really read the code, use the help pane to understand what it is you are doing, test different components, etc.\nDon’t just run the code and assume it works - like I’ve said before, if you do that, you’ll get really good at running Matthew’s code, but not your own.\n\ntest_data &lt;- tibble(var_1 = c(1:20),\n                    var_2 = var_1^2,\n                    var_3 = 4 * var_2)\n\nTo test your understanding, can you mutate() the test_data tibble and add a new column called var_4 which is var_1 divided by var_3? Make sure to assign it as test_data still!\n\n\n\n\n\n\nanswer\n\n\n\n\n\n\ntest_data &lt;- test_data |&gt; \n  mutate(var_4 = var_1 / var_3)\n\ntest_data\n\n# A tibble: 20 × 4\n   var_1 var_2 var_3  var_4\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     1     1     4 0.25  \n 2     2     4    16 0.125 \n 3     3     9    36 0.0833\n 4     4    16    64 0.0625\n 5     5    25   100 0.05  \n 6     6    36   144 0.0417\n 7     7    49   196 0.0357\n 8     8    64   256 0.0312\n 9     9    81   324 0.0278\n10    10   100   400 0.025 \n11    11   121   484 0.0227\n12    12   144   576 0.0208\n13    13   169   676 0.0192\n14    14   196   784 0.0179\n15    15   225   900 0.0167\n16    16   256  1024 0.0156\n17    17   289  1156 0.0147\n18    18   324  1296 0.0139\n19    19   361  1444 0.0132\n20    20   400  1600 0.0125\n\n\n\n\n\nWe can use the plot() function to display variables 1 and 2.\n\nplot(test_data$var_1, test_data$var_2)\n\n\n\n\nThere are many types of line. Try out changing the line types by modifying the value for the type argument.\n\nplot(test_data$var_1, test_data$var_2, type = \"p\") # points\n\n\n\nplot(test_data$var_1, test_data$var_2, type = \"b\") # both points and lines\n\n\n\nplot(test_data$var_1, test_data$var_2, type = \"o\") # overplots points and lines\n\n\n\nplot(test_data$var_1, test_data$var_2, type = \"l\") # lines\n\n\n\nplot(test_data$var_1, test_data$var_2, type = \"n\") # no points or lines\n\n\n\n\noften we need to display more than one line. This is how it works in Base R\nFirst, we plot one line. (We’re also adding color and labels.)\n\nplot(\n  test_data$var_1,\n  test_data$var_2,\n  type = \"b\",\n  frame = FALSE,\n  pch = 20,\n  col = \"turquoise\",\n  xlab = \"Age\",\n  ylab = \"Scores\"\n)\n\n#Now, we can add a second line on top.\n\nlines(test_data$var_1,\n      test_data$var_2,\n      pch = 20,\n      col = \"darkblue\",\n      type = \"b\")\n\n\n\n# Let's add a legend to the plot to make it clear what the lines refer to.\n# Of course there are more compexities to base plots and how they look, but again, ggplot is where we are going to be heading.\n\n\nlegend(\n  \"topleft\",\n  legend = c(\"Group A\", \"Group B\"),\n  col = c(\"turquoise\", \"darkblue\"),\n  lty = 1:2,\n  cex = 0.8\n)\n\n\n\n\nOkay, enough of base R plotting. We’ve seen how it works with three basic types of plot. We could go further and do scattergraphs with plot() and we just don’t try and add in line types, or we could do boxplots with boxplot() but I would rather show you these ideas and to revisit the other plots using ggplot. Why? It is far more powerful, lends itself to the tidy process and just generally is better. Once you start making them, I wouldn’t be surprised if you start seeing ggplot-made graphs in publications you read, I know I see them all the time!"
  },
  {
    "objectID": "Worksheets/Worksheet_wk4.html#step-3-ggplot",
    "href": "Worksheets/Worksheet_wk4.html#step-3-ggplot",
    "title": "4. Data visualization",
    "section": "Step 3: ggplot",
    "text": "Step 3: ggplot\nggplot is a powerful system for producing elegant graphics. ggplot is included in the tidyverse package, so if you have loaded the latter you’re good to go. Alternatively, you can load the ggplot2 package directly as follows.\nTo learn more about the ggplot, I recommend the textbooks above, but also the following websites.\nAn excellent reference for ggplot: https://ggplot2.tidyverse.org/index.html\nA useful cheat sheet: https://github.com/rstudio/cheatsheets/blob/main/data-visualization.pdf\nGorgeous graphs in ggplot: https://r-graph-gallery.com/ggplot2-package.html\nThe gg in ggplot means grammar of graphics (Wickham, 2010). This is a system for mapping variables in a dataset to properties of a plot (e.g., shape, size, color position).\nWe will focus on using ggplot() to produce our visualizations.\nEvery time we use the ggplot command we need to specify three things (at least).\n\nThe dataset containing all the data. This can be a tibble, data frame, vector, etc.\nThe set of aesthetic mappings (aes). These describe how variables in the dataset are mapped to visual properties (aesthetics) of the type of graph we want to produce (geometric objects). Aesthetics is used to indicate x and y variables, to control the color, the size or the shape of points, the height of bars, and so forth.\nThe set of layers that render the aesthetic mappings. This is usually done according to prespecified geometric objects (geoms) such as lines, points, bars. The geometry defines the type of graphics we wish to plot (histogram, bar chart, boxplot, etc.)\n\nIn other words, we need to tell R what data to use for the plot, the type of graph, and the mapping between the visual cue and the variable (position, color, fill, etc.)\nTo understand how this work in practice, let’s start with a simple example.\nThe first argument accepted is the data, and then we provide the mapping and tell what we want the x and y axes to be.\n\nggplot(language_exams,\n       mapping = aes(x = exam_1, y = exam_2))\n\n\n\n\nBut wait, what do you see in the Plot tab? A blank canvas, right? This is because we did not specify the type of geometric object (geom). We need to add an additional line to specify the type of plot that we want\n\nScatter plot\n\nggplot(language_exams,\n       mapping = aes(x = exam_1, y = exam_2)) +\n  geom_point() # for a scatter plot\n\n\n\n\nAs you can see in your output, you should now see a scatterplot with the data from exam_1 and exam_2.\n\n\nLine plot\nIf you prefer a different type of plot, just change the geom. Below, we now produce a line plot.\n\nggplot(language_exams,\n       mapping = aes(x = exam_1, y = exam_2)) +\n  geom_line() # for a line plot\n\n\n\n\nThe funadamental difference between base and ggplot in terms of code layout, is that base in just “drawing thing” whereas ggplot attempts to make a coherent package where you are writing a little section of code that layers different components to create complex plots. Ggplot relies on an underlying dataframe, and encourages you to have your data in the correct format (nice and tidy) before plotting, whereas base R can be bodged quite a bit resulting in really messy sections of code.\nThe sequential nature of layering a ggplot figure is quite simplistic and I find easier to alter and manage. For further plots, let us pipe in the data frame to ggplot for improved readability.\nWe can also specify, in the mapping aes, what shape the different points in the scatterplot should have. For example, we can use a third variable (non-numeric) as a shape or as a color.\n\nlanguage_exams |&gt; \n  ggplot(mapping = aes(x = exam_1, y = exam_2, shape = age_cohort)) + \n  geom_point()\n\n\n\n\nand\n\nlanguage_exams |&gt; \n  ggplot(mapping = aes(x = exam_1, y = exam_2, color = age_cohort)) +\n  geom_point()\n\n\n\n\nOne more thing. We actually don’t need to write mapping, nor do we need to spell out x and y. By default, ggplot will assume that the first function after you specify the dataset (language_exams) is the mapping (aes), and that the first two arguments with the aes() are the x-axis and y-axis respectively.\nCompare the following and see that they are functionally equivalent:\n\nlanguage_exams |&gt; \n  ggplot(mapping = aes(x = exam_1, y = exam_2)) + geom_point() #explicit\n\n\n\nlanguage_exams |&gt; \n  ggplot(aes(exam_1, exam_2)) + geom_point() #condensed\n\n\n\n\nWe will use the condensed version of writing, because why wouldn’t we?"
  },
  {
    "objectID": "Worksheets/Worksheet_wk4.html#step-4-histograms-and-frequency-polygons-with-ggplot",
    "href": "Worksheets/Worksheet_wk4.html#step-4-histograms-and-frequency-polygons-with-ggplot",
    "title": "4. Data visualization",
    "section": "Step 4: Histograms and frequency polygons with ggplot",
    "text": "Step 4: Histograms and frequency polygons with ggplot\nLet’s do some plotting with ggplot. We begin with histograms.\nWe first load the data from the Nettle (1999) book about language diversity and create a new object called languages.\n\nlanguages &lt;- read_csv(\"nettle_1999_climate.csv\")\n\nRows: 74 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Country\ndbl (4): Population, Area, MGS, Langs\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nHistograms are plotted if we use geom_histogram() in the ggplot() command.\n\nlanguages |&gt; \n  ggplot(mapping = aes(x = Langs)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNotice that we only supply an x axis for a histogram, because it does the y-axis automatically based on count data.\n\nThemes\nLet’s pause for a second to consider themes(). In ggplot, we can make many, many, many, many detailed changes to our graphs, which we cannot cover here. Check out the graph gallery here. https://r-graph-gallery.com/ggplot2-package.html\nBut we can also make bigger changes easily by adding themes() to our ggplot() commands.\nLet’s compare the following four types of themes. (There are many other themes.)\n\nlanguages |&gt; \n  ggplot(mapping = aes(x = Langs)) + \n  geom_histogram() # No theme\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nlanguages |&gt; \n  ggplot(mapping = aes(x = Langs)) + \n  geom_histogram() + \n  theme_gray() # Same as before, the default\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nlanguages |&gt; \n  ggplot(mapping = aes(x = Langs)) + \n  geom_histogram() + \n  theme_minimal()   # Theme minimal\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nlanguages |&gt; \n  ggplot(mapping = aes(x = Langs)) + \n  geom_histogram() + \n  theme_classic()   # Theme classic\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nBack to histograms\nWe can also format the different elements of the histogram. Below, we change the widths of the bins and later add some color, too. Again, compare what the commands do.\n\nlanguages |&gt; \n  ggplot(aes(Langs)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nlanguages |&gt; \n  ggplot(aes(Langs)) + \n  geom_histogram(binwidth = 50)\n\n\n\nlanguages |&gt; \n  ggplot(aes(Langs)) + \n  geom_histogram(binwidth = 50,\n                 color = 'turquoise4',\n                 fill = 'paleturquoise')\n\n\n\n\nSometimes you might prefer using a frequency polygon to display data rather than a histogram. For this, just use the geom_freqpoly() command.\nCompare. First, a histogram. (You can see the plot above.)\n\nlanguages |&gt; \n  ggplot(aes(Langs)) + geom_histogram(binwidth = 50,\n                                      color = 'turquoise4',\n                                      fill = 'paleturquoise') \n\n\n\n\nThen, a frequency polygon.\n\nlanguages |&gt; \n  ggplot(aes(Langs)) + geom_freqpoly(binwidth = 50,\n                                     color = 'turquoise4',\n                                     fill = 'paleturquoise') \n\nWarning in geom_freqpoly(binwidth = 50, color = \"turquoise4\", fill =\n\"paleturquoise\"): Ignoring unknown parameters: `fill`\n\n\n\n\n\n\n\nBar charts\nLet’s try out bar charts now. Again, we load data first. This comes from the Andrews (2021) textbook. Make sure to load it in, and have a look at the data to get an idea for it.\n\ntitanic &lt;- read_csv(\"titanic.csv\")\n\nRows: 1309 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): name, survived, sex, passengerClass\ndbl (1): age\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nHow many columns?\nHow many rows?\nWhat kind of data is in the object?\n\nTo do bar charts, we use geom_bar().\nCompare what the different commands do.\n\ntitanic |&gt; \n  ggplot(aes(passengerClass)) + \n  geom_bar()\n\n\n\ntitanic |&gt; \n  ggplot(aes(passengerClass, fill = survived)) + \n  geom_bar()\n\n\n\ntitanic |&gt; \n  ggplot(aes(passengerClass, fill = survived)) + \n  geom_bar(position =   \"dodge\")\n\n\n\n\nAs you can see, we can use nominal categories as fill in aesthetic mapping (fill = survived), and we can manipulate the position of the bars (position = “dodge”).\nLet’s more data before we illustrate further. This dataset is also from Andrew (2021), it displays information about cars (lots of them).\n\ncar_prices &lt;- read_csv(\"carprice.csv\")  \n\nNew names:\nRows: 48 Columns: 10\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): Type dbl (9): ...1, Min.Price, Price, Max.Price, Range.Price, RoughRange,\ngpm100,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\nThe following command creates a bar chart visualizing the prices (y-axis) according to type of car (x-axis).\n\ncar_prices |&gt; \n  ggplot(aes(Type, Price)) + \n  geom_bar(stat = \"identity\")\n\n\n\n\nWe can also group the data to further explore. The command below creates a new object called cars_prices_grouped, loads the car price data, groups it by car type, and calculates the mean prices for each car type.\nWe’ve seen something as complex as the below before, but take a look and make sure you know what’s happening, on the LHS and RHS of each pipe.\n\ncars_prices_grouped &lt;- car_prices %&gt;%\n  group_by(Type) %&gt;%\n  summarise(Price = mean(Price))\n\nIf you now type the name of the new object cars_prices_grouped, you will get a tibble with the average price.\n\ncars_prices_grouped # Compare to car_prices above\n\n# A tibble: 6 × 2\n  Type    Price\n  &lt;chr&gt;   &lt;dbl&gt;\n1 Compact  12.8\n2 Large    24.3\n3 Midsize  21.8\n4 Small    10.0\n5 Sporty   19.4\n6 Van      18.3\n\n\nWe can now do another bar chart with the grouped data.\n\ncars_prices_grouped |&gt; \n  ggplot(aes(Type, Price)) + \n  geom_bar(stat = \"identity\")\n\n\n\n\nWe can even combine the grouping process into the plotting too, bear with me on this next chunk of code, it’ll be the longest pipeline we’ve done so far\n\ncar_prices  |&gt; \n  group_by(Type)  |&gt; \n  summarise(Price = mean(Price)) |&gt; \n  ggplot(aes(Type, Price)) + \n  geom_bar(stat = \"identity\")\n\n\n\n\nThree pipes and a plot, that’s a lot. But break it down, compare it against the previous lines of code we have seen, and can you understand exactly how we went from the dataset car_prices through to a barplot? Could you take the above code and split it out into the two separate actions again?\nWhy might we want to pipe all these different commands together? What benefit does it give us?\n\n\nBoxplots\nFor the boxplots, we will use the built-in dataset ToothGrowth.\nFirst, let’s convert the variable dose from a numeric to a factor variable. These are categorical variables that can be either numeric or string variables and that can be used more easily in different types of graphics. We can do this in one of two ways, there is the base option which is the first line but commented out (as it is only to show you), and the tidy version.\n\n#ToothGrowth$dose &lt;- as.factor(ToothGrowth$dose) ## just as an example\n\nToothGrowth &lt;- ToothGrowth |&gt; \n  mutate(dose = as.factor(dose))\n\nIn an ideal world, we want to keep our writing style either in base or in tidy, I will teach you tidy because it means you can pipe things together and have really well-written, easily understandable code that can be run using one button press, rather than multiple. It helps to keep our code programmatic, logical, and easily understood where multiple functions are used to complete one meta-action.\nIn the above, the base version may seen to be more easily written, but when things get more complicated, then tidy is better. It is best to understand the priciples now, because it’ll be easier to carry out in your own analyses later on. I am not teaching you anything that I don’t personally do in my own coding. There is no pedagogically-motivated reason for me to teach Tidy over Base, beyond that the Tidy style is one of the most common ways of writing R code.\nLet’s look at generating a boxplot\n\nToothGrowth |&gt; \n  ggplot(aes(x = dose, y = len)) + \n  geom_boxplot()\n\n\n\n\nTo make our life a bit easier, let’s assign the command above (which produces a plot) to a new object called our_boxplot. This command creates an object called our_boxplot, which consists of, well, a boxplot.\n\nour_boxplot &lt;- ggplot(ToothGrowth, aes(x = dose, y = len)) + \n  geom_boxplot()\n\nIf you now run this command, you create the boxplot.\n\nour_boxplot\n\n\n\n\nTo rotate a boxplot, just coord_flip()to your boxplot, as in the example below.\n\nour_boxplot + coord_flip()\n\n\n\n\nNotched box plots are also useful. The notch refers to the narrowing around the median. You can create a notched box plots as follows. Did you notice that there is an outlier? (See above the top whisker.) By default, outliers are in the color of the box. But we can change color, shape and size of the outlier.\nLet’s try out different shapes. See how the different values for the outlier.shape argument affect the plot. What happens when you run the following commands?\n\nToothGrowth |&gt; \n  ggplot(aes(x = dose, y = len)) +\n  geom_boxplot(\n    outlier.colour = \"darkblue\",\n    outlier.shape = 0, # square\n    outlier.size = 4)\n\n\n\nToothGrowth |&gt; \n  ggplot(aes(x = dose, y = len)) +\n  geom_boxplot(\n    outlier.colour = \"darkblue\",\n    outlier.shape = 1, # circle\n    outlier.size = 4)\n\n\n\n\nTry out some other numbers for outlier.shape, what do they produce?\nThe function stat_summary() can be used to add mean points to a box plot, as in the following command.\n\nour_boxplot + stat_summary(\n  fun.y = mean,\n  geom = \"point\",\n  shape = 23,\n  size = 4\n)\n\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\nℹ Please use the `fun` argument instead.\n\n\n\n\n\nWe can add points to a box plot by using the functions geom_dotplot() or geom_jitter().\nIn the following example, our box plot also has a dot plot.\n\nour_boxplot + \n  geom_dotplot(binaxis = 'y',\n               stackdir = 'center',\n               dotsize = 0.5)\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\nAnd in this example, it has jittered points with 0.2 degree of jitter in x direction. The jitter geom adds a bit of random variation to the location of each point. This is useful when too many points are overlapping.\n\nour_boxplot + \n  geom_jitter(shape = 16, position = position_jitter(0.2))\n\n\n\n\nWe can also use special types of boxplots that combine a density plot with the boxplot. These are called violin plots, becasue they look like violins, if you squint a lot.\n\nToothGrowth |&gt; \n  ggplot(aes(x = dose, y = len)) + \n  geom_violin()\n\n\n\n\nThese plots show you where the central tendency is and how the distribution sits.\n\n\nScatterplots again\nLet’s now turn to scatterplots. Again, let’s some data, this time the Nettle (1999) data about language diversity, used in Winter (2019).\n\nlanguages &lt;- read_csv(\"nettle_1999_climate.csv\")\n\nRows: 74 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Country\ndbl (4): Population, Area, MGS, Langs\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe geom_point() command creates scatterplots. This first one below uses points.\n\nggplot(languages, aes(MGS, Langs)) + geom_point()\n\n\n\n\nBut this one uses the text, drawn from the Country variable in our dataset languages.\n\nggplot(languages, aes(MGS, Langs, label = Country)) + geom_text()\n\n\n\n\nLet’s load some additional data.\n\nlanguage_exams &lt;- read_csv(\"language_exams_shorter.csv\")\n\nRows: 10 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): age_cohort, student\ndbl (5): exam_1, exam_2, exam_3, level, age\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIn the following scatterplots, we will use the variable age cohort for color.\n\nlanguage_exams |&gt; \n  ggplot(aes(x = exam_1, y = exam_2, color = age_cohort)) + \n  geom_point()\n\n\n\n\nThere are many parameters you can add, delete or edit in your plots. The cheat sheet is very helpful in that regard.\nBy default, the size of your points are in size 2. You can see this by comparing the scatterplot above, which doesn’t have a size specification, to the one created by the following command. (The plot looks the same, so we won’t plot this here.)\n\nlanguage_exams |&gt; \n  ggplot(aes(x = exam_1, y = exam_2, color = age_cohort)) + \n  geom_point(size = 2)\n\n\n\n\nLet’s play around with size and shapes in scatterplots. See what happens when you run these commands.\n\nlanguage_exams |&gt; \n  ggplot(aes(x = exam_1, y = exam_2, color = age_cohort))   +\n  geom_point(size = 3)\n\n\n\nlanguage_exams |&gt; \n  ggplot(aes(x = exam_1, y = exam_2, color = age_cohort)) + \n  geom_point(size = 4)\n\n\n\n\nAnd now let’s try out different shapes. How do the commands change your plots?\n\nlanguage_exams |&gt; \n  ggplot(aes(x = exam_1, y = exam_2, color = age_cohort)) +\n  geom_point(size = 3, shape = 'triangle') # Same as writing 2\n\n\n\nlanguage_exams |&gt; \n  ggplot(aes(x = exam_1, y = exam_2, color = age_cohort)) +\n  geom_point(size = 4, shape = 'diamond') # Same as writing 5\n\n\n\nlanguage_exams |&gt; \n  ggplot(aes(x = exam_1, y = exam_2, color = age_cohort)) +\n  geom_point(size = 3, shape = 'square') # Same as writing 0\n\n\n\n\nWe can also use labels for data points.\n\nlanguage_exams |&gt; \n  ggplot(aes(x = exam_1,\n             y = exam_2,\n             label = student,\n             color = age_cohort)) + \n  geom_point(size = 3, shape = 'triangle') +\n  geom_text(size = 4)\n\n\n\n\nThe geom_text_repel() function is useful if we intend to use labels.\nWe need to install and load it first. It comes from a package called ggrepel - can you remember how to install and load libraries? Check on previous week’s content if you can’t remember exactly.\n\n#|echo: false\n\n\nlibrary(ggrepel)\n\n\nlanguage_exams |&gt; \n  ggplot(aes(x = exam_1,\n             y = exam_2,\n             label = student,\n             color = age_cohort)) + \n  geom_point(size = 3) +\n  geom_text_repel(size = 4, segment.alpha = 2)"
  },
  {
    "objectID": "Worksheets/Worksheet_wk4.html#step-5-saving-plots",
    "href": "Worksheets/Worksheet_wk4.html#step-5-saving-plots",
    "title": "4. Data visualization",
    "section": "Step 5: Saving plots",
    "text": "Step 5: Saving plots\nThis is the ggsave() command. Let’s try saving our plots in a few different file formats (png, pdf, jpeg). Have a look at the working directory to see if you can find your three new files.\n\nggsave('our_plot.png', width = 8, height = 6)\n\nggsave('our_plot.pdf', width = 8, height = 6)\n\nggsave('our_plot.jpeg', width = 8, height = 6)\n\nYou can also save plots with specific resolutions. They default is dpi = 300.\nAlternatively, you can also write string input: “retina” (dpi 320), “print” (dpi 300), or “screen” (dpi 72). Have a go at the following.\n\nggsave(\n  'our_plot_300.jpeg',\n  width = 8,\n  height = 6,\n  dpi = 300\n)\n\nggsave(\n  'our_plot_screen.jpeg',\n  width = 8,\n  height = 6,\n  dpi = 'screen'\n)\n\nggsave(\n  'our_plot_retina.jpeg',\n  width = 8,\n  height = 6,\n  dpi = 'retina'\n)\n\nThis may have seemed like a deep dive into ggplot, but honestly, we have only touched very lightly on ggplot. I recommend you check out online materials and start exploring! The best thing is that components layer upon each other, and you can learn and add new things as you go. The tidy structure of the syntax also means things can be changed or swapped out easily."
  },
  {
    "objectID": "Worksheets/Worksheet_wk4.html#step-6-colours",
    "href": "Worksheets/Worksheet_wk4.html#step-6-colours",
    "title": "4. Data visualization",
    "section": "Step 6: Colours",
    "text": "Step 6: Colours\nIn R, you can either specify colors by writing their names (e.g., “mistyrose”) or you can write the hexadecimal code (#FFE4E1).\nYou can try out compare the following.\nThe first uses the color names. The second uses the same colors but refers to them hexadecimal code.\n\nbarplot(c(2,5), col=c(\"paleturquoise\", \"mistyrose\"))\n\n\n\nbarplot(c(2,5), col=c(\"#30D5C8\", \"#FFE4E1\"))\n\n\n\n\nThe following website provides color names and a hex code finder:\nhttps://r-graph-gallery.com/ggplot2-color.html"
  },
  {
    "objectID": "Worksheets/Worksheet_wk4.html#take-home-task",
    "href": "Worksheets/Worksheet_wk4.html#take-home-task",
    "title": "4. Data visualization",
    "section": "Take home task",
    "text": "Take home task\n\nCan you create a plot using the language_exams object to draw a scatter plot between exam_1 and exam_2 scores?\n\n\nDraw a boxplot using ToothGrowth to show the differences between the supplement type and tooth length.\n\nHint: take a look at ?ToothGrowth to see what each column means.\n\nCan you update the code for question 2 in order to plot only the dose differences for the OJ supplement? Hint, you’ll need to use filter on the dataset before creating the plot. We learnt about filter() in last week’s worksheet, or look at ?filter (second option, the one from dplyr “Keep rows that match a condition”) and check out examples at the bottom.\n\nThe plot should only show data from OJ tests for each level of dose.\n\nCan you turn this into a violin plot? What do you think is better for commuicating the information?"
  },
  {
    "objectID": "Worksheets/Worksheet_wk9.html",
    "href": "Worksheets/Worksheet_wk9.html",
    "title": "9. ANOVA and linear regression",
    "section": "",
    "text": "Three important things to remember:"
  },
  {
    "objectID": "Worksheets/Worksheet_wk9.html#task-0-setting-up-our-environment",
    "href": "Worksheets/Worksheet_wk9.html#task-0-setting-up-our-environment",
    "title": "9. ANOVA and linear regression",
    "section": "Task 0: Setting up our environment",
    "text": "Task 0: Setting up our environment\nYou should be able to do all these things, if not - check back on previous week’s content as a reminder\n\nCreate a new script and call it Week 9.\nLoad in the tidyverse library at the top of the script"
  },
  {
    "objectID": "Worksheets/Worksheet_wk9.html#task-1-between-participant-anova",
    "href": "Worksheets/Worksheet_wk9.html#task-1-between-participant-anova",
    "title": "9. ANOVA and linear regression",
    "section": "Task 1: Between-participant ANOVA",
    "text": "Task 1: Between-participant ANOVA\nWe aren’t going to spend much time on ANOVA, because as I said in the lecture it is a special case of the linear regression, and I personally believe that while you need to know that ANOVAs exist (because people still use them) you are always better off using the linear model. In fact, to provde their equivalence, I will demonstrate in Task 3 that the same output from the anova is achieved using lm()\n\nData\nread in the file called robo_lab.csv and call the object robot.\nThis is a “toy” dataset - we’ve come across these before, but essentially it’s synthetic data to drive home an analytical point. Let’s create some context for this dataset called robot. I have decided that teaching could be more effectively done by a robot, rather than myself. So in my spare time I built three different robots designed to teach statistics. But I don’t know which of the three is best. Robot Alpha (A) is pretty bare bones, I designed in first, and built it with scrap materials. Robot Beta (B) has a dopey little face I made. Finally, I learnt how to 3D print more likelike faces, so I built Robot Omega (O) too.\n  \nFrom these three different robots, who only differ in physical appearance, I believed that the more likelife the robot, the better students would score on a statistics test, because they would be less scared of the robot. Formally, my hypothesis is “scores would be significantly higher amongst individuals assigned Robot O(mega) than those assigned Robot A(lpha) or B(eta)”.\nIt was now time to test how the students responded to the robots. In a controlled experiment, the researchers randomly assigned groups of students either Robot A(lpha), Robot B(eta), or Robot O(mega). Note this experiment is one factor (Robot assignment) study made of three levels (Robot A, Robot B, Robot C). Those individuals assigned Robot A(lpha) were denoted as belonging to ‘Group A’.\nThose individuals assigned Robot B(eta) were denoted as belonging to ‘Group B’.\nThose individuals assigned Robot O(mega) were denoted as beloning to ‘Group O’.\nNote. The groups are mutually exclusively - i.e., a participant was assigned to either Group A/Robot A, Group B/Robot B, Group O/Robot O.\nPrior to the experiment, independent raters reliably agreed that Robot O(mega) more closely resembled a human being than Robot B(eta) [second closest resemblance] and Robot A(lpha) [lowest resemblance].\nThe dataset contains three variables: participant ID (ID), Which robot they were taught by (Group), How they rated the robot (Likeability), and their test score (Score).\nFirst things first, let us summarise the data and get a mean and SD for score per group. We will need to use group_by() and summarise(). Assign it to an object called robot_descriptives\nWe can also use max() and min() to get the maximum and minimum scores on the test too.\n\n\nVisualising\nNext, let’s have a look at the data in plot form.\nFirst a histogram, noting that I specify an argument bins = 12. Can you look in the help tab and search for geom_histogram to see what bins does?\n\nrobot |&gt; \n  ggplot(aes(Score)) +\n  geom_histogram(bins = 12)\n\n\n\n\nNow a boxplot We can use a boxplot because the Group variable is a factor, it is categorical.\n\n\n\n\n\n\n\nAnalysis\nTo identify whether the differences between groups are statistically significant, we need to perform an Analysis of Variance. First, let’s be sure we have chosen the correct type of ANOVA. We have one factor. This is because we have one Independent Variable (Robot) with three levels (Robot A, B and O). Also, the design is between group, as each participant is assigned to only one robot, and data from only one time point is taken.\nTo run our one-factor between participants ANOVA, we will use tidyverse’s aov() function. Let’s set ‘Score’ as our first dependent variable of examination. ‘Group’ is then our predictive factor (independent variable). Note, in the code below, the DV and the IV are separated by the ‘~’ character. Please carry out the following command. This also includes requesting a summary and effectsize for our model.\n\nmodel_robot &lt;- aov(data = robot, Score ~ Group)\nsummary(model_robot) #We ask for a summary of this model. This provides F statistic and P value\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nGroup         2   1223   611.3   12.52 6.77e-06 ***\nResiduals   237  11571    48.8                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFor calculating effect sizes for ANOVAs, we use eta-squared, or partial eta-squared. For one-way between subjects designs, partial eta-squared is equivalent to eta-squared. We can calculate this using the function effectsize() from the package effectsize. So install and load effectsize first. effectsize also gives us the confidence intervals of the effect too, but note it is only bounded at the bottom and it gives us a message to say so too.\n\nlibrary(effectsize)\n\neffectsize(model_robot) # we ask for an eta2 effect size for our model\n\nFor one-way between subjects designs, partial eta squared is equivalent\n  to eta squared. Returning eta squared.\n\n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\nGroup     | 0.10 | [0.04, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nANOVA effect sizes can be interpreted as:\n\\(\\eta^2\\) = 0.01 indicates a small effect \\(\\eta^2\\) = 0.06 indicates a medium effect \\(\\eta^2\\) = 0.14 indicates a large effect\nBefore we move on, realise that we tested three groups and all the ANOVA tells us is that there is a significant difference between groups. But which ones? We need to run additional post-hoc tests to determine the differences.\nPost-hoc tests, as mentioned in the lecture, allow us to test each group individually. It is the equivalent (in this case) of running a t-test between each group, but where we have reduced our p-value threshold (acknowledging the issue of multiple comparisons - again see the lecture).\nWe need to install and load a new library called rstatix first\n\nlibrary(rstatix)\n\n\nAttaching package: 'rstatix'\n\n\nThe following objects are masked from 'package:effectsize':\n\n    cohens_d, eta_squared\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\npairwise_t_test(robot,\n  Score ~ Group, # indicate this is paired (within participants) data\n  p.adjust.method = \"bonferroni\" #adjust for bonferroni correction\n  )\n\n# A tibble: 3 × 9\n  .y.   group1 group2    n1    n2          p p.signif     p.adj p.adj.signif\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;       \n1 Score A      B         80    80 0.0417     *        0.125     ns          \n2 Score A      O         80    80 0.00000123 ****     0.0000037 ****        \n3 Score B      O         80    80 0.00372    **       0.0111    *           \n\n\nThe output from this gives us a lot of new information. Importantly, it breaks down the differences between each group, compares them, and gives us an adjusted p-value, which is adjusted for the multiple comparisons. It also tells us whether or not the new p-value is significant or not. In this case, we can see that groups A & B are not significantly different, A & O are, and B & O are too. So the most lifelike robot leads to greater test scores, and A and B are not significantly different, perhaps because they’re both too robot-y.\nRight, we’ve covered a lot of analysis and now we need to gather all the relevant information together for reporting the ANOVA results in APA format. To do this, let’s run all the outputs together so we can identify all the necessary values.\n\nrobot_descriptives\n\n# A tibble: 3 × 3\n  Group  mean    SD\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A      58.1  6.45\n2 B      60.4  7.27\n3 O      63.6  7.22\n\nsummary(model_robot)\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nGroup         2   1223   611.3   12.52 6.77e-06 ***\nResiduals   237  11571    48.8                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\neffectsize(model_robot)\n\nFor one-way between subjects designs, partial eta squared is equivalent\n  to eta squared. Returning eta squared.\n\n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\nGroup     | 0.10 | [0.04, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\npairwise_t_test(robot,\n  Score ~ Group, # indicate this is paired (within participants) data\n  p.adjust.method = \"bonferroni\" #adjust for bonferroni correction\n  )\n\n# A tibble: 3 × 9\n  .y.   group1 group2    n1    n2          p p.signif     p.adj p.adj.signif\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;       \n1 Score A      B         80    80 0.0417     *        0.125     ns          \n2 Score A      O         80    80 0.00000123 ****     0.0000037 ****        \n3 Score B      O         80    80 0.00372    **       0.0111    *           \n\n\nAnd now to answer questions on the values we have\n\nWhat are the degrees of freedom of the ANOVA? Reported as “(X, Y)”. (, )\nWhat is the F-value? \nWhat is the p-value? Report this as 3 decimal places, unless it is under .001, in which case report “&lt; .001”. p =&lt; \nIs this a significant model? yesno\nWhat is the effect size, reported to 2dp? \nWhat is the adjusted p-value for group A & B? Again to 3dp unless it is under .001, in which case report “&lt; .001”. p &lt;= \nWhat is the adjusted p-value for group A & O? Again to 3dp unless it is under .001, in which case report “&lt; .001”. p =&lt; \nWhat is the adjusted p-value for group B & O? Again to 3dp unless it is under .001, in which case report “&lt; .001”. p &lt;= \n\nGreat. Now we need to write this in a fashion suitable for a report. Pay attention to where each value goes.\nA one-factor between-participants ANOVA revealed that test scores were significantly different between our robot groups (Robot A M = 58.10, SD = 6.45, Robot B M = 60.36, SD = 7.27; Robot O M = 63.60, SD = 7.22), F(2,237) = 12.52, p &lt; .001, η2 = 0.10. Posthoc tests using bonferroni correction found that group O yielded significantly higher scores than group A (padj &lt; .001), as well as higher scores than group B (padj &lt; .011). No significant difference was seen between groups A and B (padj = .125). This suggests that the more likelike robot Omega produces significantly higher test scores than robots Alpha and Beta."
  },
  {
    "objectID": "Worksheets/Worksheet_wk9.html#task-2-within-participant-anova",
    "href": "Worksheets/Worksheet_wk9.html#task-2-within-participant-anova",
    "title": "9. ANOVA and linear regression",
    "section": "Task 2: Within-participant ANOVA",
    "text": "Task 2: Within-participant ANOVA\nSo that was a between-participant ANOVA, where participants were only in one group. Not both. Now what if we wated to test how a factor can influence a participant. Perhaps levels of self-esteem over three different time points.\nWe can find this data in the datarium package, so another one to install! We don’t need to load it when we then run this next command\n\ndata(\"selfesteem\", package = \"datarium\")\n\nselfesteem\n\n# A tibble: 10 × 4\n      id    t1    t2    t3\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1  4.01  5.18  7.11\n 2     2  2.56  6.91  6.31\n 3     3  3.24  4.44  9.78\n 4     4  3.42  4.71  8.35\n 5     5  2.87  3.91  6.46\n 6     6  2.05  5.34  6.65\n 7     7  3.53  5.58  6.84\n 8     8  3.18  4.37  7.82\n 9     9  3.51  4.40  8.47\n10    10  3.04  4.49  8.58\n\n\nCurrently the data is in what is called wide format. We have a participant per row, and we have all their data associated in different columns. For a lot of analyses, we are better off with a long format or tidy format. For tidy data, each row represents a single observation, and the observations are grouped together into cases based on the value of a variable. Sounds confusing, let’s run some code that does this for us, and have a look at what changes\n\nselfesteem_long &lt;- selfesteem |&gt; \n  pivot_longer(cols = t1:t3, names_to = \"time\", values_to = \"score\")\n\nselfesteem_long\n\n# A tibble: 30 × 3\n      id time  score\n   &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n 1     1 t1     4.01\n 2     1 t2     5.18\n 3     1 t3     7.11\n 4     2 t1     2.56\n 5     2 t2     6.91\n 6     2 t3     6.31\n 7     3 t1     3.24\n 8     3 t2     4.44\n 9     3 t3     9.78\n10     4 t1     3.42\n# ℹ 20 more rows\n\n\nCompute some descriptive statistics like above and save it to a object called selfesteem_descriptives. Don’t forget to group by time\nCreate a boxplot\n\nselfesteem_long |&gt; \n  ggplot(aes(time, score)) +\n  geom_boxplot()\n\n\n\n\nLooks like we can expect a pretty big difference in self-esteem over time!\n\nmodel_selfesteem &lt;- anova_test(data = selfesteem_long, dv = score, wid = id, within = time)\n\nget_anova_table(model_selfesteem)\n\nANOVA Table (type III tests)\n\n  Effect DFn DFd      F        p p&lt;.05   ges\n1   time   2  18 55.469 2.01e-08     * 0.829\n\n\nThe self-esteem score was statistically significantly different at the different time points during the diet, F(2, 18) = 55.5, p &lt; 0.0001, eta2[g] = 0.83.\n\nF Indicates that we are comparing to an F-distribution (F-test); (2, 18) indicates the degrees of freedom in the numerator (DFn) and the denominator (DFd), respectively; 55.5 indicates the obtained F-statistic value\np specifies the p-value\nges is the generalized effect size (amount of variability due to the within-subjects factor) - eta-squared\n\n\npairwise_t_test(selfesteem_long,\n  score ~ time, paired = TRUE,\n  p.adjust.method = \"bonferroni\"\n  )\n\n# A tibble: 3 × 10\n  .y.   group1 group2    n1    n2 statistic    df           p p.adj p.adj.signif\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 score t1     t2        10    10     -4.97     9 0.000772     2e-3 **          \n2 score t1     t3        10    10    -13.2      9 0.000000334  1e-6 ****        \n3 score t2     t3        10    10     -4.87     9 0.000886     3e-3 **          \n\n\nAll the pairwise differences are statistically significant.\nHow would you write this up? Give it a go\n\n\n\n\n\n\nTip\n\n\n\n\n\nThe self-esteem score was statistically significantly different at the different time points, F(2, 18) = 55.5, p &lt; 0.0001, generalized eta squared = 0.82. Post-hoc analyses with a Bonferroni adjustment revealed that all the pairwise differences, between time points, were statistically significantly different (p &lt;= 0.05).\n\n\n\nBrilliant. We breezed through this second ANOVA because it is similar to the first but it just uses some different functions to arrive at the same point. The output values are the same type, just formatted differently.\nYou will note that we didn’t really check the assumptions of our ANOVAs"
  },
  {
    "objectID": "Worksheets/Worksheet_wk9.html#task-3-linear-regression",
    "href": "Worksheets/Worksheet_wk9.html#task-3-linear-regression",
    "title": "9. ANOVA and linear regression",
    "section": "Task 3: Linear regression",
    "text": "Task 3: Linear regression\nThis is taken from Glasgow’s excellent PsyTeachR materials.\nWe are analysising responses from the STARS Statistics Anxiety Survey. It was administered to students in the third-year statistics course in Psychology at the University of Glasgow. All responses are anonymised.\n“The STARS survey (Cruise, Cash, & Bolton, 19851) is a 51-item questionnaire, with each response on a 1 to 5 scale, with higher numbers indicating greater anxiety.”\nIt looks something like:\nExample STARS items\nUsing data from attendance at weekly drop-in sessions for extra support, let’s see whether a relationship between attendance and anxiety exists. We would expect that greater attendance of sessions results in lower statistical anxiety\nWe hypothesise that increased drop-in session attendance results in reduced anxiety scores.\n\n\nRows: 37 Columns: 52\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (52): ID, Q01, Q02, Q03, Q04, Q05, Q06, Q07, Q08, Q09, Q10, Q11, Q12, Q1...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 100 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): ID, n_weeks\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nData Tidying\nWe want to read in the two datafiles (L3_stars.csv, psess.csv) called stars and engage respectively.\nHave an explore of the data. The ID column is shared across the datasets (remember we experienced something similar last week), and the variable n_weeks in the psess.csv file tells you how many (out of eight) a given student attended. The rest of the data in stars are the results of the test.\nOur next step is to calculate a mean anxiety score for each student. We currently have 51 different answers to the scale items, and we just need one value per participant.\nCurrently the data is in what is called wide format. We have a participant per row, and we have all their data associated in different columns. For a lot of analyses, we are better off with a long format or tidy format. For tidy data, each row represents a single observation, and the observations are grouped together into cases based on the value of a variable. Sounds confusing, let’s run some code that does this for us, and have a look at what changes\n\nstars_long &lt;- stars |&gt; \n  pivot_longer(cols = Q01:Q51, names_to = \"Question\", values_to = \"Score\")\n\nhead(stars_long)\n\n# A tibble: 6 × 3\n     ID Question Score\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1     3 Q01          1\n2     3 Q02          1\n3     3 Q03          1\n4     3 Q04          1\n5     3 Q05          1\n6     3 Q06          1\n\n\nWhat changed? We went from a 37x52 dataset, to a 1887x3 dataset, where we now have ID as one column, Question (which was the columns going Q01, Q02,…, Q51) and the values from those columns in the Score column. We converted from a wide to a long format.\nThe convenience of having a long (or tidy) format is that we can calculate statistics (like means and SDs) far more conveniently.\nIn the code chunk below, replace the *** with the right variables\n\nstars_mean &lt;- stars_long |&gt; \n  na.omit() |&gt; #remove any non-responses\n  group_by(***) |&gt; \n  summarise(mean_anxiety = mean(***))\n\nThe next step is to join the two datasets together (stars_mean and engage) using inner_join() which we used in last week’s worksheet. Using the same code structure as last week, but replacing the variables as needed, can you get the same output below saved to an object called data?\n\n\n# A tibble: 37 × 3\n      ID mean_anxiety n_weeks\n   &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;\n 1     3         1.06       5\n 2     7         2.71       2\n 3    12         2.24       3\n 4    16         2.86       2\n 5    23         1.71       6\n 6    29         1.80       7\n 7    39         1.96       2\n 8    42         2.24       7\n 9    43         2.69       5\n10    44         1.92       4\n# ℹ 27 more rows\n\n\nWe can also run the below to get descriptive statistics for the overall sample. This can be useful for a future writeup of the research. Run the following code to get an easy summary of the data, make sure you understand what each line does and how it corresponds to the output\n\ndescriptives &lt;- data  |&gt; \n  summarise(mean_anx = mean(mean_anxiety, na.rm = TRUE),\n            sd_anx = sd(mean_anxiety, na.rm = TRUE),\n            mean_weeks = mean(n_weeks, na.rm = TRUE),\n            sd_weeks = sd(n_weeks, na.rm = TRUE))\n\n\n\nData Visualisation\nGreat. Now we have the data in the desired format, we need to plot the data to see if a general trend is present\nWe will use ggplot (as we have done before) to create a scatterplot of mean anxiety on the x-axis and n_weeks on the y-axis. You actually used almost identical code to create this in week 8 - Conducting a correlation\nIt should look like this:\n\ndata |&gt; \n  ggplot(aes(mean_anxiety, n_weeks)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWhat does this general trend show us? What do we see as a relationship in the data?\n\n\nConducting a linear regression\nNow we have tidied the data and visualised what we want to do, we can run the regression. We tidy the data to get it into the format required for the regression, and we plot the data to check some of the assumptions (linearity), and to see whether the data is suitable for regression modelling.\nFor linear regression we will use the function lm() which stands for linear model. It comes from base R and is commonly used for a lot of things. We use the same kind of syntax we used before in analysis DV ~ IV.\nWe run a linear regression using the following code to predict the anxiety from the number of weeks. So we want to know if increased attendance significantly reduces anxiety.\n\nmodel_stars &lt;- lm(mean_anxiety ~ n_weeks, data)\n\nsummary(model_stars)\n\n\nCall:\nlm(formula = mean_anxiety ~ n_weeks, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.96605 -0.29900 -0.08293  0.32864  1.03414 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.61215    0.17402  15.010  &lt; 2e-16 ***\nn_weeks     -0.11746    0.03392  -3.463  0.00143 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4929 on 35 degrees of freedom\nMultiple R-squared:  0.2552,    Adjusted R-squared:  0.2339 \nF-statistic: 11.99 on 1 and 35 DF,  p-value: 0.001428\n\n\nGreat, we have a summary of our regression analysis, and now we need to extract the relevant information from it. The output should look familiar to the t-test and correlation output from last session. I go through an example of this in this session’s lecture material for understanding an interpretation.\nAnswer the following questions based on the output\n\nThe estimate of the y-intercept for the model, rounded to two decimal places, is \nTo three decimal places, if the GLM for this model is y = mX=C, what is m? \nTo three decimal places, for each additional week attended, mean anxiety decreasesincreases by \nTo two decimal places, what is the overall F-ratio of the model? \nIs the overall model significant? yescannot tellno\nWhat proportion of the variance does the model explain (to 2dp)? \n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\nIn the summary table, this is the estimate of the intercept. 2.In the summary table, this is the estimate of mean_anxiety, i.e., the slope. 3.In the summary table, this is also the estimate of mean_anxiety, the slope is how much it decreases so you just remove the - sign.\nIn the summary table, the F-ratio is noted as the F-statistic.\nThe overall model p.value is .001428 which is less than .05, therefore significant.\nThe variance explained is determined by R-squared, you simply multiple it by 100 to get the percent. You should always use the adjusted R-squared value.\n\n\n\n\nCool. Before we move on, let’s look at the interpretation of the model, and specifically question 3 here. We say that the mean anxiety for those who attend no weeks is the same as the Y intercept (the answer to Q1), but for each week attended, anxiety decreases by .12. Hopefully you see how we interpret this from the data that we have.\n\n\nChecking assumptions\nRecall from the lecture that we cannot check the assumptions until we have our model residuals. It seems a bit backwards, create a model and check whether we can run the model, but that’s how it works unfortunately. The assumptions are\n\nThe outcome/DV is a interval/ratio level data\nThe predictor variable is interval/ratio or categorical (with two levels)\nAll values of the outcome variable are independent (i.e., each score should come from a different participant)\nThe predictors have non-zero variance\nThe relationship between outcome and predictor is linear\nThe residuals should be normally distributed\nThere should be homoscedasticity (homogeneity of variance, but for the residuals)\n\nAssumptions 1-3 are straightforward. We know this from the data we have and the design of the study. Assumption 4 simply means that there is some spread in the data - for example, there’s no point running a regression with age as a variable if all your participants are 20 years old. We can check this using the scatterplot we created already and we can see that this assumption is met. Phew!\nFor the rest of the assumptions, we’re going to use functions from the packages performance that make life a whole lot easier. We will use a package to do the assumption visualisations for us called performance which is quite handy. So first, install and load performance.\nWe specifically want check_model() from performance and it is run as:\n\ncheck_model(model_stars)\n\n\n\n\nLooking at the plots (you may wish to zoom into it for easier viewing), it even tells us what we are wanting to look for. All of them look okay, for Assumption 5, linearity, the plot suggests it’s not perfect but it looks good enough.\nFor Assumption 6, normality of residuals, the plot does suggest that the residuals might be normal, but we can double check this with check_normality() which runs a Shapiro-Wilk test.\n\ncheck_normality(model_stars)\n\nOK: residuals appear as normally distributed (p = 0.625).\n\n\nFor homoscedasticity, the plot looks mostly fine, but we can double check this with check_heteroscedasticity() and the result confirms that the data have met this assumption.\n\ncheck_heteroscedasticity(model_stars)\n\nOK: Error variance appears to be homoscedastic (p = 0.301).\n\n\nGreat. So we can be content that the model we created is suitable. It meets the assumptions and tests our hypothesis appropriately. Let us move on to the next step, determining the effect size. We did this for chi-squared and t-tests, anovas above, so let us conduct one for a linear regression too.\n\n\nEffect sizes\nFor a linear regression, we can use Cohen’s f2 (Selya, 2012)2 which is calculated through \\(F^2 = R^2/(1-R^2)\\). We have to calculate this ourselves which is relatively trivial.\nWe first want to extract the R2 value from our model (making sure it is the adjusted one)\n\nmodel_stars_summary &lt;- summary(model_stars) # save to an object\n\nr2 &lt;- model_stars_summary$adj.r.squared #extract the relevant value\n\nr2/(1-r2) #compute f2\n\n[1] 0.3053367\n\n\nAccording to Cohen’s (1988)3 guidelines, f2 ≥ 0.02, f2 ≥ 0.15, and f2 ≥ 0.35 represent small, medium, and large effect sizes, respectively.\nSo what size effect do we have? smallmediumlarge\n\n\nWriting up a linear regression result\nOkay, wow. We’ve done a lot of computation to get to this point. It is all worthless if we cannot articulate it in a meaningful manner for our dear reader. We want to revisit our descriptives object for means and SDs, our model output in model_stars_summary, and the effect size we calculated. For APA formatting, we report all values to 2 decimal places, except for p-values which are 3dp, unless it is under .001, then we report &lt;.001. Some of the values are filled in for you.\nA simple linear regression was conducted to determine whether statistics anxiety (mean = , SD = ) was predicted via attendance of drop-in sessions (mean = 4.54, SD = 2.42). The regression found a significant decrease in mean anxiety, F(1, 35) = , p = .001, R2adj = , f2 = , accounting for % of variance. Attendance was a significant predictor (β = -.12, p = 0.001.)"
  },
  {
    "objectID": "Worksheets/Worksheet_wk9.html#task-4-proof-anova-linear-regression",
    "href": "Worksheets/Worksheet_wk9.html#task-4-proof-anova-linear-regression",
    "title": "9. ANOVA and linear regression",
    "section": "Task 4: Proof: ANOVA == Linear Regression",
    "text": "Task 4: Proof: ANOVA == Linear Regression\nUsing the same data as the robot ANOVA\n\nmodel_robot_lm &lt;- lm(Score ~ Group, robot)\nmodel_robot_lm_summary &lt;- summary(model_robot_lm)\n\nmodel_robot_lm_summary\n\n\nCall:\nlm(formula = Score ~ Group, data = robot)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.3625  -4.6000  -0.2313   4.9000  15.4000 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  58.1000     0.7812  74.372  &lt; 2e-16 ***\nGroupB        2.2625     1.1048   2.048   0.0417 *  \nGroupO        5.5000     1.1048   4.978 1.23e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.987 on 237 degrees of freedom\nMultiple R-squared:  0.09557,   Adjusted R-squared:  0.08794 \nF-statistic: 12.52 on 2 and 237 DF,  p-value: 6.768e-06\n\n\n\nround(model_robot_lm_summary$fstatistic[[1]], 4) == round(summary(model_robot)[[1]][[\"F value\"]][[1]], 4) #excuse what looks messy, but if you decipher what I have written, I have extracted the F-value from both the original model (on the right) and the linear model (on the left) and tested their equivalence. It returns true because they are the exact same number\n\n[1] TRUE\n\n\nI won’t prove all the other numbers are the same, you can do that by looking at the summary outputs\nBut posthoc values are computed using emmeans like below. Check that the values are the same as in the ANOVA output.\n\nlibrary(emmeans)\n\nemmeans(model_robot_lm, pairwise ~ Group)\n\n$emmeans\n Group emmean    SE  df lower.CL upper.CL\n A       58.1 0.781 237     56.6     59.6\n B       60.4 0.781 237     58.8     61.9\n O       63.6 0.781 237     62.1     65.1\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast estimate  SE  df t.ratio p.value\n A - B       -2.26 1.1 237  -2.048  0.1032\n A - O       -5.50 1.1 237  -4.978  &lt;.0001\n B - O       -3.24 1.1 237  -2.930  0.0103\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\nemmeans(model_robot, pairwise ~ Group)\n\n$emmeans\n Group emmean    SE  df lower.CL upper.CL\n A       58.1 0.781 237     56.6     59.6\n B       60.4 0.781 237     58.8     61.9\n O       63.6 0.781 237     62.1     65.1\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast estimate  SE  df t.ratio p.value\n A - B       -2.26 1.1 237  -2.048  0.1032\n A - O       -5.50 1.1 237  -4.978  &lt;.0001\n B - O       -3.24 1.1 237  -2.930  0.0103\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nBecause we used emmeans rather than pairwise_t_tests (the latter won’t work on lm()) we get different outputs from the ANOVA section above, due to differences in how things are calculated. But we see we get the same numbers if we run emmeans here on both. Note that despite differences between emmeans and pairwise_t_tests the same interpretation would be made.\nTa-da! ANOVA is a linear regression, just masked. My advice is to use linear regressions in your own work, but know what an ANOVA is in case someone asks for one (in which case do it using lm anyway) or you come across one in a paper."
  },
  {
    "objectID": "Worksheets/Worksheet_wk9.html#other",
    "href": "Worksheets/Worksheet_wk9.html#other",
    "title": "9. ANOVA and linear regression",
    "section": "Other",
    "text": "Other\n\nThe trout ANOVA example from the slides\nData is simulated using Fry and Cox (1970) as a basis4\n\nset.seed(0603) #set the seed for replicability\n\n#generate some data, with length sampled from the normal distribution\ntrout &lt;- tibble(\n  waterspeed = c(rep(\"fast\", 20), rep(\"slow\", 20), rep(\"very slow\", 20)),\n  length = c(rnorm(20, 50, 10), rnorm(20, 60, 10), rnorm(20, 65, 10))\n) |&gt; \n  mutate(length = round(length, 2))\n\ntrout\n\n# A tibble: 60 × 2\n   waterspeed length\n   &lt;chr&gt;       &lt;dbl&gt;\n 1 fast         66.5\n 2 fast         46.5\n 3 fast         49.0\n 4 fast         59.4\n 5 fast         52.2\n 6 fast         77.0\n 7 fast         44.8\n 8 fast         39.6\n 9 fast         49.3\n10 fast         71.7\n# ℹ 50 more rows\n\ntrout |&gt; \n  ggplot(aes(waterspeed, length)) +\n  geom_boxplot()\n\n\n\naov(length ~ waterspeed, trout) |&gt; summary()\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nwaterspeed   2   1839   919.5   8.911 0.000429 ***\nResiduals   57   5881   103.2                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\neffectsize(aov(length ~ waterspeed, trout))\n\nFor one-way between subjects designs, partial eta squared is equivalent\n  to eta squared. Returning eta squared.\n\n\n# Effect Size for ANOVA\n\nParameter  | Eta2 |       95% CI\n--------------------------------\nwaterspeed | 0.24 | [0.08, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\npairwise_t_test(trout,\n  length ~ waterspeed, # indicate this is paired (within participants) data\n  p.adjust.method = \"bonferroni\" #adjust for bonferroni correction\n  )\n\n# A tibble: 3 × 9\n  .y.    group1 group2       n1    n2        p p.signif    p.adj p.adj.signif\n* &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;     &lt;int&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;       \n1 length fast   slow         20    20 0.384    ns       1        ns          \n2 length fast   very slow    20    20 0.000176 ***      0.000527 ***         \n3 length slow   very slow    20    20 0.0027   **       0.00809  **          \n\n\nThe bigger the fish, the slower it can swim. So smaller sized fish are more suitable for faster currents as they are not swept away.\n\n\nThe house regression example\nThe market historical data set of real estate valuation are collected from Sindian Dist., New Taipei City, Taiwan\n\nhouses &lt;- read_csv(\"Real estate.csv\")\n\nRows: 414 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (8): ID, transaction_date, house_age, distance_MRT, convenience_store_co...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhouses$unit_price |&gt; hist() # looks mostly normal (one very high house price)\n\n\n\nhouses_filt &lt;- houses |&gt; filter(unit_price &lt; 90)\n\n#can we use house_age as a predictor? It isn't a linear relationship, so no\nhouses_filt |&gt; \n  ggplot(aes(house_age, unit_price)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n#number of shops is more linear\nhouses_filt |&gt; \n  ggplot(aes(convenience_store_count, unit_price)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nlm(unit_price ~ convenience_store_count, houses_filt) |&gt; summary()\n\n\nCall:\nlm(formula = unit_price ~ convenience_store_count, data = houses_filt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-35.339  -7.098  -1.398   6.002  30.661 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              26.6567     0.8717   30.58   &lt;2e-16 ***\nconvenience_store_count   2.7138     0.1727   15.71   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.32 on 411 degrees of freedom\nMultiple R-squared:  0.3753,    Adjusted R-squared:  0.3738 \nF-statistic: 246.9 on 1 and 411 DF,  p-value: &lt; 2.2e-16\n\n#effect size\n0.3738/(1-0.3738) #compute f2\n\n[1] 0.5969339"
  },
  {
    "objectID": "Worksheets/Worksheet_wk9.html#footnotes",
    "href": "Worksheets/Worksheet_wk9.html#footnotes",
    "title": "9. ANOVA and linear regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCruise, R. J., Cash, R. W., & Bolton, D. L. (1985). Development and validation of an instrument to measure statistical anxiety. Proceedings of the American Statistical Association, Section on Statistical Education, Las Vegas, NV.↩︎\nSelya, A. S., Rose, J. S., Dierker, L. C., Hedeker, D., & Mermelstein, R. J. (2012). A practical guide to calculating Cohen’sf 2, a measure of local effect size, from PROC MIXED. Frontiers in psychology, 3, 111.↩︎\nCohen J. E. (1988). Statistical Power Analysis for the Behavioral Sciences. Hillsdale, NJ: Lawrence Erlbaum Associates, Inc↩︎\nFry, F. E. J., & Cox, E. T. (1970). A relation of size to swimming speed in rainbow trout. Journal of the Fisheries Board of Canada, 27(5), 976-978.↩︎"
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk4.html",
    "href": "Worksheets/Answers/Worksheet_wk4.html",
    "title": "4. Data visualization",
    "section": "",
    "text": "Three important things to remember:"
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk4.html#step-0-preparing-your-environment",
    "href": "Worksheets/Answers/Worksheet_wk4.html#step-0-preparing-your-environment",
    "title": "4. Data visualization",
    "section": "Step 0: Preparing your Environment",
    "text": "Step 0: Preparing your Environment\nFirt things first, open up a new R script and load in the Tidyverse library\n\n\n\n\n\n\nNeed a reminder?\n\n\n\n\n\n\nlibrary(tidyverse)\n\n\n\n\nIn addition, please download the following data files from Moodle and place them in your working directory.\n\nlanguage_exams_shorter.csv\nnettle_1999_climate.csv\ntitanic.csv\ncarprice.csv\n\nYou will also notice when completing the handout, we will also use a “built-in” dataset. These are datasets that come with R (or are loaded with different packages). They are a nice way to illustrate some of the features of R.\nTo see the list of pre-loaded data, execute the function data(). This will display the available datasets in the script editor.\n\ndata()\n\nThe data is already installed, you don’t need to load it with the data() function. You can just use the built-in datasets in your commands. Often, these datasets come with packages and are used to demonstrate examples in these packages."
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk4.html#step-1-two-plotting-systems-in-r-base-r-and-ggplot",
    "href": "Worksheets/Answers/Worksheet_wk4.html#step-1-two-plotting-systems-in-r-base-r-and-ggplot",
    "title": "4. Data visualization",
    "section": "Step 1: Two plotting systems in R: Base R and ggplot",
    "text": "Step 1: Two plotting systems in R: Base R and ggplot\nThere are fundamentally two methods for plotting in R, one that we touched on last week which was base R, and the other is via a library that comes bundled in the tidyverse called ggplot2. ggplot is currently the most widely used plotting system in R, and it can do lots of really cool things.\nBase R is called as such because it only uses functions that come with R in its most basic form. These are plots we can create without any additional packages required. You may come across examples of base plots in online resources such as StackOverflow when you go looking for help, so it is worthwhile understanding how they are created. Ultimately, you will be creating almost all your plots and graphics using ggplot, so I won’t spend much time on base R."
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk4.html#step-2-base-r",
    "href": "Worksheets/Answers/Worksheet_wk4.html#step-2-base-r",
    "title": "4. Data visualization",
    "section": "Step 2: Base R",
    "text": "Step 2: Base R\nFirst, load in the data for this task and have a quick look at it\n\nlanguage_exams &lt;- read_csv(\"language_exams_shorter.csv\")\n\nRows: 10 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): age_cohort, student\ndbl (5): exam_1, exam_2, exam_3, level, age\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(language_exams)\n\n# A tibble: 6 × 7\n  age_cohort exam_1 exam_2 exam_3 level student    age\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1 15-17          57     52     59     1 Alesha      16\n2 15-17          92     76     81     1 Chiara      15\n3 15-17          63     60     66     1 Davina      17\n4 15-17          40     24     30     1 Sallie      16\n5 18-22          25     13     19     1 Roxanne     18\n6 18-22          84     76     82     1 Maariyah    21\n\n\nNext, we want to get the mean scores of the variables exam_1, exam_2, and exam_3. We can do this using the summarise() function that comes from dplyr (part of Tidyverse). Take a look at what the help tab has to say about this function by typing ?summarise, or by going to the Help tab in the top-right, and using the search bar to look for summarise.\nIt tells us that “summarise() creates a new data frame. It returns one row for each combination of grouping variables; if there are no grouping variables, the output will have a single row summarising all observations in the input. It will contain one column for each grouping variable and one column for each of the summary statistics that you have specified.”\nIf we then scroll to the bottom of the help pane, we get to the examples - have a look at the very first one, which I paste below. Make sure you know where the examples are, they are invaluable for understanding how new code works.\nNote: mtcars is one of these preloaded datasets I just mentioned, and see how it lets you run example code very easily.\n\nmtcars %&gt;%\n  summarise(mean = mean(disp), n = n())\n\n      mean  n\n1 230.7219 32\n\n\nSo we can run example code, and it gives us an output. How do we go about understanding what it has just done? In this section of code, we have four components, and only one of which we have seen before. Let me break it down for you:\n\nmtcars - a prepackaged dataset, and was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models). So it’s just a small “toy” datatset we can test ideas on.\nsummarise() - a new function that creates summary tables from larger datasets\nmean = mean(disp) - we have used mean() before. Here, we are telling summarise() to get the mean of the variable disp, and show it in a column called mean.\nn = n() - this is asking for the Number of observations (rows) in the dataset. n for number.\n\nCan you use this new knowledge to construct a summary table that will give you an output of the mean of exam_1, exam_2, and exam_3?\n\n\n\n\n\n\nanswer\n\n\n\n\n\n\nmean_scores &lt;- language_exams |&gt; \n  summarise(mean_1 = mean(exam_1),\n            mean_2 = mean(exam_2),\n            mean_3 = mean(exam_3))\n\nmean_scores\n\n# A tibble: 1 × 3\n  mean_1 mean_2 mean_3\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   63.9   53.8   60.9\n\n\nNotice how after each comma I start a new line, this is for ease of reading, and R knows to look at the next line if it seems a comma at the end of a line.\n\n\n\nOkay, so we have our summary table made from summarise() but it isn’t quite in the right format to use in plots, so let’s coerce the mean_scores object to a numeric vector:\n\nclass(mean_scores) # to show it's a tibble\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nmean_scores &lt;- mean_scores |&gt; \n  as.numeric() #quite literally, telling R to read it as numbers and not a tibble.\n\nclass(mean_scores) #now it's numeric!\n\n[1] \"numeric\"\n\n\n\nBarplots\nNow let’s use it in a barplot\n\nbarplot(mean_scores)\n\n\n\n\nYou should now see the bar chart in the Plot tab of the Files, Plots, etc. pane, bottom right.\nBy default, the plot is vertical, but we can change this.\nCompare the following output to the output produced by the previous command.\n\nbarplot(mean_scores, horiz = TRUE) \n\n\n\n\nThis is all very good, but we need to add names to our columns, of course. We can do this by adding the names.arg() argument as below.\n\nbarplot(mean_scores, names.arg = c(\"Exam 1\", \"Exam 2\", \"Exam 3\"))\n\n\n\n\nWe can go on and add titles and axis labels like so. Or change colours and make it look fancy, but really Base plots should be used for very fast, basic plots for you to check things quick. We will cover these ideas properly in the ggplot section\n\nbarplot(\n  mean_scores,\n  names.arg = c(\"1\", \"2\", \"3\"),\n  # Note: We change the column names to avoid duplication\n  main = \"Performance on the exams\", # title\n  xlab = \"Exams\", #x axis\n  ylab = \"Scores\" #y axis\n)\n\n\n\nbarplot(mean_scores, col = \"turquoise\", border = \"steelblue\")\n\n\n\nbarplot(\n  mean_scores,\n  col = \"turquoise\",\n  border = \"steelblue\",\n  names.arg = c(\"1\", \"2\", \"3\"),\n  # Note: We change the column names to avoid duplication\n  main = \"Performance on the exams\",\n  xlab = \"Exams\",\n  ylab = \"Scores\"\n)\n\n\n\n\n\n\nHistograms\nLet’s quickly look at histograms in Base R. We start by creating two new objects, exam_1 and ages, both extracted from our language_exams data.\n\nexam_1 &lt;- language_exams$exam_1\n\nages &lt;- language_exams$age\n\nA histogram can be created using the hist() function. This plots the frequency of the variable ages in the our language exams dataset. You can play with colours if you want here, it’s the same as with barplot\n\nhist(ages)\n\n\n\n\nWe can change the number of breaks, i.e. the breakpoints between histogram cells. This is useful as sometimes the default breakpoint obscures the data.\n\nhist(ages, breaks = 20)"
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk4.html#lineplots",
    "href": "Worksheets/Answers/Worksheet_wk4.html#lineplots",
    "title": "4. Data visualization",
    "section": "Lineplots",
    "text": "Lineplots\nTo illustrate line plots in Base R, let’s start by generating some data and creating three variables. We will create our own tibble from ideas we used in previous weeks. I’m leaving this code intentionally uncommented and a bit barebones - I would like for you to really read the code, use the help pane to understand what it is you are doing, test different components, etc.\nDon’t just run the code and assume it works - like I’ve said before, if you do that, you’ll get really good at running Matthew’s code, but not your own.\n\ntest_data &lt;- tibble(var_1 = c(1:20),\n                    var_2 = var_1^2,\n                    var_3 = 4 * var_2)\n\nTo test your understanding, can you mutate() the test_data tibble and add a new column called var_4 which is var_1 divided by var_3? Make sure to assign it as test_data still!\n\n\n\n\n\n\nanswer\n\n\n\n\n\n\ntest_data &lt;- test_data |&gt; \n  mutate(var_4 = var_1 / var_3)\n\ntest_data\n\n# A tibble: 20 × 4\n   var_1 var_2 var_3  var_4\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     1     1     4 0.25  \n 2     2     4    16 0.125 \n 3     3     9    36 0.0833\n 4     4    16    64 0.0625\n 5     5    25   100 0.05  \n 6     6    36   144 0.0417\n 7     7    49   196 0.0357\n 8     8    64   256 0.0312\n 9     9    81   324 0.0278\n10    10   100   400 0.025 \n11    11   121   484 0.0227\n12    12   144   576 0.0208\n13    13   169   676 0.0192\n14    14   196   784 0.0179\n15    15   225   900 0.0167\n16    16   256  1024 0.0156\n17    17   289  1156 0.0147\n18    18   324  1296 0.0139\n19    19   361  1444 0.0132\n20    20   400  1600 0.0125\n\n\n\n\n\nWe can use the plot() function to display variables 1 and 2.\n\nplot(test_data$var_1, test_data$var_2)\n\n\n\n\nThere are many types of line. Try out changing the line types by modifying the value for the type argument.\n\nplot(test_data$var_1, test_data$var_2, type = \"p\") # points\n\n\n\nplot(test_data$var_1, test_data$var_2, type = \"b\") # both points and lines\n\n\n\nplot(test_data$var_1, test_data$var_2, type = \"o\") # overplots points and lines\n\n\n\nplot(test_data$var_1, test_data$var_2, type = \"l\") # lines\n\n\n\nplot(test_data$var_1, test_data$var_2, type = \"n\") # no points or lines\n\n\n\n\noften we need to display more than one line. This is how it works in Base R\nFirst, we plot one line. (We’re also adding color and labels.)\n\nplot(\n  test_data$var_1,\n  test_data$var_2,\n  type = \"b\",\n  frame = FALSE,\n  pch = 20,\n  col = \"turquoise\",\n  xlab = \"Age\",\n  ylab = \"Scores\"\n)\n\n#Now, we can add a second line on top.\n\nlines(test_data$var_1,\n      test_data$var_2,\n      pch = 20,\n      col = \"darkblue\",\n      type = \"b\")\n\n\n\n# Let's add a legend to the plot to make it clear what the lines refer to.\n# Of course there are more compexities to base plots and how they look, but again, ggplot is where we are going to be heading.\n\n\nlegend(\n  \"topleft\",\n  legend = c(\"Group A\", \"Group B\"),\n  col = c(\"turquoise\", \"darkblue\"),\n  lty = 1:2,\n  cex = 0.8\n)\n\n\n\n\nOkay, enough of base R plotting. We’ve seen how it works with three basic types of plot. We could go further and do scattergraphs with plot() and we just don’t try and add in line types, or we could do boxplots with boxplot() but I would rather show you these ideas and to revisit the other plots using ggplot. Why? It is far more powerful, lends itself to the tidy process and just generally is better. Once you start making them, I wouldn’t be surprised if you start seeing ggplot-made graphs in publications you read, I know I see them all the time!"
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk4.html#step-3-ggplot",
    "href": "Worksheets/Answers/Worksheet_wk4.html#step-3-ggplot",
    "title": "4. Data visualization",
    "section": "Step 3: ggplot",
    "text": "Step 3: ggplot\nggplot is a powerful system for producing elegant graphics. ggplot is included in the tidyverse package, so if you have loaded the latter you’re good to go. Alternatively, you can load the ggplot2 package directly as follows.\nTo learn more about the ggplot, I recommend the textbooks above, but also the following websites.\nAn excellent reference for ggplot: https://ggplot2.tidyverse.org/index.html\nA useful cheat sheet: https://github.com/rstudio/cheatsheets/blob/main/data-visualization.pdf\nGorgeous graphs in ggplot: https://r-graph-gallery.com/ggplot2-package.html\nThe gg in ggplot means grammar of graphics (Wickham, 2010). This is a system for mapping variables in a dataset to properties of a plot (e.g., shape, size, color position).\nWe will focus on using ggplot() to produce our visualizations.\nEvery time we use the ggplot command we need to specify three things (at least).\n\nThe dataset containing all the data. This can be a tibble, data frame, vector, etc.\nThe set of aesthetic mappings (aes). These describe how variables in the dataset are mapped to visual properties (aesthetics) of the type of graph we want to produce (geometric objects). Aesthetics is used to indicate x and y variables, to control the color, the size or the shape of points, the height of bars, and so forth.\nThe set of layers that render the aesthetic mappings. This is usually done according to prespecified geometric objects (geoms) such as lines, points, bars. The geometry defines the type of graphics we wish to plot (histogram, bar chart, boxplot, etc.)\n\nIn other words, we need to tell R what data to use for the plot, the type of graph, and the mapping between the visual cue and the variable (position, color, fill, etc.)\nTo understand how this work in practice, let’s start with a simple example.\nThe first argument accepted is the data, and then we provide the mapping and tell what we want the x and y axes to be.\n\nggplot(language_exams,\n       mapping = aes(x = exam_1, y = exam_2))\n\n\n\n\nBut wait, what do you see in the Plot tab? A blank canvas, right? This is because we did not specify the type of geometric object (geom). We need to add an additional line to specify the type of plot that we want\n\nScatter plot\n\nggplot(language_exams,\n       mapping = aes(x = exam_1, y = exam_2)) +\n  geom_point() # for a scatter plot\n\n\n\n\nAs you can see in your output, you should now see a scatterplot with the data from exam_1 and exam_2.\n\n\nLine plot\nIf you prefer a different type of plot, just change the geom. Below, we now produce a line plot.\n\nggplot(language_exams,\n       mapping = aes(x = exam_1, y = exam_2)) +\n  geom_line() # for a line plot\n\n\n\n\nThe funadamental difference between base and ggplot in terms of code layout, is that base in just “drawing thing” whereas ggplot attempts to make a coherent package where you are writing a little section of code that layers different components to create complex plots. Ggplot relies on an underlying dataframe, and encourages you to have your data in the correct format (nice and tidy) before plotting, whereas base R can be bodged quite a bit resulting in really messy sections of code.\nThe sequential nature of layering a ggplot figure is quite simplistic and I find easier to alter and manage. For further plots, let us pipe in the data frame to ggplot for improved readability.\nWe can also specify, in the mapping aes, what shape the different points in the scatterplot should have. For example, we can use a third variable (non-numeric) as a shape or as a color.\n\nlanguage_exams |&gt; \n  ggplot(mapping = aes(x = exam_1, y = exam_2, shape = age_cohort)) + \n  geom_point()\n\n\n\n\nand\n\nlanguage_exams |&gt; \n  ggplot(mapping = aes(x = exam_1, y = exam_2, color = age_cohort)) +\n  geom_point()\n\n\n\n\nOne more thing. We actually don’t need to write mapping, nor do we need to spell out x and y. By default, ggplot will assume that the first function after you specify the dataset (language_exams) is the mapping (aes), and that the first two arguments with the aes() are the x-axis and y-axis respectively.\nCompare the following and see that they are functionally equivalent:\n\nlanguage_exams |&gt; \n  ggplot(mapping = aes(x = exam_1, y = exam_2)) + geom_point() #explicit\n\n\n\nlanguage_exams |&gt; \n  ggplot(aes(exam_1, exam_2)) + geom_point() #condensed\n\n\n\n\nWe will use the condensed version of writing, because why wouldn’t we?"
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk4.html#step-4-histograms-and-frequency-polygons-with-ggplot",
    "href": "Worksheets/Answers/Worksheet_wk4.html#step-4-histograms-and-frequency-polygons-with-ggplot",
    "title": "4. Data visualization",
    "section": "Step 4: Histograms and frequency polygons with ggplot",
    "text": "Step 4: Histograms and frequency polygons with ggplot\nLet’s do some plotting with ggplot. We begin with histograms.\nWe first load the data from the Nettle (1999) book about language diversity and create a new object called languages.\n\nlanguages &lt;- read_csv(\"nettle_1999_climate.csv\")\n\nRows: 74 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Country\ndbl (4): Population, Area, MGS, Langs\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nHistograms are plotted if we use geom_histogram() in the ggplot() command.\n\nlanguages |&gt; \n  ggplot(mapping = aes(x = Langs)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNotice that we only supply an x axis for a histogram, because it does the y-axis automatically based on count data.\n\nThemes\nLet’s pause for a second to consider themes(). In ggplot, we can make many, many, many, many detailed changes to our graphs, which we cannot cover here. Check out the graph gallery here. https://r-graph-gallery.com/ggplot2-package.html\nBut we can also make bigger changes easily by adding themes() to our ggplot() commands.\nLet’s compare the following four types of themes. (There are many other themes.)\n\nlanguages |&gt; \n  ggplot(mapping = aes(x = Langs)) + \n  geom_histogram() # No theme\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nlanguages |&gt; \n  ggplot(mapping = aes(x = Langs)) + \n  geom_histogram() + \n  theme_gray() # Same as before, the default\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nlanguages |&gt; \n  ggplot(mapping = aes(x = Langs)) + \n  geom_histogram() + \n  theme_minimal()   # Theme minimal\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nlanguages |&gt; \n  ggplot(mapping = aes(x = Langs)) + \n  geom_histogram() + \n  theme_classic()   # Theme classic\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nBack to histograms\nWe can also format the different elements of the histogram. Below, we change the widths of the bins and later add some color, too. Again, compare what the commands do.\n\nlanguages |&gt; \n  ggplot(aes(Langs)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nlanguages |&gt; \n  ggplot(aes(Langs)) + \n  geom_histogram(binwidth = 50)\n\n\n\nlanguages |&gt; \n  ggplot(aes(Langs)) + \n  geom_histogram(binwidth = 50,\n                 color = 'turquoise4',\n                 fill = 'paleturquoise')\n\n\n\n\nSometimes you might prefer using a frequency polygon to display data rather than a histogram. For this, just use the geom_freqpoly() command.\nCompare. First, a histogram. (You can see the plot above.)\n\nlanguages |&gt; \n  ggplot(aes(Langs)) + geom_histogram(binwidth = 50,\n                                      color = 'turquoise4',\n                                      fill = 'paleturquoise') \n\n\n\n\nThen, a frequency polygon.\n\nlanguages |&gt; \n  ggplot(aes(Langs)) + geom_freqpoly(binwidth = 50,\n                                     color = 'turquoise4',\n                                     fill = 'paleturquoise') \n\nWarning in geom_freqpoly(binwidth = 50, color = \"turquoise4\", fill =\n\"paleturquoise\"): Ignoring unknown parameters: `fill`\n\n\n\n\n\n\n\nBar charts\nLet’s try out bar charts now. Again, we load data first. This comes from the Andrews (2021) textbook. Make sure to load it in, and have a look at the data to get an idea for it.\n\ntitanic &lt;- read_csv(\"titanic.csv\")\n\nRows: 1309 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): name, survived, sex, passengerClass\ndbl (1): age\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nHow many columns?\nHow many rows?\nWhat kind of data is in the object?\n\nTo do bar charts, we use geom_bar().\nCompare what the different commands do.\n\ntitanic |&gt; \n  ggplot(aes(passengerClass)) + \n  geom_bar()\n\n\n\ntitanic |&gt; \n  ggplot(aes(passengerClass, fill = survived)) + \n  geom_bar()\n\n\n\ntitanic |&gt; \n  ggplot(aes(passengerClass, fill = survived)) + \n  geom_bar(position =   \"dodge\")\n\n\n\n\nAs you can see, we can use nominal categories as fill in aesthetic mapping (fill = survived), and we can manipulate the position of the bars (position = “dodge”).\nLet’s more data before we illustrate further. This dataset is also from Andrew (2021), it displays information about cars (lots of them).\n\ncar_prices &lt;- read_csv(\"carprice.csv\")  \n\nNew names:\nRows: 48 Columns: 10\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): Type dbl (9): ...1, Min.Price, Price, Max.Price, Range.Price, RoughRange,\ngpm100,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\nThe following command creates a bar chart visualizing the prices (y-axis) according to type of car (x-axis).\n\ncar_prices |&gt; \n  ggplot(aes(Type, Price)) + \n  geom_bar(stat = \"identity\")\n\n\n\n\nWe can also group the data to further explore. The command below creates a new object called cars_prices_grouped, loads the car price data, groups it by car type, and calculates the mean prices for each car type.\nWe’ve seen something as complex as the below before, but take a look and make sure you know what’s happening, on the LHS and RHS of each pipe.\n\ncars_prices_grouped &lt;- car_prices %&gt;%\n  group_by(Type) %&gt;%\n  summarise(Price = mean(Price))\n\nIf you now type the name of the new object cars_prices_grouped, you will get a tibble with the average price.\n\ncars_prices_grouped # Compare to car_prices above\n\n# A tibble: 6 × 2\n  Type    Price\n  &lt;chr&gt;   &lt;dbl&gt;\n1 Compact  12.8\n2 Large    24.3\n3 Midsize  21.8\n4 Small    10.0\n5 Sporty   19.4\n6 Van      18.3\n\n\nWe can now do another bar chart with the grouped data.\n\ncars_prices_grouped |&gt; \n  ggplot(aes(Type, Price)) + \n  geom_bar(stat = \"identity\")\n\n\n\n\nWe can even combine the grouping process into the plotting too, bear with me on this next chunk of code, it’ll be the longest pipeline we’ve done so far\n\ncar_prices  |&gt; \n  group_by(Type)  |&gt; \n  summarise(Price = mean(Price)) |&gt; \n  ggplot(aes(Type, Price)) + \n  geom_bar(stat = \"identity\")\n\n\n\n\nThree pipes and a plot, that’s a lot. But break it down, compare it against the previous lines of code we have seen, and can you understand exactly how we went from the dataset car_prices through to a barplot? Could you take the above code and split it out into the two separate actions again?\nWhy might we want to pipe all these different commands together? What benefit does it give us?\n\n\nBoxplots\nFor the boxplots, we will use the built-in dataset ToothGrowth.\nFirst, let’s convert the variable dose from a numeric to a factor variable. These are categorical variables that can be either numeric or string variables and that can be used more easily in different types of graphics. We can do this in one of two ways, there is the base option which is the first line but commented out (as it is only to show you), and the tidy version.\n\n#ToothGrowth$dose &lt;- as.factor(ToothGrowth$dose) ## just as an example\n\nToothGrowth &lt;- ToothGrowth |&gt; \n  mutate(dose = as.factor(dose))\n\nIn an ideal world, we want to keep our writing style either in base or in tidy, I will teach you tidy because it means you can pipe things together and have really well-written, easily understandable code that can be run using one button press, rather than multiple. It helps to keep our code programmatic, logical, and easily understood where multiple functions are used to complete one meta-action.\nIn the above, the base version may seen to be more easily written, but when things get more complicated, then tidy is better. It is best to understand the priciples now, because it’ll be easier to carry out in your own analyses later on. I am not teaching you anything that I don’t personally do in my own coding. There is no pedagogically-motivated reason for me to teach Tidy over Base, beyond that the Tidy style is one of the most common ways of writing R code.\nLet’s look at generating a boxplot\n\nToothGrowth |&gt; \n  ggplot(aes(x = dose, y = len)) + \n  geom_boxplot()\n\n\n\n\nTo make our life a bit easier, let’s assign the command above (which produces a plot) to a new object called our_boxplot. This command creates an object called our_boxplot, which consists of, well, a boxplot.\n\nour_boxplot &lt;- ggplot(ToothGrowth, aes(x = dose, y = len)) + \n  geom_boxplot()\n\nIf you now run this command, you create the boxplot.\n\nour_boxplot\n\n\n\n\nTo rotate a boxplot, just coord_flip()to your boxplot, as in the example below.\n\nour_boxplot + coord_flip()\n\n\n\n\nNotched box plots are also useful. The notch refers to the narrowing around the median. You can create a notched box plots as follows. Did you notice that there is an outlier? (See above the top whisker.) By default, outliers are in the color of the box. But we can change color, shape and size of the outlier.\nLet’s try out different shapes. See how the different values for the outlier.shape argument affect the plot. What happens when you run the following commands?\n\nToothGrowth |&gt; \n  ggplot(aes(x = dose, y = len)) +\n  geom_boxplot(\n    outlier.colour = \"darkblue\",\n    outlier.shape = 0, # square\n    outlier.size = 4)\n\n\n\nToothGrowth |&gt; \n  ggplot(aes(x = dose, y = len)) +\n  geom_boxplot(\n    outlier.colour = \"darkblue\",\n    outlier.shape = 1, # circle\n    outlier.size = 4)\n\n\n\n\nTry out some other numbers for outlier.shape, what do they produce?\nThe function stat_summary() can be used to add mean points to a box plot, as in the following command.\n\nour_boxplot + stat_summary(\n  fun.y = mean,\n  geom = \"point\",\n  shape = 23,\n  size = 4\n)\n\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\nℹ Please use the `fun` argument instead.\n\n\n\n\n\nWe can add points to a box plot by using the functions geom_dotplot() or geom_jitter().\nIn the following example, our box plot also has a dot plot.\n\nour_boxplot + \n  geom_dotplot(binaxis = 'y',\n               stackdir = 'center',\n               dotsize = 0.5)\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\nAnd in this example, it has jittered points with 0.2 degree of jitter in x direction. The jitter geom adds a bit of random variation to the location of each point. This is useful when too many points are overlapping.\n\nour_boxplot + \n  geom_jitter(shape = 16, position = position_jitter(0.2))\n\n\n\n\nWe can also use special types of boxplots that combine a density plot with the boxplot. These are called violin plots, becasue they look like violins, if you squint a lot.\n\nToothGrowth |&gt; \n  ggplot(aes(x = dose, y = len)) + \n  geom_violin()\n\n\n\n\nThese plots show you where the central tendency is and how the distribution sits.\n\n\nScatterplots again\nLet’s now turn to scatterplots. Again, let’s some data, this time the Nettle (1999) data about language diversity, used in Winter (2019).\n\nlanguages &lt;- read_csv(\"nettle_1999_climate.csv\")\n\nRows: 74 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Country\ndbl (4): Population, Area, MGS, Langs\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe geom_point() command creates scatterplots. This first one below uses points.\n\nggplot(languages, aes(MGS, Langs)) + geom_point()\n\n\n\n\nBut this one uses the text, drawn from the Country variable in our dataset languages.\n\nggplot(languages, aes(MGS, Langs, label = Country)) + geom_text()\n\n\n\n\nLet’s load some additional data.\n\nlanguage_exams &lt;- read_csv(\"language_exams_shorter.csv\")\n\nRows: 10 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): age_cohort, student\ndbl (5): exam_1, exam_2, exam_3, level, age\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIn the following scatterplots, we will use the variable age cohort for color.\n\nlanguage_exams |&gt; \n  ggplot(aes(x = exam_1, y = exam_2, color = age_cohort)) + \n  geom_point()\n\n\n\n\nThere are many parameters you can add, delete or edit in your plots. The cheat sheet is very helpful in that regard.\nBy default, the size of your points are in size 2. You can see this by comparing the scatterplot above, which doesn’t have a size specification, to the one created by the following command. (The plot looks the same, so we won’t plot this here.)\n\nlanguage_exams |&gt; \n  ggplot(aes(x = exam_1, y = exam_2, color = age_cohort)) + \n  geom_point(size = 2)\n\n\n\n\nLet’s play around with size and shapes in scatterplots. See what happens when you run these commands.\n\nlanguage_exams |&gt; \n  ggplot(aes(x = exam_1, y = exam_2, color = age_cohort))   +\n  geom_point(size = 3)\n\n\n\nlanguage_exams |&gt; \n  ggplot(aes(x = exam_1, y = exam_2, color = age_cohort)) + \n  geom_point(size = 4)\n\n\n\n\nAnd now let’s try out different shapes. How do the commands change your plots?\n\nlanguage_exams |&gt; \n  ggplot(aes(x = exam_1, y = exam_2, color = age_cohort)) +\n  geom_point(size = 3, shape = 'triangle') # Same as writing 2\n\n\n\nlanguage_exams |&gt; \n  ggplot(aes(x = exam_1, y = exam_2, color = age_cohort)) +\n  geom_point(size = 4, shape = 'diamond') # Same as writing 5\n\n\n\nlanguage_exams |&gt; \n  ggplot(aes(x = exam_1, y = exam_2, color = age_cohort)) +\n  geom_point(size = 3, shape = 'square') # Same as writing 0\n\n\n\n\nWe can also use labels for data points.\n\nlanguage_exams |&gt; \n  ggplot(aes(x = exam_1,\n             y = exam_2,\n             label = student,\n             color = age_cohort)) + \n  geom_point(size = 3, shape = 'triangle') +\n  geom_text(size = 4)\n\n\n\n\nThe geom_text_repel() function is useful if we intend to use labels.\nWe need to install and load it first. It comes from a package called ggrepel - can you remember how to install and load libraries? Check on previous week’s content if you can’t remember exactly.\n\n#|echo: false\n\n\nlibrary(ggrepel)\n\n\nlanguage_exams |&gt; \n  ggplot(aes(x = exam_1,\n             y = exam_2,\n             label = student,\n             color = age_cohort)) + \n  geom_point(size = 3) +\n  geom_text_repel(size = 4, segment.alpha = 2)"
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk4.html#step-5-saving-plots",
    "href": "Worksheets/Answers/Worksheet_wk4.html#step-5-saving-plots",
    "title": "4. Data visualization",
    "section": "Step 5: Saving plots",
    "text": "Step 5: Saving plots\nThis is the ggsave() command. Let’s try saving our plots in a few different file formats (png, pdf, jpeg). Have a look at the working directory to see if you can find your three new files.\n\nggsave('our_plot.png', width = 8, height = 6)\n\nggsave('our_plot.pdf', width = 8, height = 6)\n\nggsave('our_plot.jpeg', width = 8, height = 6)\n\nYou can also save plots with specific resolutions. They default is dpi = 300.\nAlternatively, you can also write string input: “retina” (dpi 320), “print” (dpi 300), or “screen” (dpi 72). Have a go at the following.\n\nggsave(\n  'our_plot_300.jpeg',\n  width = 8,\n  height = 6,\n  dpi = 300\n)\n\nggsave(\n  'our_plot_screen.jpeg',\n  width = 8,\n  height = 6,\n  dpi = 'screen'\n)\n\nggsave(\n  'our_plot_retina.jpeg',\n  width = 8,\n  height = 6,\n  dpi = 'retina'\n)\n\nThis may have seemed like a deep dive into ggplot, but honestly, we have only touched very lightly on ggplot. I recommend you check out online materials and start exploring! The best thing is that components layer upon each other, and you can learn and add new things as you go. The tidy structure of the syntax also means things can be changed or swapped out easily."
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk4.html#step-6-colours",
    "href": "Worksheets/Answers/Worksheet_wk4.html#step-6-colours",
    "title": "4. Data visualization",
    "section": "Step 6: Colours",
    "text": "Step 6: Colours\nIn R, you can either specify colors by writing their names (e.g., “mistyrose”) or you can write the hexadecimal code (#FFE4E1).\nYou can try out compare the following.\nThe first uses the color names. The second uses the same colors but refers to them hexadecimal code.\n\nbarplot(c(2,5), col=c(\"paleturquoise\", \"mistyrose\"))\n\n\n\nbarplot(c(2,5), col=c(\"#30D5C8\", \"#FFE4E1\"))\n\n\n\n\nThe following website provides color names and a hex code finder:\nhttps://r-graph-gallery.com/ggplot2-color.html"
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk4.html#take-home-task",
    "href": "Worksheets/Answers/Worksheet_wk4.html#take-home-task",
    "title": "4. Data visualization",
    "section": "Take home task",
    "text": "Take home task\n\nCan you create a plot using the language_exams object to draw a scatter plot between exam_1 and exam_2 scores?\n\n\nlanguage_exams |&gt; \n  ggplot(aes(exam_1, exam_2)) +\n  geom_point()\n\n\n\n\n\nDraw a boxplot using ToothGrowth to show the differences between the supplement type and tooth length.\n\nHint: take a look at ?ToothGrowth to see what each column means.\n\nToothGrowth |&gt; \n  ggplot(aes(supp, len)) +\n  geom_boxplot()\n\n\n\n\n\nCan you update the code for question 2 in order to plot only the dose differences for the OJ supplement? Hint, you’ll need to use filter on the dataset before creating the plot. We learnt about filter() in last week’s worksheet, or look at ?filter (second option, the one from dplyr “Keep rows that match a condition”) and check out examples at the bottom.\n\nThe plot should only show data from OJ tests for each level of dose.\n\nToothGrowth |&gt; \n  filter(supp == \"OJ\") |&gt;  # keep only content OJ observations\n  ggplot(aes(dose, len)) +\n  geom_boxplot()\n\n\n\n\n\nCan you turn this into a violin plot? What do you think is better for commuicating the information?\n\n\nToothGrowth |&gt; \n  filter(supp == \"OJ\") |&gt; \n  ggplot(aes(dose, len)) +\n  geom_violin()\n\n\n\n\nAs for which is better, it’s subjective and depends on what you need to convey as part of your narrative."
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk2_Answers.html",
    "href": "Worksheets/Answers/Worksheet_wk2_Answers.html",
    "title": "2. Data management and data wrangling",
    "section": "",
    "text": "Three important things to remember:"
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk2_Answers.html#step-0-installing-tidyverse",
    "href": "Worksheets/Answers/Worksheet_wk2_Answers.html#step-0-installing-tidyverse",
    "title": "2. Data management and data wrangling",
    "section": "Step 0: Installing tidyverse",
    "text": "Step 0: Installing tidyverse\nI mentioned tidyverse in the lecture, and now we will intall and load it, before using it (mainly for pipes!)\nAs a one-off (on a per machine basis) the first command only needs to be run when we first want a package. As noted before, there are thousands of functions available to R, having them all pre-packaged would break your computer and you don’t need every single one.\n\ninstall.packages(\"tidyverse\")\n\nThere is little to know about this at this stage, as the function does a lot of the legwork for us. It looks on the CRAN (The Comprehensive R Archive Network) which contains the R approved packages. It downloads it so you can use all the functions contained in a spacific package.\nAs stated before, tidyverse is a collection of packages, and we will need to understand that later on, but for now: no need.\nNow to load the package so we can use the functions:\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.4     ✔ purrr   1.0.2\n✔ tibble  3.2.1     ✔ dplyr   1.1.2\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nAs above, you will get a bunch of messages in the console, we can reasonably ignore these for now.\n\n\n\n\n\n\nNote\n\n\n\nAs a general point, we would run install.packages() in the console, and place library() at the top of a script. The next section should make this a bit clearer as to the difference"
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk2_Answers.html#step-1-scripts",
    "href": "Worksheets/Answers/Worksheet_wk2_Answers.html#step-1-scripts",
    "title": "2. Data management and data wrangling",
    "section": "Step 1: Scripts",
    "text": "Step 1: Scripts\nA script is essentially a sequence of commands that we want R to execute. As Winter (2019) points out, we can think of our R script as the recipe and the R console as the kitchen that cooks according to this recipe. Let’s try out the script editor and write our first script. Typing commands in the console is good for one off commands (maybe to check the class() or to install.packages()), but the script is better for keeping the steps in order.\nWhen working in R, try to work as much as possible in the script. This will be a summary of all of your analyses, which can then be shared with other researchers, together with your data. This way, others can reproduce your analyses.\nThus far, you have typed your command lines in the console. This was useful to illustrate the functioning of our R, but in most of your analyses you won’t type much in the console. Instead, we will use the script editor.\nThe script editor is the pane on the top left of your window. If you don’t see it, you need to open a new script first. For this, press Cmd+Shift+N (Mac) or Ctrl+Shift+N (Windows). Alternatively, in the menu, click File &gt; New File &gt; RScript.)\nIn the script editor (not the console), type the following command in line 1 press Return (Mac) / Enter (Windows).\n\n2 + 3\n\n[1] 5\n\n\nAs you can see, nothing happened. There is no output in the Console pane; the cursor just moved to the next line in the script editor (line 2). This is because you did not execute the script.\nTo execute a command in the script editor, you need to place your cursor anywhere on the line you wish to execute and then click the Run icon in the Script editor pane. If you do this, then the following output will appear in your Console.\nYou can also run the current command line or selection in the script by pressing Cmd+Return (Mac) or Ctrl+Enter (Windows). This will also send your command from the script editor to the console. (I suggest using the shortcut, it’s much more efficient.)\nIn the script, you can have as many lines of code as you wish. For example, you can add the following three commands to your script.\n\nscores &lt;- c(145, 234, 653, 876, 456) \n\nmean(scores)\n\n[1] 472.8\n\nsd(scores)\n\n[1] 299.9178\n\n\nTo execute each one separately, just go to the line in question and click the Run icon or, even better, press the keyboard shortcut.\nYou can also run multiple commands in one go. For this, you either highlight several lines and then press the Run icon (or keyboard shortcut). Try it with the above three lines.\nTo execute all commands in the script, you click the Source icon (next to the Run icon) in the Script editor pane. Or just use the shortcut Cmd+Option+R (Mac) or Ctrl+Alt+R (Windows).\n\nMultiline commands\nUsing the script editor is particularly useful when we write long and complex commands. The example below illustrates this nicely.\nThis is a fairly long command, written in the console in one line.\n\ndf &lt;- data.frame(name = c('jane', 'michaela', 'laurel', 'jaques'), age = c(23, 25, 46, 19), occupation = c('doctor', 'director', 'student', 'spy'))\n\nbut in a multiline format:\n\ndf &lt;- data.frame(name = c('jane', 'michaela', 'laurel', 'jaques'), \n                 age = c(23, 25, 46, 19), \n                 occupation = c('doctor', 'director', 'student', 'spy'))\n\nNote the indentations, this is done automatically by RStudio as it recognises what is grouped according to parentheses.\n\n\nComments\nAn important feature of R (and other programming languages) is the option to write comments in the code files. Comments are notes, written around the code, that are ignored when the script is executed. In R, anything followed by the # symbol on any line is treated as a comment. This means that a line starting with # is ignored when the code is being run. And if we place a # at any point in a line, anything after the hash tag is also ignored. The following code illustrates this.\nComments are really useful for writing explanatory notes to ourselves or others.\n\n# Here is data frame with three variables.\n# The variables refer to the names, ages, and occupations of the participants.\ndf &lt;- data.frame(name = c('jane', 'michaela', 'laurel', 'jaques'), \n                 age = c(23, 25, 46, 19),\n                 occupation = c('doctor', 'director', 'student', 'spy'))\n\nor\n\n2 + 3 #This is addition in R.\n\n[1] 5\n\n\n\n\nCode sections\nTo make your script even clearer, you can use code sections. These divide up your script into sections as in the example below. To create a code section, go the line in the script editor where you would like to create the new section, then press Cmd+Shift+R (Mac) or Ctrl+Shift+R (Windows). Alternatively, in the Menu, select Code &gt; Insert Section.\nThe lines with the many hypens create the sections\n\n# Create vectors ---------------------------------------------------\n\nscores_test1 &lt;- c(1, 5, 6, 8, 10) # These are the scores on the pre-test.\nscores_test2 &lt;- c(25, 23, 52, 63) # These are the scores on the post-test.\n\n# A few calculations -----------------------------------------------\n\nmean_test1 &lt;- mean(scores_test1)\nmean_test2 &lt;- mean(scores_test2)\n\nround(mean_test1 - mean_test2) # The difference between pre and post-tests.\n\n[1] -35\n\n\nOnce you have created a section, you can ask R to run only the code in a specific region. This is because R recognizes script sections as distinct regions of code.\nTo run the code in a specific section, first go to the section in question (e.g., the section called # A few calculations ————) and then either press Cmd+Option+T (Mac) or Ctrl+Alt+T (Windows). You can also use the menu, Code &gt; Run Region &gt; Run Section. Have a go to see if this works out well.\n\n\nSaving scripts\nFinally, you can also save your script. To do this, just click the Save icon in the Script editor pane or press Cmd+S (Mac) or Ctrl+S (Windows). The script can be named anything, but it is often recommended to use lowercase letters, numbers and underscores only. (That is, no spaces, hyphens, dots, etc.)\nThe script is saved in the .R format in your directory. If you later double click it, the file will open in RStudio by default, but you can also view and edit the file in Word and similar programs."
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk2_Answers.html#step-2-a-bit-more-on-packages",
    "href": "Worksheets/Answers/Worksheet_wk2_Answers.html#step-2-a-bit-more-on-packages",
    "title": "2. Data management and data wrangling",
    "section": "Step 2: A bit more on packages",
    "text": "Step 2: A bit more on packages\nIt’s important to acknowledge the important work done by the developers who make R packages available for free and open source. When you use a package for your analyses (e.g., tidyverse or lme4), you should acknowledge their work by citing them in your output (dissertation, presentation, articles, etc.). You can find the reference for each package via the citation() function, as in the examples below.\n\ncitation(\"tidyverse\")\n\n\nTo cite package 'tidyverse' in publications use:\n\n  Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R,\n  Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller\n  E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V,\n  Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to\n  the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686.\n  doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {Welcome to the {tidyverse}},\n    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n    year = {2019},\n    journal = {Journal of Open Source Software},\n    volume = {4},\n    number = {43},\n    pages = {1686},\n    doi = {10.21105/joss.01686},\n  }\n\ncitation(\"lme4\")\n\n\nTo cite lme4 in publications use:\n\n  Douglas Bates, Martin Maechler, Ben Bolker, Steve Walker (2015).\n  Fitting Linear Mixed-Effects Models Using lme4. Journal of\n  Statistical Software, 67(1), 1-48. doi:10.18637/jss.v067.i01.\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {Fitting Linear Mixed-Effects Models Using {lme4}},\n    author = {Douglas Bates and Martin M{\\\"a}chler and Ben Bolker and Steve Walker},\n    journal = {Journal of Statistical Software},\n    year = {2015},\n    volume = {67},\n    number = {1},\n    pages = {1--48},\n    doi = {10.18637/jss.v067.i01},\n  }\n\n\nYou can also install packages by using the Packages tab in the Files, Plots, Packages, etc. pane. As you see in the figure below, the base package is already installed. You can install more packages by scrolling through the list (or using the search option to narrow down the choices) and then selecting the tick box to the left of the package. If you do this, you will see that the click will run the install.packages() command in the console.\nAs I mentioned above, run install.packages() in the console as a one-off command, you do not need to run this every time you want to use a package. Everytime we want to use a package in a given session, we need to tell R to load it up, which is why we put library() at the top of the script, so we can use the functions."
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk2_Answers.html#step-3-working-directories-and-clean-workspaces",
    "href": "Worksheets/Answers/Worksheet_wk2_Answers.html#step-3-working-directories-and-clean-workspaces",
    "title": "2. Data management and data wrangling",
    "section": "Step 3: Working directories and clean workspaces",
    "text": "Step 3: Working directories and clean workspaces\nEvery R session has a working directory. This is essentially the directory or folder from which files are read and to which files are written.\nYou can find out your working directory by typing the following command. Your output will obviously look different from the one below, which refers to my machine\n\ngetwd()\n\n[1] \"/Users/ivorym/Documents/PhD/Teaching/23_24/FASS512/Worksheets/Answers\"\n\n\nYou can also use a command to list the content in the working directory. (Alternatively, you can see your direct by using the Files tab in the Files, Packages, Plot, etc. pane.)\n\nlist.files()\n\n [1] \"carprice.csv\"                    \"example_file.txt\"               \n [3] \"language_exams_new.csv\"          \"language_exams_shorter.csv\"     \n [5] \"language_exams.csv\"              \"nettle_1999_climate.csv\"        \n [7] \"our_plot_300.jpeg\"               \"our_plot_retina.jpeg\"           \n [9] \"our_plot_screen.jpeg\"            \"our_plot.jpeg\"                  \n[11] \"our_plot.pdf\"                    \"our_plot.png\"                   \n[13] \"scores.csv\"                      \"simd.xlsx\"                      \n[15] \"titanic.csv\"                     \"Worksheet_wk1_Answers 2.html\"   \n[17] \"Worksheet_wk1_Answers 3.html\"    \"Worksheet_wk1_Answers 4.html\"   \n[19] \"Worksheet_wk1_Answers.qmd\"       \"Worksheet_wk2_Answers 2.html\"   \n[21] \"Worksheet_wk2_Answers 3.html\"    \"Worksheet_wk2_Answers 4.html\"   \n[23] \"Worksheet_wk2_Answers 5.html\"    \"Worksheet_wk2_Answers.qmd\"      \n[25] \"Worksheet_wk2_Answers.rmarkdown\" \"Worksheet_wk3 2.html\"           \n[27] \"Worksheet_wk3 3.html\"            \"Worksheet_wk3.qmd\"              \n[29] \"Worksheet_wk4 2.html\"            \"Worksheet_wk4 3.html\"           \n[31] \"Worksheet_wk4.qmd\"              \n\n\nI suggest you create a new working directory on your computer desktop and then use it for the entire course. Important files related to your R tasks (scripts, data, etc.) should later be downloaded to this folder.\nThe first step is for you to create a folder called FASS512 (or similar) in a sensible place on your computer. You can do this by going to the Files tab (in the Files, Packages, etc. pane) and clicking the “Create a new folder” icon. Place each weekly set of weekly files in their own weekly folders.\nOnce you have created the “statistics” folder on the desktop, go to the menu to set the default working directory to the new “statistics” folder. The easiest way is to go to the menu, RStudio &gt; Preferences. This should call up the following window.\nIn the window, click the Browse button and set the default working directory to the “statistics” folder in the desktop."
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk2_Answers.html#step-4-loading-data",
    "href": "Worksheets/Answers/Worksheet_wk2_Answers.html#step-4-loading-data",
    "title": "2. Data management and data wrangling",
    "section": "Step 4: Loading data",
    "text": "Step 4: Loading data\nWhen we are dealing with data in our analyses, we usually begin by importing a data file. R allows you to important data files in many different formats, but the most likely ones are .csv and .xlsx.\nI have uploaded several data files to our Moodle page. Please go to folder called “Data sets to download for this session” in the section for today’s session, then download the files in the folder and place them in your working directory (the statistics folder you just created). The files are from Winter (2019) and Fogarty (2019).\nLet’s try out loading data files. In the examples below, you will import three types of files: .csv, .txt, and .xlsx. Remember: You need to download the data files from our Moodle page and place them in our working directory first. Otherwise, you cannot import the files from our directory into R.\n\nCSV\nWe can use the read_csv() function from dplyr (part of the tidyverse) to load data that is in .csv format. The command below will load the data set (‘nettle_1999_climate.csv’) and create a new label for this data set (languages). There exists a read.csv() function in base, but it is slower and not as ‘smart’ as read_csv().\n\nlanguages &lt;- read.csv('nettle_1999_climate.csv')\n\nAlternatively, you can load data files by clicking File &gt; Import Dataset &gt; From Text (readr). In the dialogue window, then click browse and select the file nettle_1999_climate.csv. You can change the name of the data set in the text box at the bottom left, below Import Options, where it says Name.\n\n\n\n\n\n\nNote\n\n\n\nI am giving you these alternative GUI-based methods for carrying out the same steps as what is written in the script. I offer these to highlight how things can be done in many ways, but preferably you will use the script for pretty much everything. This creates a record of the commands needed to reproduce your analysis, which is better for future researchers (which includes you in a week’s time)\n\n\n\n\nTXT\nThe data file you just imported is in the .csv format. You can important data from files in other formats, too. If the data is in .txt format, you can simply use the following command.\n\ntext_file &lt;- read_table('example_file.txt') #(Note: Ignore the warning message in the console.)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  Participant = col_character(),\n  Test_1 = col_double(),\n  Test_2 = col_double()\n)\n\n\nThe command creates a new data set called text_file.\n\n\nxlsx\nIf the data is an Excel spreadsheet (e.g., .xlsx format), you can proceed as follows. Ideally it shouldn’t be, as csv are a universal file format that can be read across many machines. As a general rule, it is important to use these universal filetypes (csv, txt, pdf, html…) for better reproducibility and data management (Towse et al., 2021)1\n\nlibrary(readxl) #you may need to run install.packages(\"readxl\") first\n\nspreadsheet_exl &lt;- read_excel('simd.xlsx', sheet = 'simd')\n\nWarning: Expecting numeric in O2646 / R2646C15: got 'NA'\n\n\nWarning: Expecting numeric in E3702 / R3702C5: got 'NA'\n\n\nWarning: Expecting numeric in F3702 / R3702C6: got 'NA'\n\n\nWarning: Expecting numeric in I3702 / R3702C9: got 'NA'\n\n\nWarning: Expecting numeric in J3702 / R3702C10: got 'NA'\n\n\nWarning: Expecting numeric in K3702 / R3702C11: got 'NA'\n\n\nWarning: Expecting numeric in M3702 / R3702C13: got 'NA'\n\n\nWarning: Expecting numeric in N3702 / R3702C14: got 'NA'\n\n\nWarning: Expecting numeric in E3723 / R3723C5: got 'NA'\n\n\nWarning: Expecting numeric in F3723 / R3723C6: got 'NA'\n\n\nWarning: Expecting numeric in I3723 / R3723C9: got 'NA'\n\n\nWarning: Expecting numeric in J3723 / R3723C10: got 'NA'\n\n\nWarning: Expecting numeric in K3723 / R3723C11: got 'NA'\n\n\nWarning: Expecting numeric in M3723 / R3723C13: got 'NA'\n\n\nWarning: Expecting numeric in N3723 / R3723C14: got 'NA'\n\n\nFirst, you need to install the readxl package. Then, you create a new data set called spreadsheet_exl by using the read_excel() function.\nNote: Since spreadsheets have multiple sheets, you need to specify the name of the sheet you would like to import by using the sheet argument. In our case, the sheet is called simd, hence sheet = ‘simd’.\nRStudio can handle many other file extensions to import datasets. You can find out information on how to import other file types by using the R help function (or by searching on Google)."
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk2_Answers.html#step-5-examining-datasets",
    "href": "Worksheets/Answers/Worksheet_wk2_Answers.html#step-5-examining-datasets",
    "title": "2. Data management and data wrangling",
    "section": "Step 5: Examining datasets",
    "text": "Step 5: Examining datasets\nIf you have followed the steps above, you will have imported three data sets, languages, spreadsheet_exl, and text_file. You can now start exploring the data. We will focus on languages as an example.\nEvery time you import data, it’s good to check the content, just to make sure you imported the correct file.\nThe easiest way to do this is by using the View() function. This allows you to inspect the data set in the script editor. Note: The function requires a capital V. If you have tidyverse loaded, which we do, then there is a view() function as well. These are functionally equivalent. Use whichever, but View() will always work\nIf you run the command below, you will see that this shows the data (a table) in a tab of the script editor. It will also be displayed in the console.\n\n\n\n\n\n\nNote\n\n\n\nRemember what I have said previously about some content being better off in the console rather than the script? This is another example of what to put in the console instead (like class() or install.packages().\nWhy? Great question, because it’s a one-off command that we don’t need in our script. It’s a sanity check, like class(), and it doesn’t add anything of value to the script. The script should be the minimum series of commands that are required to go from one stage to another. Taking a visual look at a dataframe is superfluous to the actual analysis\n\n\n\nView(spreadsheet_exl) \nView(languages)\n\nYou can also inspect your data by visiting the Environment tab in the Environment, History, Connections, etc. pane. As you can see in the figure below, this will tell you thatlanguageshas 74 observations (rows) and five variables (columns).\nIf you would like to examine variables, you can start by using the str() function (str for structure), as in the example below.\n\nstr(languages)\n\n'data.frame':   74 obs. of  5 variables:\n $ Country   : chr  \"Algeria\" \"Angola\" \"Australia\" \"Bangladesh\" ...\n $ Population: num  4.41 4.01 4.24 5.07 3.69 3.88 3.13 5.19 3.97 3.5 ...\n $ Area      : num  6.38 6.1 6.89 5.16 5.05 6.04 5.76 6.93 5.44 5.79 ...\n $ MGS       : num  6.6 6.22 6 7.4 7.14 6.92 4.6 9.71 5.17 8.08 ...\n $ Langs     : int  18 42 234 37 52 38 27 209 75 94 ...\n\n\nAs you can see above, the str() function will tell you many useful things about your dataset. For example, it will reveal the number of observations (rows, 74) and variables (columns, 5), and then list the variables (Country, Population, Area, MGS, Langs). For each variable, it will also indicate the variable type (chr = character strings, num = numeric, intd = integer). The str() function will also display the first observations of each variable (Algeria, Angola, Australia, Bangladesh, etc.).\nYou can also check the names of variables separately by using the names() function, or check the variable type by checking the class() function, but it’s easier to just use the str() function as in the example above.\nIf you prefer, you can restrict your inspection of to the first or final rows of the data set. You can do this by using the head() and tail() function. This is helpful if your tables has lots of rows. It complements str() as it shows you a sample of the actual data, not just the structure.\n\nhead(languages) #default is six rows to display\n\n     Country Population Area  MGS Langs\n1    Algeria       4.41 6.38 6.60    18\n2     Angola       4.01 6.10 6.22    42\n3  Australia       4.24 6.89 6.00   234\n4 Bangladesh       5.07 5.16 7.40    37\n5      Benin       3.69 5.05 7.14    52\n6    Bolivia       3.88 6.04 6.92    38\n\ntail(languages, n = 5) #show last five rows\n\n    Country Population Area  MGS Langs\n70  Vietnam       4.83 5.52 8.80    88\n71    Yemen       4.09 5.72 0.00     6\n72    Zaire       4.56 6.37 9.44   219\n73   Zambia       3.94 5.88 5.43    38\n74 Zimbabwe       4.00 5.59 5.29    18\n\n\nHow could you show the first 10 rows?\n\n\n        Country Population Area  MGS Langs\n1       Algeria       4.41 6.38 6.60    18\n2        Angola       4.01 6.10 6.22    42\n3     Australia       4.24 6.89 6.00   234\n4    Bangladesh       5.07 5.16 7.40    37\n5         Benin       3.69 5.05 7.14    52\n6       Bolivia       3.88 6.04 6.92    38\n7      Botswana       3.13 5.76 4.60    27\n8        Brazil       5.19 6.93 9.71   209\n9  Burkina Faso       3.97 5.44 5.17    75\n10          CAR       3.50 5.79 8.08    94\n\n\nThere is also a very helpful function called summary(). As you can see in the example below, this function will provide you with summary information for each of your variables.\nFor numeric/integer variables such as Populations, Area, MGS, and Langs, this command will calculate the minimum and maximum values, quartiles, median and mean. (We will discuss summary statistics in more detail later.)\nFor character variables, as in Country, the command will simply provide you with the number of observations (length) for this variable.\n\nsummary(languages)\n\n   Country            Population         Area            MGS        \n Length:74          Min.   :2.010   Min.   :4.090   Min.   : 0.000  \n Class :character   1st Qu.:3.607   1st Qu.:5.223   1st Qu.: 5.348  \n Mode  :character   Median :3.990   Median :5.640   Median : 7.355  \n                    Mean   :3.992   Mean   :5.618   Mean   : 7.029  \n                    3rd Qu.:4.393   3rd Qu.:6.032   3rd Qu.: 9.193  \n                    Max.   :5.930   Max.   :6.930   Max.   :12.000  \n     Langs       \n Min.   :  1.00  \n 1st Qu.: 17.25  \n Median : 40.00  \n Mean   : 89.73  \n 3rd Qu.: 93.75  \n Max.   :862.00  \n\n\nIn large datasets, you might want to examine only a specific variable. You can do this by using the $ as an index. For example, if you would just like to examine the variable Population in the languages dataset, you could proceed as follows.\n\nstr(languages)\n\n'data.frame':   74 obs. of  5 variables:\n $ Country   : chr  \"Algeria\" \"Angola\" \"Australia\" \"Bangladesh\" ...\n $ Population: num  4.41 4.01 4.24 5.07 3.69 3.88 3.13 5.19 3.97 3.5 ...\n $ Area      : num  6.38 6.1 6.89 5.16 5.05 6.04 5.76 6.93 5.44 5.79 ...\n $ MGS       : num  6.6 6.22 6 7.4 7.14 6.92 4.6 9.71 5.17 8.08 ...\n $ Langs     : int  18 42 234 37 52 38 27 209 75 94 ...\n\nstr(languages$Population)\n\n num [1:74] 4.41 4.01 4.24 5.07 3.69 3.88 3.13 5.19 3.97 3.5 ...\n\nclass(languages$Population)\n\n[1] \"numeric\"\n\nhead(languages$Population)\n\n[1] 4.41 4.01 4.24 5.07 3.69 3.88\n\ntail(languages$Population)\n\n[1] 4.31 4.83 4.09 4.56 3.94 4.00\n\nsummary(languages$Population)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.010   3.607   3.990   3.992   4.393   5.930 \n\n\nWhich of the above six commands are best placed in the script or console?\n\n\n\n\n\n\nNote\n\n\n\n\n\nUltimately, there is no right or wrong answer. Personally,\nstr() belongs in the console because it should just be a quick check that it is the expected shape. It could go in the script if it was part of a more formal test. A sanity check is something that makes you go “oh, I should just make sure”, whereas a test is more in line with thoughts of “if it isn’t have an identical shape to dataframe2, none of this works” - a nuanced difference that we may perhaps explore in later sessions.\nclass() goes in the console - it is very much a sanity check. If it transpires the class isn’t what you wanted, we can coerce them into different classes, which we would include as a step in the script, but we don’t need to run the check everytime in the script if we are just going to coerce it anyway…\nhead() and tails() depends. If you’re just having a little look, then console. If it is something you are then using in the analysis, the script. Most likely the console though. If you can exit RStudio and reopen the script and it runs without errors, then it’s fine to leave in the console. If it fails, maybe you need things in the script?\nsummary() is one I usually keep in the script - particularly if I am reporting the summary of statistics (see a later session) because it is meaningful content that I need."
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk2_Answers.html#step-6-closing-your-r-session",
    "href": "Worksheets/Answers/Worksheet_wk2_Answers.html#step-6-closing-your-r-session",
    "title": "2. Data management and data wrangling",
    "section": "Step 6: Closing your R session",
    "text": "Step 6: Closing your R session\nThe last step is to close your R session. When you quit RStudio, a prompt will ask whether you want to save the content of your workspace. It is better to NOT save the workspace. When you start RStudio again, you will have a clean workspace. You then just re-run your scripts.\nIf you have written your scripts well, upon re-open, you should be able to produce the exact same steps without error and without odd additional windows opening (because we put View() in a script…).\nSo, I would save R scripts (especially if these are very long and are relevant to your analyses), but I would not the workspace contents."
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk2_Answers.html#take-home-task",
    "href": "Worksheets/Answers/Worksheet_wk2_Answers.html#take-home-task",
    "title": "2. Data management and data wrangling",
    "section": "Take home task",
    "text": "Take home task\nTo complete this homework task, you will need to download the language_exams data file from our Moodle page into your working directory.\nIn the file, you will find the (fictional) scores and ages of 475 students who took an intermediate Portuguese language course at university. Students were tested three times: first in September to check their Portuguese proficiency at the beginning of the course, then again in January as part of their mid-term examination, and finally in June as part of their final examination. On each occasion, students had to complete three subtests to respectively assess their Portuguese vocabulary, grammar and pronunciation. The scores for exams 1, 2 and 3 are composite scores, i.e. each combines the results of the three subtests.\nYour task is to run a basic analysis of the exam data using an R script.\nIn your script, please include all the steps, including the command that loaded the data.\nPlease also include sections to make your script very clear, as well as comments.\n\nHow many observations and columns does the datafile contain?\n\n\n\n'data.frame':   474 obs. of  5 variables:\n $ student_id: int  17970 13785 15457 13336 10990 14877 12433 12922 13031 16772 ...\n $ age       : int  18 19 19 25 23 22 25 24 18 24 ...\n $ exam_1    : int  57 92 63 40 25 84 93 90 41 54 ...\n $ exam_2    : int  52 76 60 24 13 76 85 85 23 44 ...\n $ exam_3    : int  59 81 66 30 19 82 94 90 28 52 ...\n\n\n\nRun commands to display the first and the last five lines of the table.\n\n\n\n  student_id age exam_1 exam_2 exam_3\n1      17970  18     57     52     59\n2      13785  19     92     76     81\n3      15457  19     63     60     66\n4      13336  25     40     24     30\n5      10990  23     25     13     19\n\n\n    student_id age exam_1 exam_2 exam_3\n470      12747  23     38     47     40\n471      16685  21     46     65     59\n472      18452  19     57     64     55\n473      15087  25     44     54     50\n474      19762  25     88    102     96\n\n\n\nWhat is the average age of participants? Report this as a whole number\n\n\n\n[1] 21.5865\n\n\n[1] 22\n\n\n\nWhat type of variable is student_id?\n\n\n\n[1] \"integer\"\n\n\n\nWhat is the rounded mean score on exam 3 to 2 decimal places?\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nNot sure how? Type ?round() into the console and read the help page. Specifically look under the Arguments section and the examples (the second to last is the best one)\n\n\n\n\n\n[1] 61\n\n\n\nWhat is the difference between the mean scores on exams1 and 2?\n\n\n\n[1] 3.253165\n\n\nPlease save the script to discuss at the next session."
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk2_Answers.html#footnotes",
    "href": "Worksheets/Answers/Worksheet_wk2_Answers.html#footnotes",
    "title": "2. Data management and data wrangling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTowse, A. S., Ellis, D. A., & Towse, J. (2021). Making data meaningful: Guidelines for good quality open data. The Journal of Social Psychology, 161(4), 395–402. https://doi.org/10.1080/00224545.2021.1938811↩︎"
  },
  {
    "objectID": "Assignments/Final assignment, due Monday, April 24, 2023, 12pm-20240223/Final Assignment.html",
    "href": "Assignments/Final assignment, due Monday, April 24, 2023, 12pm-20240223/Final Assignment.html",
    "title": "Assignment 2: Significance testing",
    "section": "",
    "text": "Word limit: 3,500 words maximum, excluding the R script\nDeadline: Monday, April 22, 2024, 12pm"
  },
  {
    "objectID": "Assignments/Final assignment, due Monday, April 24, 2023, 12pm-20240223/Final Assignment.html#how-to-submit",
    "href": "Assignments/Final assignment, due Monday, April 24, 2023, 12pm-20240223/Final Assignment.html#how-to-submit",
    "title": "Assignment 2: Significance testing",
    "section": "How to submit",
    "text": "How to submit\nThis assignment must be submitted online via the Assignment area of our Moodle site. Please do not submit work by e-mail. Each copy must have a completed Coversheet for Coursework attached to it. The coversheet can be downloaded from our Moodle page.\nWhen completing the assignment, please remember that\n\nyou should answer all of the questions and all the parts of each question;\nyour assignment submission must include a complete log of your R session, including your R script with the commands and any graphical outputs produced. The log and the plots don’t count towards the word limit;\nyou do not need to write a lot in response to each question, but you should present your results, etc., in a neat, orderly, and professional manner, as if you were including them in a formal paper (e.g., a report or a dissertation). That is, do not just dump raw R outputs into the main text of your assignment;\nthe word-limit for this assignment is a maximum upper limit only. You must be thorough and precise to do well, but you need not “pad out” your assignment to get anywhere close to this limit;\nadhere to APA formatting style for reporting your findings. See here for information: https://apastyle.apa.org/instructional-aids/numbers-statistics-guide.pdf"
  },
  {
    "objectID": "Assignments/Final assignment, due Monday, April 24, 2023, 12pm-20240223/Final Assignment.html#question-one",
    "href": "Assignments/Final assignment, due Monday, April 24, 2023, 12pm-20240223/Final Assignment.html#question-one",
    "title": "Assignment 2: Significance testing",
    "section": "Question One",
    "text": "Question One\nThe file undergrads.csv (available on Moodle) contains information regarding two groups of universities in the UK: the Russell Group and the Cathedrals Group. The information relates to the academic year 2020-2021 (source: Higher Education Statistics Agency, HESA).\nThe four variables included in the data set are:\n\nukpc– the percentage of registered undergraduates who are domiciled in the UK\neupc– the percentage of registered undergraduates who are domiciled in EU countries\nnoneupc– the percentage of registered undergraduates who are domiciled in non-EU countries (other than the UK)\nfempc– the percentage of registered undergraduates who are female\n\nThe group to which each university belongs is coded as either russ or cath in the additional variable group.\nConduct a statistical comparison of the two groups of universities in relation to each of the four variables. Write a very brief report on what kinds of analyses you conducted and what you discovered. Remember to justify your choice of method(s) for the data analysis."
  },
  {
    "objectID": "Assignments/Final assignment, due Monday, April 24, 2023, 12pm-20240223/Final Assignment.html#question-two",
    "href": "Assignments/Final assignment, due Monday, April 24, 2023, 12pm-20240223/Final Assignment.html#question-two",
    "title": "Assignment 2: Significance testing",
    "section": "Question Two",
    "text": "Question Two\nTable 1 contains information about the results of random mandatory drug testing in English prisons (source: HMPPS Annual Digest 2019-2020). It shows the numbers of positive tests for different types of drugs in the years ending March 2018 and March 2020 respectively.\n\n\n\nTable 1. Positive drug tests for years ending March 2018 and March 2020\n\n\nDrug\n2018\n2020\n\n\n\n\nAmphetamines\n62\n122\n\n\nBarbiturates\n2\n3\n\n\nBenzodiazepines\n320\n217\n\n\nBuprenorphine\n1063\n779\n\n\nCannabis\n3067\n3082\n\n\nCocaine\n259\n259\n\n\nMethadone\n325\n375\n\n\nOpiates\n1257\n1110\n\n\nPsychoactive Substances\n6636\n2203\n\n\n\n\n\nComplete a statistical analysis to test whether the distributions of positive tests are different between the two years. Write a very brief report on what kinds of analyses you undertook and what you discovered. Ensure that you justify your choice of method(s) for the data analysis. If you find a statistically significant difference, comment on the size of – and the main contributors to – the overall effect."
  },
  {
    "objectID": "Assignments/Final assignment, due Monday, April 24, 2023, 12pm-20240223/Final Assignment.html#question-three",
    "href": "Assignments/Final assignment, due Monday, April 24, 2023, 12pm-20240223/Final Assignment.html#question-three",
    "title": "Assignment 2: Significance testing",
    "section": "Question Three",
    "text": "Question Three\nThe data set usrelig.csv, available on Moodle, contains a number of social indicators, for the years 2008-2009, relating to the 50 states of the USA, plus the District of Columbia (Source: US Census Bureau, Statistical Abstracts of the United States). Data concerning the proportion of women smokers in each state are also included (source: Behavioral Risk Factor Surveillance System, 2009), as are two estimates of religiosity and religious affiliation (source: Pew Research Center, 2014; data for 2008-2009 were not available).\nThe following variables are included in the file:\n\nvrelig - the percentage of people in the state who described themselves as “highly religious”\ncathpc - the estimated percentage of Roman Catholics in the state’s population\nabort - the abortion rate for the state (per thousand women)\nfemsmoke - the estimated proportion of female smokers in the state (as a percentage of the state’s female population)\npov - the percentage of individuals in the state who live below the poverty level\nhsch - the percentage of the state’s population who have graduated from high school\nuniv - the percentage of graduates in the state’s population (i.e. those with at least a bachelor’s degree)\nurban - the percentage of the state’s population which lives in urban (as opposed to rural) areas\n\nRead the data set into R and use an appropriate method (or methods) to investigate a possible relationship between the percentage of “highly religious” people (as your outcome variable) and the percentage of graduates in the state’s population (as your predictor variable).\nIn your report, make sure that you describe clearly what kinds of analyses you undertook and what you found out. Ensure that you justify fully your choice of method(s) for the data analysis. When performing any regression analyses, ensure that you also undertake and report all of the necessary diagnostics. Where appropriate, make sure that you also plot your regression model(s) and write down the model(s) in the form of an equation that we could potentially use to make predictions manually."
  },
  {
    "objectID": "Assignments/Final assignment, due Monday, April 24, 2023, 12pm-20240223/Final Assignment.html#question-four",
    "href": "Assignments/Final assignment, due Monday, April 24, 2023, 12pm-20240223/Final Assignment.html#question-four",
    "title": "Assignment 2: Significance testing",
    "section": "Question Four",
    "text": "Question Four\nTable 2 shows the sizes of the male and female prison populations in England in two corresponding weeks of 2021 and 2022 (source: HMPPS weekly reports).\n\n\n\nTable 2. English prisons: numbers of prisoners by gender in corresponding weeks of 2020 and 2022\n\n\nReported Gender\n28-Feb-2020\n25-Feb-2022\n\n\n\n\nFemale\n3721\n3202\n\n\nMale\n80147\n76522\n\n\n\n\n\nUndertake a statistical analysis to test whether there is any association between year and prisoner gender. Write a very brief report on what kinds of analyses you undertook and what you found out. Ensure that you justify your choice of method(s) for the data analysis. If you do find a statistically significant association, ensure that you also comment on the effect size."
  },
  {
    "objectID": "Assignments/Mid-term assignment, due Monday, Feb 19, 2024, 12pm-20240116/Assignment_script.html",
    "href": "Assignments/Mid-term assignment, due Monday, Feb 19, 2024, 12pm-20240116/Assignment_script.html",
    "title": "Mid-term Assignment",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.4     ✔ purrr   1.0.2\n✔ tibble  3.2.1     ✔ dplyr   1.1.2\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\ncrime_data &lt;- read_csv(\"/Users/ivorym/Downloads/Mid-term assignment, due Monday, Feb 19, 2024, 12pm-20240220/crimes-sep21.csv\")\n\nRows: 37 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): area\ndbl (13): region, totcrim, vioper, stalk, sex, theft, burgle, vehicle, shopl...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncrime_data\n\n# A tibble: 37 × 14\n   area  region totcrim vioper stalk   sex theft burgle vehicle shoplift crimdam\n   &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1 Clev…      1   119.    45.9  19     3.9  30.3    7.1     5.1      8.5    16.1\n 2 Durh…      1    88     38.6  18     3    18.8    4       3.3      5.1    13.4\n 3 Nort…      1    87.8   32.6  12.1   3    21.7    3.9     4.1      5.3    12.7\n 4 Ches…      2    81.6   37.9  17     3.1  15.1    3.1     2.4      3.7     7.4\n 5 Cumb…      2    65.4   29    10.4   2.8  12.2    2.7     1.4      3.1     9  \n 6 Grea…      2   113.    42.6  15.7   3.7  30.4    7.3     8.5      4.4    11.1\n 7 Lanc…      2    85.2   37.2  13.4   3    21.7    5.1     4.3      4.3    10.6\n 8 Mers…      2   101.    41.1  14.4   2.8  20.9    4.9     4.5      4.2    10.2\n 9 Humb…      3    93     37.1  13.5   2.9  22.2    5.3     3.2      5.8    11.4\n10 Nort…      3    52.2   21.9   6.3   2.2  12.6    2.5     1.7      3.4     6.5\n# ℹ 27 more rows\n# ℹ 3 more variables: drugs &lt;dbl&gt;, weapons &lt;dbl&gt;, pubord &lt;dbl&gt;"
  },
  {
    "objectID": "Assignments/Mid-term assignment, due Monday, Feb 19, 2024, 12pm-20240116/Assignment_script.html#question-1",
    "href": "Assignments/Mid-term assignment, due Monday, Feb 19, 2024, 12pm-20240116/Assignment_script.html#question-1",
    "title": "Mid-term Assignment",
    "section": "Question 1",
    "text": "Question 1\nUsing R, calculate the mean rates and standard deviations for (a) stalking and harassment, and (b) theft. Report your results in one or two complete sentences, as if you were including them in a research report, rounded to two decimal places.\n\ndata_summary_1 &lt;- crime_data |&gt; \n  summarise(stalk_mean = mean(stalk), stalk_sd = sd(stalk),\n            theft_mean = mean(theft), theft_sd = sd(theft))\n\nBetween September 2020 and September 2021, the mean number of stalking and harassment-related crimes (per 1,000 head of population) across the UK, excluding London, was 11.51 (SD = 4.03). For theft-related crimes, more were reported at 19.48 (SD = 4.91)"
  },
  {
    "objectID": "Assignments/Mid-term assignment, due Monday, Feb 19, 2024, 12pm-20240116/Assignment_script.html#question-2",
    "href": "Assignments/Mid-term assignment, due Monday, Feb 19, 2024, 12pm-20240116/Assignment_script.html#question-2",
    "title": "Mid-term Assignment",
    "section": "Question 2",
    "text": "Question 2\nUsing R, calculate the median rates and interquartile ranges for (a) drugs offences, and (b) violence against the person. Report your results in one or two complete sentences, as if you were including them in a research report, rounded to two decimal places.\n\nsummary(crime_data$drugs)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.100   2.100   2.400   2.646   2.900   8.600 \n\nsummary(crime_data$vioper)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  20.70   26.10   29.00   31.76   37.10   50.70 \n\n\nBetween September 2020 and September 2021, the median number of drug-related crimes in the UK per 1,000 head of the population was 2.4 (IQR = 2.1, 2.6459459). In comparison, for reported violence against the person, the median number was 29 (IQR = 26.1, 31.7621622), indicating that far more violence-related crimes were reported than drug-related."
  },
  {
    "objectID": "Assignments/Mid-term assignment, due Monday, Feb 19, 2024, 12pm-20240116/Assignment_script.html#question-3",
    "href": "Assignments/Mid-term assignment, due Monday, Feb 19, 2024, 12pm-20240116/Assignment_script.html#question-3",
    "title": "Mid-term Assignment",
    "section": "Question 3",
    "text": "Question 3\nThe following R code will draw a set of side-by-side boxplots for the rates of public-order offences in the eight regions. (Hint: Remember to name your object appropriately)\n\ncrime_data |&gt;\n  ggplot(aes(reorder(region, pubord, median), pubord)) +\n  geom_boxplot()\n\nProduce the set of plots, then explain briefly what each element of the plot shows. That is, label and explain, in general terms, its component parts. Then go on to outline what you can infer from it about the present data set.Include the plot in your report. (You may like to cross-refer to the original data table to get the most out of this.)\n\ncrime_data |&gt;\n     ggplot(aes(reorder(region, pubord, median), pubord)) +\n     geom_boxplot()\n\n\n\n\n\nThe x-axis shows the boxplots ordered by median reported number of crimes for public-order offences across the eight regions included in the data set.\nThe y-axis shows the median value of public-order offences.\nEach boxplot for the region shows the quartile range contained within the box, and the whiskers extend to 1.5 * IQR, values outside of these ranges are represented as single observations. Smaller boxplots indicate a smaller distribution of the reported crimes.\nThe solid black line is the median score, and this confirms that the data is ordered by median values (as they ascend in value from left to right)\n\nFrom the plot above, we can infer that variance exists across the different regions in the UK for the reported public order offences committed between September 2020 and September 2021. For example, the South West and West Midlands report very few of these crimes comparied to the North West or Yorkshire & the Humber regions. From this, we can suggest that the levels of public order offences are not homogeneous across the UK, with variatoin between police forces."
  },
  {
    "objectID": "Assignments/Mid-term assignment, due Monday, Feb 19, 2024, 12pm-20240116/Assignment_script.html#question-4",
    "href": "Assignments/Mid-term assignment, due Monday, Feb 19, 2024, 12pm-20240116/Assignment_script.html#question-4",
    "title": "Mid-term Assignment",
    "section": "Question 4",
    "text": "Question 4\n\na. Modify the R commands from Question 3 to produce a set of side-by-side boxplots for the rates of criminal damage and arson in the eight regions. Outline briefly what your plots tell us about these data. Comment especially on any peculiar-looking features of these plots, and attempt to explain why they have occurred. Include the plot in your report. (Again, you may wish to cross-refer to the data table here)\n\ncrime_data |&gt;\n     ggplot(aes(reorder(region, crimdam, median), crimdam)) +\n     geom_boxplot()\n\n\n\n\nFrom this plot, we can see the different reported levels of criminal damage and arson across the eight recorded regions in the UK. There exist two outlier values for region 7 (South East) and region 3 (Yorkshire and the Humber). One outstanding feature warranting note is the very high proportion of recorded criminal damage and arson crimes in the North East. There are numerous possible reasons for this that range from the spurious (that the population of the North East are predisposed to commit arson specifically), to the more likely such as that there are typically higher rates of crime in the North East region (as can be verified in the total crime rates for each region). Other acceptable reasons or inferences (non-exhaustive) would include that perhaps differences within the propensity of police forces reporting crimes influence these levels. Perhaps a higher sensitivity towards what constitutes arson. Other reasons may exist, and any potentially justifiable idea that indicates these differences are worthy of marks.\nGoing further, a good point to note, to show real interrogation of the data (and evidence higher grade attainment) would be to order the x-axis by total crime reported to show that the highest crime rate is for region 3 (Yorkshire and the Humber) but has a lower arson level.\n\ncrime_data |&gt;\n     ggplot(aes(reorder(region, totcrim, median), crimdam)) +\n     geom_boxplot()\n\n\n\n\n\n\nb. modify the plot for question 4 and give it appropriate label axes (both x and y), and change the region labels from numbers to the names of the regional areas (e.g. 1 should be North East). Hint: you will need to use mutate() to update the data like we did in week 3. Include the publication-ready plot in your report with a suitable caption.\n\ncrime_data |&gt;\n  mutate(region = str_replace(region, \"1\", \"North East\"),\n         region = str_replace(region, \"2\", \"North West\"),\n         region = str_replace(region, \"3\", \"Yorkshire and the Humber\"),\n         region = str_replace(region, \"4\", \"East Midlands\"),\n         region = str_replace(region, \"5\", \"West Midlands\"),\n         region = str_replace(region, \"6\", \"East of England\"),\n         region = str_replace(region, \"7\", \"South East\"),\n         region = str_replace(region, \"8\", \"South East\")\n         ) |&gt; \n     ggplot(aes(reorder(region, crimdam, median), crimdam)) +\n     geom_boxplot() +\n  labs(x = \"UK region\", y = \"Reported rates of Criminal Damage and Arson\",\n       caption = \"Boxplot to show the different reported rates of Criminal Damage and Arson across the eight regions in the UK (excluding London)\") +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1))"
  },
  {
    "objectID": "Worksheets/Worksheet_wk8.html",
    "href": "Worksheets/Worksheet_wk8.html",
    "title": "8. Correlations and t-tests",
    "section": "",
    "text": "Three important things to remember:"
  },
  {
    "objectID": "Worksheets/Worksheet_wk8.html#task-0-setting-up-our-environment",
    "href": "Worksheets/Worksheet_wk8.html#task-0-setting-up-our-environment",
    "title": "8. Correlations and t-tests",
    "section": "Task 0: Setting up our environment",
    "text": "Task 0: Setting up our environment\nYou should be able to do all these things, if not - check back on previous week’s content as a reminder\n\nCreate a new script and call it Week 8.\nLoad in the tidyverse library at the top of the script"
  },
  {
    "objectID": "Worksheets/Worksheet_wk8.html#task-1-correlations",
    "href": "Worksheets/Worksheet_wk8.html#task-1-correlations",
    "title": "8. Correlations and t-tests",
    "section": "Task 1: Correlations",
    "text": "Task 1: Correlations\n\nGuessing Correlations\nFirst off, let’s play a game of: guess the correlation. Click “track performance” to keep score, and then click “new sample”, take a guess at what the correlation is showing and check your guess. Try out a few. See if you can beat my score of r = .976 over 10 guesses! We will shortly see what the scores mean, but the closer to 1 the better\n\n\n\nMy attempt at guessing the correlations\n\n\n\n\nConducting a correlation\nNow, we need some data if we are to conduct an analysis. Download the datafile from Moodle, and read in the MillerHadenData.csv to an object called data.\nThis data contains five columns: Participant which indicates they are unique observations, reading ability Abil, intelligence measured with IQ, minutes per week spent reading at home Home, and the number of minutes per week spent watching TV at home TV. The participants were 25 eight-year-old children.\n\ndata &lt;- read_csv(\"MillerHadenData.csv\")\n\nRows: 25 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): Participant, Abil, IQ, Home, TV\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nTake the opportunity to View() the data, or look at a subsection of the data using head(). It is always useful to actually take a look at the data we have, to get an idea of what it contains and its format. Conveniently, the data is in the right format for running the analysis, so we don’t need to do any tidying. On to visualisation.\nCan you construct a scatterplot of the relationship between the variables Home and TV? If you can’t remember the code, check back on your week 4 script or worksheet.\n\n\n\n\n\n\nNeed a hint?\n\n\n\n\n\nHere’s a code skeleton that might help:\n\ndata |&gt; ggplot(aes(...)) +\n  geom_point()\n\n\n\n\nOnce you have your scatterplot, we can improve on it by adding a line of best fit, so include + geom_smooth(method = \"lm\", se = FALSE) to your graph and reproduce it. It should look like below:\n\n\n\n\n\nWhat relationship would we expect to find? Is it positive, negative or neutral?\n\n\n\n\n\n\nAn answer\n\n\n\n\n\nI would suggest that we have a negative correlation between reading at home and watching tv. As children read more per week, they watch less TV.\n\n\n\nNow that we have visualised the data, let us see if it significant and the strength by running a correlation test. We will do this using the cor.test() function. This takes two variables (in this case vectors, so we will use the $ notation) and a couple of other necessary arguments. For our first foray into correlation, I have given you the code and output below.\nWe specify the two variables we are interested in correlating, and then the type of correlation (in this case Pearson, because both variables are continuous).\n\ncor.test(data$TV, \n         data$Home, \n         method = \"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  data$TV and data$Home\nt = -4.0766, df = 23, p-value = 0.0004651\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.8303052 -0.3393758\nsample estimates:\n       cor \n-0.6476572 \n\n\nThe output from the test isn’t exactly the friendliest of things to read, but let’s break it down line by line.\nPearson's product-moment correlation - the title and name of the test\nt = -4.0766, df = 23, p-value = 0.0004651 - this is contains important information about the test result. t is the test statistic (and we don’t typically report this), and is followed by the degrees of freedom (df). Finally, we have the p-value which tells us whether the correlation is significant or not.\nalternative hypothesis: true correlation is not equal to 0 - this states the hypothesis that is being tested. Can you guess what the null hypothesis should be?\n95 percent confidence interval: -0.8303052 -0.3393758 - this gives us the confidence interval of the correlation. So we expect the true correlation coefficient to fall between these two values 95% of the time. It could be as high as -.34, or as low as -.83.\n-0.6476572 And finally the final line is the correlation value, r. The preceding two lines just tell us it’s a correlation.\nAwesome. So we have all this information, now we need to translate our R output into something human-readable:\nA pearson’s correlation coefficient was used to assess the relationship between time spent reading at home and watching TV. There was a significant negative correlation r(23) = -.65 [-.34, -.83], p &lt; .001, demonstrating that children who read more at home tend to watch less television.\nDoes the above make sense? Can you see where the values from the output fit into the table? As a general rule, we report the statistic (r) and then we include the degrees of freedom inside the brackets, we then give the coeffiecient followed by the confidence interval in square brackets, and then the p-value. There are standards as to how we report statistics, and we use the APA format. APA is great because there exists lots of online information on formatting statistics if you need help. Confidence intervals are not always reported, but p values and the coefficient are the key values.\nWe can actually take our reporting a step further by reporting an additional value that we can comput ourselves. The correlation coefficient tells us how well the two variables relate to each other, but by itself it does not explain how well variable y accounts for differences in x (or vice versa). We can explain this using \\(r^2\\) (r squared).\nCalculating \\(r^2\\) is actually quite easy; we square r. Squaring removes the sign of the number and gives us a positive number between 0 and 1 which is the percentage of how well X explains Y. So for our example above:\n\n(-0.6476572)^2\n\n[1] 0.4194598\n\n\nWe can say that \\(r^2\\) = .42 (rounded to 2dp), which is equivalent to 42%. So “42% of the variation in time spent reading is explained by time spent watching TV”. It should be reasonably clear that the higher the correlation coefficient (ignoring the sign), the more it explains because:\n\n(1)^2 # == 1, or 100%. A strong positive correlation\n\n[1] 1\n\n(-1)^2 # == 1, or 100%. A strong negative correlation\n\n[1] 1\n\n(.1)^2 # == .01, or 1%, a very weak positive correlation\n\n[1] 0.01\n\n(-.1)^2 # == .01, or 1%, a very weak positive correlation\n\n[1] 0.01\n\n## Just as a fun check in R, we can use the == notation (double equals) to say \"is equal to\"\n## If I want to check if one number is equal to another I can do the following\n(1)^2 == (-1)^2\n\n[1] TRUE\n\n## It returns TRUE, meaning that they are exactly the same number. Hopefully that not only highlights that what I'm saying above is true, but also that with more complicated, longer numbers, we can just have R tell us what we need to know\n(.0304)^2 == (-.0304)^2 # == 0.00092416, but I don't need to remember that tiny number if R can do it for me. This would explain .09% of the variation. \n\n[1] TRUE\n\n\n\n\nMy second correlation\nCan you carry out another correlation between IQ and reading? Make a plot, then run the analysis, and interpret the findings. Is the correlation positive, negative, or neither? Is it significant? How much variance is explained?\n\n\nMy third correlation\nNow, can you carry out another correlation between IQ and reading? Make a plot, then run the analysis, and interpret the findings. Is the correlation positive, negative, or neither? Is it significant? How much variance is explained?"
  },
  {
    "objectID": "Worksheets/Worksheet_wk8.html#task-2-t-tests",
    "href": "Worksheets/Worksheet_wk8.html#task-2-t-tests",
    "title": "8. Correlations and t-tests",
    "section": "Task 2: t-tests",
    "text": "Task 2: t-tests\n\nBetween-Subjects t-tests (two-sample)\nFirst we need to import the data we are working with for this section. These are the files called evaluators.csv and ratings.csv.\nThis data comes from Schroeder and Epley (2015)1 and, it is the ratings from 39 recruiters from Fortune 500 companies evaluated job pitches of M.B.A. candidates from an American Business School.\n\nevaluators &lt;- read_csv(\"evaluators.csv\")\n\nratings &lt;- read_csv(\"ratings.csv\")\n\nHave a look at the datasets and explore it a bit to get a feel for it. What you should notice is that a) I am talking about one dataset but two files?, and b) both contain a column called eval_id. This dataset contains more information that we want to use today, so let’s tidy up our dataset. I will explain here what we are doing in the same sequence as in the code below, try and match up what I write here to the code lines\n\ncreate a new dataframe called ratings_intellect -keep only the rows that relate to “competent”, “thoughtful”, “intelligent” so that we can create a composite variable called intellect\ngroup the rows by their unique participant ID\nget a mean rating of the three intellect ratings, and\ncreate a new column called Category that contains the word intellect for all rows\n\n\nratings_intellect &lt;- ratings |&gt; \n  filter(Category %in% c(\"competent\", \"thoughtful\", \"intelligent\")) |&gt; \n  group_by(eval_id) |&gt; \n  summarise(Rating = mean(Rating)) |&gt; \n  mutate(Category = \"intellect\")\n\nThis creates a dataframe that is useful for our next steps when we:\n\nCreate a new dataframe called ratings_tidy\nfilter the dataset to keep only data from categories “impression” or “hire”\nadd the rows from ratings_intellect to ratings_tidy (using bind_rows)\nrearrange the dataframe so that it is easier to read, arrange it by id and then category, otherwise the second dataframe (ratings_intellect) would be at the bottom of the dataframe.\n\n\nratings_tidy &lt;- ratings |&gt; \n  filter(Category %in% c(\"impression\", \"hire\")) |&gt; \n  bind_rows(ratings_intellect) |&gt; \n  arrange(eval_id, Category)\n\nFinally, you may have noticed we have only touched one object so far, and that’s ratings, now we want to join it with evaluators to get our final dataset. Provided that two objects share a common variable (e.g., eval_id), we can combine these. We can do this using the function inner_join() which is a new one.\n\nratings_tidy &lt;- ratings_tidy |&gt;\n  inner_join(evaluators, \"eval_id\") |&gt;\n  select(-age, -sex)\n\nBefore we run any statistical tests, what do we need to do? That’s right, visualise the data!\nA boxplot is the best way to visualise group means. We will visualise the intellect rating we created\n\nratings_tidy |&gt; \n  filter(Category == \"intellect\") |&gt; \n  ggplot(aes(condition, Rating)) +\n  geom_boxplot()\n\n\n\n\nCan we observe what direction of an effect we might see? What condition is associated with higher scores of intellect?\nNow we want to check the assumptions of our data, which for a between-subjects t-test are:\n\nThe data are continuous, i.e. interval/ratio\nThe data are independent\nThe residuals are normally distributed for each group\n\n1 and 2 are true from the study design, and the measures. To test 3, we create what is called a QQ plot. Truly to test the residuals, we calculate the difference of each data point from the mean of each group, so if the mean of group A is 10 and a participant in group A scores 12, the residual for that participant is 2. It basically measn the data left over, that isn’t explained by the group mean. We can also use the raw data as a proxy of the residuals too.\nTo run a QQ plot, we need a new package. So install and load car, then we need to extract just the values we want and then plot it. With pipes, we can do this all in one.\n\n#install.packages(car) \nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nratings_tidy |&gt; \n  filter(Category == \"intellect\") |&gt; \n  pull(Rating) |&gt; \n  qqPlot()\n\n\n\n\n[1] 12 16\n\n\nWhat we are looking for is the data to fall along the diagonal line as best as possible. Looks good to me.\nWe can actually test the normality distribution using a statistic, but we will save it for next week. For now, be satisified that we have met our assumptions\nSo, finally, on to our t-test! For this, we need to split out our data to ensure we are running the t-test on one category at a time. Here, I will only test the difference for intellect, and then you can rerun the process on impression and hire too.\n\n#filter only the intellect ratings\nintellect &lt;- filter(ratings_tidy, Category == \"intellect\")\n\nt.test(Rating ~ condition,\n       data = intellect,\n       paired = FALSE,\n       alternative = \"two.sided\")\n\n\n    Welch Two Sample t-test\n\ndata:  Rating by condition\nt = 3.4787, df = 33.434, p-value = 0.001421\nalternative hypothesis: true difference in means between group listened and group read is not equal to 0\n95 percent confidence interval:\n 0.8253765 3.1481685\nsample estimates:\nmean in group listened     mean in group read \n              5.634921               3.648148 \n\n\nThe output looks quite similar to when we ran a correlation, so it should be familiar-ish. We can tidy this up a bit, so it’s easier to read using the tidy() function from broom. So install and load broom if needed\n\n#install.packages(\"broom\")\nlibrary(broom)\n\nttest_intellect &lt;- t.test(Rating ~ condition,\n       data = intellect,\n       paired = FALSE,\n       alternative = \"two.sided\") |&gt; \n  tidy()\n\nttest_intellect\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     1.99      5.63      3.65      3.48 0.00142      33.4    0.825      3.15\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\n\nestimate is the difference between the two means\nestimate1 is group 1’s mean\nestimate2 is group 2’s mean\nstatistic is the t-statistic\np.value is the p value\nparameter is the degrees of freedom\ncon.low and conf.high are the confidence interval of the estimate\nmethod is the type of test, Welch’s, Student’s, paired, or one-sample\nalternative is whether the test was one or two-tailed\n\nNow, can you repeat the steps above to compute the t-test for the categories ‘impression’ and ‘hire’?\nHopefully, you have ended up with three objects, one called ttest_intellect, another called ttest_impression, and the last called ttest_hire. Let’s combine these for easier reading\n\nresults &lt;- bind_rows(intellect = ttest_intellect,\n                     impression = ttest_impression,\n                     hire = ttest_hire,\n                     .id = \"test\")\n\nWe aren’t quite finished yet, because we have something new to consider, as well as effect sizes.\n\n\nMultiple Comparisons\nSince we ran three tests on the same data, we increase our chances of detecting a false positive (a significant effect where one doesn’t exist), so we must adjust our p-value to reflect this. We can do this relatively simply. We will update our results object to show the adjusted p-value\n\nresults_adj &lt;- results |&gt; \n  mutate(p.adj = p.adjust(p.value, method = \"bonferroni\"), .after = p.value) # .after tells R where to put the new column\n\nFinally, let’s compute the effect size and add this to our table of findings. For this we need yet another package called effectsize - you know the drill, install if needed and load it in.\nFor t-tests, we use Cohen’s D and as a general rule, we can interpret the output as:\n\nSmall effect = 0.2\nMedium Effect = 0.5\nLarge Effect = 0.8\n\n\nlibrary(effectsize)\n\nintellect_d &lt;- cohens_d(Rating ~ condition, \n                      pooled_sd = FALSE, \n                      data = intellect)\n\nimpression_d &lt;- cohens_d(Rating ~ condition, \n                      pooled_sd = FALSE, \n                      data = impression)\n\nhire_d &lt;- cohens_d(Rating ~ condition, \n                      pooled_sd = FALSE, \n                      data = hire)\n\n#now add them to the table\nresults_adj &lt;- results_adj |&gt; mutate(\n  d = c(intellect_d$Cohens_d, impression_d$Cohens_d, hire_d$Cohens_d)\n)\n\nresults_adj\n\n# A tibble: 3 × 13\n  test       estimate estimate1 estimate2 statistic p.value   p.adj parameter\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1 intellect      1.99      5.63      3.65      3.48 0.00142 0.00426      33.4\n2 impression     1.89      5.97      4.07      2.82 0.00804 0.0241       33.8\n3 hire           1.89      5.97      4.07      2.82 0.00804 0.0241       33.8\n# ℹ 5 more variables: conf.low &lt;dbl&gt;, conf.high &lt;dbl&gt;, method &lt;chr&gt;,\n#   alternative &lt;chr&gt;, d &lt;dbl&gt;\n\n\nWe can now use these new p-values, and effect sizes to determine the significance of these three ratings. p values tell us if the findings are significant, the effect size tells us how much of a real-world effect they are likely to have\nCombined with the visualisations we should have carried out, how would we interpret our findings?\nLet me do intellect as an example:\nTo test whether professional recruiters’ rated the presenter’s intellect differently between listening (mean = 5.63, SD = 1.61) and reading (mean = 3.65, SD = 1.91), a significant difference was seen, t(33.43) = 1.99, p = .004, d = 1.12 with presenters rated higher in intellect than in the reading condition.\nCan you interpret the other two?\n\n\nWithin-subjects\nThis one we will go through more quickly, with a lot of the work being repetitive of the above, but we run a within-subjects when participants complete both groups. Same people, different conditions.\nFor this we will be using another dataset, this time from the psychological sciences. Let’s look at the effect of behaviour automaticity using the Stroop task.\nYou can try the Stroop yourself here\nThe stroop task is one of the best known psychological experiments named after John Ridley Stroop. The Stroop phenomenon demonstrates that it is difficult to name the ink color of a color word if there is a mismatch between ink color and word. For example, the word GREEN printed in red ink.\nLoad in the stroop data and have a look\n\nstroop &lt;- read_csv(\"stroop_wk8.csv\")\n\nRows: 369 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): condition\ndbl (3): pID, time, avg_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(stroop)\n\n# A tibble: 6 × 4\n    pID condition     time avg_time\n  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1     1 control          3     3.33\n2     1 compatible       3     3.33\n3     1 incompatible     4     3.33\n4     2 control          3     4.33\n5     2 compatible       4     4.33\n6     2 incompatible     6     4.33\n\n\nWe should see that pID (participant ID) is repeated 3 times, one for each condition, and they then have a time for each, and an average time across all three conditions.\nNext, let’s plot condition and time in a boxplot\n\nstroop |&gt; \n  ggplot(aes(condition, time)) +\n  geom_boxplot()\n\n\n\n\nThat’s a funny looking plot, why is everything pushed way down? Looks like outliers! In this task, responses should be relatively swift (think back five minutes when you took the test, it is in the order of single seconds), but some people took a long time. Let’s filter out these outliers to keep only participants whose average time was under 12 seconds.\n\nstroop_filter &lt;- stroop |&gt; \n  filter(avg_time &lt; 12)\n\nTry plotting this new object\n\nstroop_filter |&gt; \n  ggplot(aes(condition, time)) +\n  geom_boxplot()\n\n\n\n\nMuch better! Looks like we can expect some kind of effect by eyeballing the plot, but let’s check it more properly.\nTo do this, we will first summarise the mean time taken by each condition in the Stroop task. In Week 3 we learnt how to use group_by() and summarise() to get summary stats (e.g., mean, sd) at each level of the IV. That’s what we want to do now:\nCopy the code below inton and edit the group_by() line to specify the IV and the summarise() line to calculate the mean() of our DV. If you do this correctly, you’ll get three values - a mean value for each level (condition) of our IV. Do these means reflect what you would expect in the Stroop task? Do they match the central tendency of the distributions you plotted?\n\nstroop_filter %&gt;%\n  group_by(name_of_IV_column) %&gt;% # you need to EDIT this for Q1\n  summarise(stroop_mean = mean(name_of_DV_column)) # you need to EDIT this for Q1\n\n\n\n# A tibble: 3 × 2\n  condition    stroop_mean\n  &lt;chr&gt;              &lt;dbl&gt;\n1 compatible          4.22\n2 control             5.69\n3 incompatible        7.28\n\n\nWe now need to filter our code to contain only two of the conditions, because a t-test only examines two groups. Let’s look at the differences between compatible and incompatible.\n\nstroop_comparison &lt;- \n  stroop_filter |&gt; \n  filter(condition %in% c(\"compatible\", \"incompatible\"))\n\nt.test(data = stroop_comparison, \n       time ~ condition, \n       paired = TRUE)\n\n\n    Paired t-test\n\ndata:  time by condition\nt = -18.257, df = 119, p-value &lt; 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -3.39021 -2.72679\nsample estimates:\nmean difference \n        -3.0585 \n\n\nAnd Cohen’s D\n\ncohens_d(time ~ condition, \n                      pooled_sd = FALSE, \n                      data = stroop_comparison)\n\nCohen's d |         95% CI\n--------------------------\n-1.65     | [-1.96, -1.35]\n\n- Estimated using un-pooled SD.\n\n\nIs our finding significant? What does the effect size tell us? How would we report and interpret our findings?\nCan you do the same t-test for the other possible group combinations? Don’t forget to conduct the multiple comparisons adjustment for the p-values."
  },
  {
    "objectID": "Worksheets/Worksheet_wk8.html#footnotes",
    "href": "Worksheets/Worksheet_wk8.html#footnotes",
    "title": "8. Correlations and t-tests",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSchroeder, J., & Epley, N. (2015). The sound of intellect: Speech reveals a thoughtful mind, increasing a job candidate’s appeal. Psychological science, 26(6), 877-891.↩︎"
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk1_Answers.html",
    "href": "Worksheets/Answers/Worksheet_wk1_Answers.html",
    "title": "1. Introduction to quantitative research methods using R - Answers",
    "section": "",
    "text": "This week, we will do our first steps in R. Please work through the following handout at your own pace.\nThree important things to remember:"
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk1_Answers.html#step-1-using-the-r-console",
    "href": "Worksheets/Answers/Worksheet_wk1_Answers.html#step-1-using-the-r-console",
    "title": "1. Introduction to quantitative research methods using R - Answers",
    "section": "Step 1: Using the R console",
    "text": "Step 1: Using the R console\nR is a command-based system. This means: You type commands (such as the ones highlighted below), R translates the commands into machine instructions, which your computer then executes.\nYou can type the R commands directly into the console. Or, as you will see later, you can also type commands into the script editor and run the sequence of commands as a batch or collection of lines.\nIn the next section, we will try out writing commands in the Console pane.\nCommands are typed at the command prompt &gt;. We then press Return (Mac) / Enter (Windows), and our command is executed. The output of the command, if there is any output, will be displayed on the next line(s)."
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk1_Answers.html#step-2-using-r-as-a-calculator",
    "href": "Worksheets/Answers/Worksheet_wk1_Answers.html#step-2-using-r-as-a-calculator",
    "title": "1. Introduction to quantitative research methods using R - Answers",
    "section": "Step 2: Using R as a Calculator",
    "text": "Step 2: Using R as a Calculator\nMost textbooks recommend familiarizing yourself with R and RStudio by first using R as a calculator. Let’s try this out.\nFor example, if you type 10 + 10 in the command prompt and press Return (Mac) / Return (Mac) / Enter (Windows), the result will be displayed underneath:\n\n10 + 10\n\n[1] 20\n\n\nPlease try out the following commands.\nIn the task below, just type the commands in the shaded area, then press Return (Mac) / Enter (Windows).\nNote: You don’t have the leave spaces around the operators (+, -, *, /, etc.), but the lines are more readable if you do.\nSo it’s better if you get used to writing 10 + 10 rather than 10+10, even though the output is the same.\nThe exception are negative values.\nHere, it is recommended not to leave a space between the minus sign - and the value we are negating. So, we would write -3, not - 3, even though it’s the same.\nPlease try out all of the commands on your computer now.\n\nThis is how we do addition\n\n\n10 + 10\n\n[1] 20\n\n\n\nSubtraction\n\n\n8 - 2\n\n[1] 6\n\n\n\nMultiplication\n\n\n10 * 14\n\n[1] 140\n\n\n\nDivision\n\n\n112/8\n\n[1] 14\n\n\n\nWe can also use exponents, i.e. raising a number to the power of another number, e.g., 2^8, as in the example below\n\n\n2 ^ 8\n\n[1] 256\n\n\n\nSquare root. This is done via a function called sqrt(). We will discuss functions later. For now, just add a numerical value in the parentheses of sqrt()\n\n\nsqrt(64)\n\n[1] 8\n\n\n\nYou can also combine +, -, *, /, ^ operators in your commands. By default, the precedence order of operations will be ^ followed by * or /, followed by + or -, just like in a calculator.\n\n\n2+3-4/2\n\n[1] 3\n\n\nor\n\n3+9/3*2^8\n\n[1] 771\n\n\n\nBut: You can use brackets () to overcome the default order to operations: Test the effect of bracketing on the precedence order of operations in the two examples below.\n\n\n#example a\n2 + 3 - 4 / 2\n\n[1] 3\n\n#example b\n(2 + 3 - 4) / 2\n\n[1] 0.5\n\n\nor\n\n#example a\n3 + 9 / 3 * 2 ^ 8\n\n[1] 771\n\n#Example b\n(3 + 9) / 3 * 2 ^ 8\n\n[1] 1024"
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk1_Answers.html#step-3-history",
    "href": "Worksheets/Answers/Worksheet_wk1_Answers.html#step-3-history",
    "title": "1. Introduction to quantitative research methods using R - Answers",
    "section": "Step 3: History",
    "text": "Step 3: History\nIn the Console pane, have you noticed that pressing the up and down arrows does not allow you to go through the different lines (as would happen in a text file, e.g. Word)? Instead, when you’re at the command prompt &gt;, pressing the up and down arrows allows you to move through the history of executed commands. This can save you a lot of time if you want to re- run one of the previous commands.\nGo ahead and rerun the last line using this method, but change the last number to a 4 (instead of an 8)\nIn the Environment, History, etc. pane, you can use the History tab to see your entire command history. If you click on any line in the History tab, it will re-run the command. Again, very helpful as it saves you a lot of typing. Let’s try this out.\nGo into the History pane and find the line where you ran sqrt(64) and rerun it"
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk1_Answers.html#step-4-incomplete-commands-and-escaping",
    "href": "Worksheets/Answers/Worksheet_wk1_Answers.html#step-4-incomplete-commands-and-escaping",
    "title": "1. Introduction to quantitative research methods using R - Answers",
    "section": "Step 4: Incomplete commands, and escaping",
    "text": "Step 4: Incomplete commands, and escaping\nWhat happens if you try to execute an incomplete command such as the one below?\n\n\n\n\n10 +\n\nError: &lt;text&gt;:2:0: unexpected end of input\n1: 10 +\n   ^\n\n\nYou will notice there is something missing (another number for the addition). Rather than giving us the result of the addition, R will just display a plus sign + in the console instead of the usual &gt;. This means that the command is incomplete. And: If you keep hitting Return (Mac) / Enter (Windows), you will just get more plus signs…\nYou can either complete the command on the next line (try adding a number to complete the addition), or you can just press Esc to exit the command"
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk1_Answers.html#step-5-variables",
    "href": "Worksheets/Answers/Worksheet_wk1_Answers.html#step-5-variables",
    "title": "1. Introduction to quantitative research methods using R - Answers",
    "section": "Step 5: Variables",
    "text": "Step 5: Variables\nThe next important step is to learn how to create variables and assign values to variables. In general, the assignment rule is:\nname &lt;- expression/value\nExpression - An expression is any R code that returns some value. This can be a single number, the result of a calculation, or a complex statistical analysis.\nAssignment operator - The &lt;- is called the assignment operator. This assigns everything that is to its right (the expression) to the variable on its left. Rather than typing &lt; and the minus sign, you can also simply press the shortcut Option+- (Mac) or Alt+- (Windows).\nName - The name is simply the name of the variable.\n\nrun the following\n\n\nx &lt;- 4 * 8\n\nIt appears that nothing happened; after all, there seems to be no output in the command prompt. However, something did happen after we executed the command x &lt;- 4 * 8.\nYou will see this when you execute the following command.\n\nx\n\n[1] 32\n\n\nAs you see, the previous command x &lt;- 4 * 8 assigned the multiplication 4 * 8 to the variable x.\nBy typing the name of the variable x in the command line and running it, the value of the variable will be displayed, in this case 32 (being the product of 4 * 8).\nThere is another way to confirm that you created a variable. If you check the Environment tab in the Environment, History, Connections, etc. pane at the top right of your RStudio window, you will now see that new variable listed.\nOnce you have created a variable, you can use it for other calculations just like you would with any other number\n\n\n\n\nx * 36\n\n[1] 1152\n\n\nWe can also assign any of these values to new variables.\n\n\n\n\ny&lt;-x*28\n\ny\n\n[1] 896"
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk1_Answers.html#step-6.-naming-your-variables",
    "href": "Worksheets/Answers/Worksheet_wk1_Answers.html#step-6.-naming-your-variables",
    "title": "1. Introduction to quantitative research methods using R - Answers",
    "section": "Step 6. Naming your variables",
    "text": "Step 6. Naming your variables\nThe name has to follow certain naming conventions. The variable name can consist of letters (upper or lower case), numbers, dots, and underscores. However, it must begin with a letter, or a dot that is not followed by a number. That is, a dot not followed by a number, OR a letter. If it is a dot and then number it will think of it as a decimal.\n\nWhich of the following would be okay?\n\n\ny157 &lt;- 2\nx_y_z &lt;- 2\nabc_123 &lt;- 2\n_x &lt;- 2\n.1px &lt;- 2\na_b_c &lt;- 2\nx-y-z &lt;- 2\n123 &lt;- 2\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nthese variable names would work:\n\ny157\nx_y_z\nabc_123\na_b_c\n\nthese will not:\n\n_x starts with an underscore\n.1px starts with a number\nx-y-z is trying to subtract z from y from x (it thinks you want to run a function)\n123 is just a number\n\n\n\n\nEven though you can use nonsense sequences such as the ones above, it is good practice to select variable names that are meaningful, short, without dots (use underscore _ instead), and ideally in lowercase characters. For example:\n\nage &lt;- 56\nincome &lt;- 101034\nis_married &lt;- TRUE\nyears_married &lt;- 27"
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk1_Answers.html#step-7-data-types",
    "href": "Worksheets/Answers/Worksheet_wk1_Answers.html#step-7-data-types",
    "title": "1. Introduction to quantitative research methods using R - Answers",
    "section": "Step 7: Data Types",
    "text": "Step 7: Data Types\nData types are variables that refer to collections of values. There are different types of data types (e.g., lists, matrices, arrays), but we will focus on two particularly important ones: vectors and data frames.\n\nVectors\nVectors are one-dimensional sequences of values. They are very simple but fundamental data structures, as you will see below when we talk about data frames. For this reason, it’s worth learning about vectors, how to create and manipulate them. Below we will cover numeric and character vectors.\nYou can create vectors by using the c() function. The c stands for combine. All the items within brackets, separated by commas, will be assigned to the variable on the left of the assignment operator.\nIf you type the following command, you will create a vector with seven elements. This is called a numeric vector as the elements within the brackets are numbers.\n\n\n\n\nnumbers &lt;- c(2, 3, 5, 7, 11, 13, 17)\n\nWe can now use this vector to perform all kinds of operations. Try out the following, for example.\n\n\n\n\nnumbers + 1\n\n[1]  3  4  6  8 12 14 18\n\nnumbers / 2\n\n[1] 1.0 1.5 2.5 3.5 5.5 6.5 8.5\n\nnumbers^2\n\n[1]   4   9  25  49 121 169 289\n\n\nNote how the operations are applied to each number in the vector.\nFor any vector (number sequence), we can also refer to individual numbers that form the vector. We do this by means of indexing operations []. For example, to get the first element of the vector, we can type the following. This will display the first element in the vector.\n\nnumbers[1]\n\n[1] 2\n\n\nYou can also extract more than one element from the vector, and even specify the order. For example:\nNote the difference in the example above and the one below. Above, we just used [1] and that was fine, but because of how indexing works (and it gets more complex when handling different datatypes, such as 2-dimensional tables), we need to use the c() function to demonstrate what we are after.\n\nnumbers[c(4,2,1)]\n\n[1] 7 3 2\n\n\nThis will retrieve the fourth element in the vector (the number 7), the first element (2) and the second (3).\n\n\n\n\n\n\nWhy is it different?\n\n\n\n\n\nWhat happens if you don’t use the c() function? Try it out and see what happens.\n\nnumbers[4,2,1]\n\nError in numbers[4, 2, 1]: incorrect number of dimensions\n\n\nYou get an error! Now, in this particular situation, the error message isn’t the most helpful, but it is important to get familiar with errors (you will be seeing a lot of them in the next 10 weeks!), and note that they are useful. R cannot carry out operations that don’t make sense, and as it happens, the above command is telling it to look for the item in three dimensions (imagine a cube made of vectors). When we get errors it is useful to check our code to see what might be wrong.\n\n\n\nOr you can refer to consecutive set of elements in the vector, such as the fourth to the seventh elements. For this, simply type:\n\nnumbers[4:7]\n\n[1]  7 11 13 17\n\n\nWe can also retrieve all elements with the exception of one. For this, we use the minus sign to exclude the vector element that we want to exclude, as in the following example.\n\nnumbers [-1]\n\n[1]  3  5  7 11 13 17\n\n\nFinally, we can also exclude a sequence of elements by adding the minus sign before the c() function. This will exclude from the output all elements that are specified within the brackets.\n\nnumbers [-c(1, 3, 5, 7)]\n\n[1]  3  7 13\n\n\nThe numbers vector is a sequence of numbers. We can find out the type of vector by using the function class(). This will confirm that numbers is a numeric vector.\n\nclass(numbers)\n\n[1] \"numeric\"\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nthe class() command is really useful, and one worth remembering for future troubleshooting. Sometimes commands don’t run as you want them to, and checking the class is what you want is a great sanity check. It has saved me on many occasions.\n\n\n\nWe can also create vectors with elements that are character strings. Here, each element needs to be surrounded by quotation marks (single or double), as in the example below\n\ncolleges &lt;- c('bowland', 'cartmel', 'county', 'furness', 'fylde', 'graduate', 'grizedale', 'lonsdale', 'pendle')\n\ncolleges\n\n[1] \"bowland\"   \"cartmel\"   \"county\"    \"furness\"   \"fylde\"     \"graduate\" \n[7] \"grizedale\" \"lonsdale\"  \"pendle\"   \n\n\nThis type of vector is character, how would you verify this?\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nclass(colleges)\n\n[1] \"character\"\n\n\n\n\n\nYou can index character vectors, just like numeric vectors.\n\ncolleges[3]\n\n[1] \"county\"\n\n\nBut you cannot perform arithmetic functions on character vectors, of course.\n\ncolleges*2\n\nError in colleges * 2: non-numeric argument to binary operator\n\n\n\nCoercing vectors\nIn vectors, all of the elements must be of the same type. For example, you cannot have a vector that has both numbers and character strings as elements. If you try to create a vector with both numbers and character strings, then some of your elements will coerced into other types.\nIf you type the following, you will see that the attempt to create a mixed vector with character strings (bowland, cartmel, county, fylde) and numbers (1, 2, 5, 7, 8) converted the numbers into character strings, as evidenced by the quotation marks. You cannot perform calculations on these numbers as R interprets them as text strings.\n\nc('bowland', 'cartmel', 'county', 'furness', 'fylde', 1, 2, 5, 7, 8)\n\n [1] \"bowland\" \"cartmel\" \"county\"  \"furness\" \"fylde\"   \"1\"       \"2\"      \n [8] \"5\"       \"7\"       \"8\"      \n\n\nFinally, you can also combine vectors using thec()function. For example, we can create a new vector called new_numbers by combining the original numbers vectors and adding the squares and cubes of numbers.\n\nnew_numbers &lt;- c(numbers, numbers ^ 2, numbers ^ 3)\n\nnew_numbers\n\n [1]    2    3    5    7   11   13   17    4    9   25   49  121  169  289    8\n[16]   27  125  343 1331 2197 4913\n\n\nWe have now produced a new vector new_numbers with 21 elements. These don’t fit all in a line and so are wrapped over two lines. The first row displays elements 1 to 12, and the second row begins with element 13. Now you can also see the meaning of the [1] on the output. The [1] is just the index of the first element of the vector show on the corresponding line. [1] refers to the first element in our vector (the number 2), and [13] refers to the 13th element in our vector (169).\n\n\nNaming Vectors\nThe elements of a vector can be named, too. Each element in our vector can have a distinct label, which can be useful.\n\nages &lt;- c(bob = 27, bill = 34, charles = 76)\n\nages\n\n    bob    bill charles \n     27      34      76 \n\n\nWe can now access the values of the vector by the label or by the index as before.\n\nages['bill']\n\nbill \n  34 \n\nages[2]\n\nbill \n  34 \n\n\nWe can also add names to existing vectors by using the names() function. In the following example, we first assign new values to the vector ages. This will delete the previous numbers and labels. We then assign names to this vector using the names() function.\n\nages &lt;- c(23, 54, 8)\nnames(ages) &lt;- c(\"michaela\", \"jane\", \"jacques\")\nages\n\nmichaela     jane  jacques \n      23       54        8 \n\n\n\n\nMissing Values\nLast but not least. Sometimes, we have missing values in our data. In R, missing values are denoted by NA. This is not treated as a character string but as a special symbol.\nYou can insert a placeholder for a missing value into a numeric or character vector by simply typing NA in the list of elements.\n\na&lt;-c(1,5,7,NA,11,14)\na\n\n[1]  1  5  7 NA 11 14\n\nb &lt;- c('michaela', 'bill', NA, 'jane')\nb\n\n[1] \"michaela\" \"bill\"     NA         \"jane\"    \n\n\n\n\n\nDataframes\nData frames are probably the most important data structure in R. They are the default form for representing data sets for statistical analyses.\nData frames have a certain number of columns, and each column has the same number of rows. Each column is a vector, and so data frames are essentially collections of equal-length vectors. (This is also why we spent so much time on vectors above…)\nThere are two ways of creating data frames. We can create a data frame in R by importing a data file, usually in .csv or .xlsx format. (We will discuss how to import files next week.)\nOr we can use the data.frame() function to create a data frame from scratch, as in the following example.\n\ndata_df &lt;- data.frame(name = c('bill', 'jane', 'jacques'), age = c(23, 54, 8))\n\ndata_df\n\n     name age\n1    bill  23\n2    jane  54\n3 jacques   8\n\n\nAs you can see, we have now created a data frame with two columns (name, age) and three rows (displaying the name and age of each person, Bill, Jane and Jacques). The columns are our variables and the rows are our observations of these variables.\nWe can refer to specific elements of the data frame by using indices. In the example below, there are two elements within the index [ ], one for the rows, one for the columns. These are separated by a comma [ , ].\nFor example, to refer to the element that is in the second row, first column, you would type the following.\n\ndata_df[2 ,1]\n\n[1] \"jane\"\n\n\nYou can also retrieve multiple elements. One way of doing this is by leaving one of the indices blank.\nIf you leave the first index blank (e.g., [ , 1]), then you are telling R that you want the information from all the rows. In the example below, you retrieve the information that is in all the rows that are in column 1.\n\ndata_df[ ,1]\n\n[1] \"bill\"    \"jane\"    \"jacques\"\n\n\nIn contrast, if you leave the second index blank ([2, ], you will retrieve the information found in the second row across the two columns.\n\ndata_df[2 , ]\n\n  name age\n2 jane  54\n\n\nWe can also be more specific. For example, you can refer to the first and third row of the second column, we would type:\n\ndata_df[c(1, 3), 2]\n\n[1] 23  8\n\n\nOn the other hand, we can also refer to a column by name. To do this, we need to use the $ notation.\n\ndata_df$name\n\n[1] \"bill\"    \"jane\"    \"jacques\"\n\ndata_df$age\n\n[1] 23 54  8\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nDid you notice that RStudio will automatically suggest the variable names after you typed the $? The code completion feature in RStudio makes writing and executing code much easier!\nIt will do this for pretty much everything that is either a function, a variable, or even an argument (we come to those later), provided that you have typed at least three characters (or press Tab to get there faster)"
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk1_Answers.html#step-8-functions",
    "href": "Worksheets/Answers/Worksheet_wk1_Answers.html#step-8-functions",
    "title": "1. Introduction to quantitative research methods using R - Answers",
    "section": "Step 8: Functions",
    "text": "Step 8: Functions\nWhile data structures hold data in R, functions are used to do things with the data. You have already encountered a few functions above, namely sqrt(), c(), and data.frame().\nAcross all R packages and R’s standard library, there are tens of thousands of functions available for you to use. However, most of our analyses in this course will require only a relatively small number of functions.\nBelow is a general introduction to functions; we will cover functions in more detail as we progress through this course.\nFunctions tend to have the following structure:\nfunction(argument 1, argument 2, argument 3, ...)\nWe can think of functions as actions and arguments as the inputs, i.e. something the functions act on. Most functions require at least one argument. If they have more than one argument, these are separated by commas as in the example above.\nFor example, see what happens when you type the following two commands.\n\nsqrt()\n\nError in sqrt(): 0 arguments passed to 'sqrt' which requires 1\n\nsqrt(4)\n\n[1] 2\n\n\nThe function sqrt() requires an argument; if you fail to supply an argument you will get an error message (Error in sqrt()…)\nHere are another few functions that are helpful for our analyses.\nWe can count the number of elements in a given vector by using the length() function.\n\nlength(new_numbers)\n\n[1] 21\n\n\nWe can calculate sum, mean, median, and standard deviation as follows. (More on this in future sessions when we discuss exploratory data analysis.)\n\nsum(new_numbers)\n\n[1] 9668\n\nmean(new_numbers)\n\n[1] 460.381\n\nmedian(new_numbers)\n\n[1] 25\n\nsd(new_numbers)\n\n[1] 1152.162\n\n\nYou can also nest functions inside each other, such as:\n\nround(sqrt(mean(new_numbers)))\n\n[1] 21\n\n\nHere, we use three functions, each nested within the other. In the example, we first calculated the mean of the vector new_numbers (460.381), then the square root of this value and finally rounded it. We could have done the same calculation in three steps, as in the example below, but nesting the functions in a single command allows us to be more efficient.\nNow, I don’t like nesting functions and neither should you. Next session I will cover this more clearly, and introduce you to a set of functions and stylistic choices that are the gold standard in psychology and have a huge user base (meaning there’s plenty of help out there). For now, just be in awe of the complexity of carrying out multiple functions in R.\n\nOptional arguments\nIn some cases, functions can also take an additional argument. A good example is the mean() function, which can an additional argument called trim. If we add the trim argument to the mean function, the command will first remove a certain proportion of the extreme values of the vector and then calculate the mean. This is very useful when we are dealing with outliers in our data, for example. (We will talk more about outliers in future sessions.)\nIn order to trim observations, we need to specify a value between 0 to 0.5. This will trim the highest and the lowest values before calculating the mean. Naturally, a trim value of 0 means you’re not trimming anything, so the value assigned to trim should be greater than 0. For example, a value of 0.1 means you’re trimming the 10% highest and lowest observations, 0.2 means you’re trimming the 20% highest and lowest, and so forth.\n\nmean(new_numbers)\n\n[1] 460.381\n\nmean(new_numbers, trim=0.0)\n\n[1] 460.381\n\nmean(new_numbers, trim=0.1)\n\n[1] 150.1765\n\nmean(new_numbers, trim=0.2)\n\n[1] 66.92308\n\nmean(new_numbers, trim=0.3)\n\n[1] 44.11111\n\nmean(new_numbers, trim=0.4)\n\n[1] 26.2\n\nmean(new_numbers, trim=0.5)\n\n[1] 25"
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk1_Answers.html#step-9-help-pages",
    "href": "Worksheets/Answers/Worksheet_wk1_Answers.html#step-9-help-pages",
    "title": "1. Introduction to quantitative research methods using R - Answers",
    "section": "Step 9: Help Pages",
    "text": "Step 9: Help Pages\nRStudio has very helpful pages for the available functions. This is useful when you’re not sure if a function requires an argument, or if you’re in doubt about the use of arguments such as trim.\nYou can access the help page for a given function in different ways.\nThe most efficient one is by typing a question mark and the function name in the command line. If you now press Return (Mac) / Enter (Windows), this command will also open the help page for the function. Alternatively, you can use thehelp()function in the command line, as below.\n\n?mean()\n\nhelp('mean')\n\nThen there is my preferred option (and again, when we start using the script pane, I will make this clear):\nYou can also go to the search line of the Help tab in the Files, Plots, Packages, Help pane (bottom right of the screen) and type the name of the function there (mean). There is a useful shortcut to search R Help, namely Ctrl+Option+F1 (Mac) and Ctrl+Alt+F1 (Windows).\nIn each of the cases above, RStudio will open the help page for the function in question in the Help tab."
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk1_Answers.html#take-home-task",
    "href": "Worksheets/Answers/Worksheet_wk1_Answers.html#take-home-task",
    "title": "1. Introduction to quantitative research methods using R - Answers",
    "section": "Take home task",
    "text": "Take home task\nThe following table displays the scores of students in two foreign language exams, one administered at the beginning of term, the other at the end of term.\nCreate a data frame called language_exams with the information provided in the table, then answer the questions below using R. To save you the headache of tediously typing it all out, you can highlight and paste the code below as-is into your own script.\n\nlanguage_exams &lt;- data.frame(\n  student_id = c('Elin', 'Spencer', 'Crystal', 'Arun', 'Lina', 'Maximilian', 'Leyton', 'Alexandra', 'Valentina', 'Lola', 'Garfield', 'Lucy', 'Shania', 'Arnold', 'Julie', 'Michaela', 'Nicholas'), \n  exam_1 = c(93, 89, 75, 52, 34, 50, 46, 62, 84, 68, 74, 51, 84, 34, 57, 25, 72), \n  exam_2 = c(98, 96, 94, 65, 50, 68, 58, 77, 95, 86, 89, 70, 90, 50, 67, 37, 90))\n\n\nWhat are the mean scores for exam 1 and exam 2?\n\n\nmean(language_exams$exam_1) \n\n[1] 61.76471\n\nmean(language_exams$exam_2)\n\n[1] 75.29412\n\n\n\nWhat is the difference between the two means?\n\n\nmean(language_exams$exam_2) - mean(language_exams$exam_1)\n\n[1] 13.52941\n\n\n\nWhat are the mean scores for the two exams if you remove extreme values (the top and bottom 20%) from each?\n\n\nmean(language_exams$exam_1, trim=0.2) \n\n[1] 62.81818\n\nmean(language_exams$exam_2, trim=0.2)\n\n[1] 77.63636\n\n\n\nBased on the previous step (with outliers removed): What is the difference between the two means now? Please round the value before reporting the result.\n\n\nmean(language_exams$exam_1, trim=0.2) # Calculate mean scores exam 1 without outliers\n\n[1] 62.81818\n\nmean(language_exams$exam_2, trim=0.2) # Calculate mean scores exam 2 without outliers\n\n[1] 77.63636\n\n77.63636 - 62.81818 # Difference between the trimmed mean exam 2 and trimmed mean exam 1\n\n[1] 14.81818\n\nround(14.81818) #Rounds the value\n\n[1] 15\n\n\n\nCan you do steps 3 and 4 in a single command?\n\n\n#the short version\nround(mean(language_exams$exam_2, trim=0.2) - mean(language_exams$exam_1, trim=0.2))\n\n[1] 15"
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk3.html",
    "href": "Worksheets/Answers/Worksheet_wk3.html",
    "title": "3. Exploratory Data Analysis - Answers",
    "section": "",
    "text": "Three important things to remember:"
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk3.html#step-0-preparing-your-environment",
    "href": "Worksheets/Answers/Worksheet_wk3.html#step-0-preparing-your-environment",
    "title": "3. Exploratory Data Analysis - Answers",
    "section": "Step 0: Preparing your Environment",
    "text": "Step 0: Preparing your Environment\nFirt things first, open up a new R script and load in the Tidyverse library\n\n\n\n\n\n\nNeed a reminder?\n\n\n\n\n\n\nlibrary(tidyverse)\n\n\n\n\nIn addition, please download the following data files from Moodle (see folder for session 3) and place them in your working directory.\n\nnettle_1999_climate.csv\nlanguage_exams_new.csv\nscores.csv"
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk3.html#step-1-tibbles",
    "href": "Worksheets/Answers/Worksheet_wk3.html#step-1-tibbles",
    "title": "3. Exploratory Data Analysis - Answers",
    "section": "Step 1: Tibbles?",
    "text": "Step 1: Tibbles?\nIn the last session, you have learned how to install the tidyverse package (Wickham, 2017).\ntidyverse is a collection of packages that great facilitates data handling in R. In our session on data visualization, you will encounter the ggplot2 package (Wickham, 2016), which is part of tidyverse. Today, we will use functions from other important packages of the tidyverse, namely tibble (Müller & Wickham, 2018), readr (Wickham et al., 2022), and dplyr (Wickham et al., 2018). These are all automatically installed when you install the tidyverse.\nWe have actually used tibbles last week. When we use the function read_csv(), this reads dataframes and gives them a class of tibble as well as dataframe. We can interpret this as meaning we can apply functions or operations that can apply to both classes.\nTibbles are like the data frames but better. For example, they load much faster, which is important when you are dealing with lots of data.\nFirst, let us load in a tibble/dataframe called languages. The data set comes from Winter’s (2019) textbook.\n\nlanguages &lt;- read_csv('nettle_1999_climate.csv')\n\nRows: 74 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Country\ndbl (4): Population, Area, MGS, Langs\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nclass(languages)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\n\nWhat classes are associated with the object languages? Which one means it has been read in as a tibble?\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n“tbl” is short for tibble!\n\n\n\nRealistically, in future steps/worksheets/discussions, provided we are all on the same page and using read_csv() not read.csv(), then the terms tibble and dataframe are interchangeable.\nFor completeness sake, and so we can see why working with tibbles is easier, let us read the data again using read.csv(), have a look and compare it against the stored langages object.\nFor this, let us just read the csv without saving it to an object, you may recall that in week 1, we played about getting R to output various things, we told it to print 2 + 2 and it did just that. It gave us the number 4 in the output. And notice that when we ran languages &lt;- read_csv('nettle_1999_climate.csv') above, we did not get any output. This is because R did what we asked, which was to assign the values of the file to the object called “languages”. Got it? Cool, we can get rid of the left hand side of the assignment operator and just have R read (and print) the values of a file to us. In practice, this is not a useful exercise, but as an educational step, it’s convenient.\nLet us run the following command again to have it stored in the environment properly, notice the function is read_csv.\n\nlanguages &lt;- read_csv('nettle_1999_climate.csv')\n\nRows: 74 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Country\ndbl (4): Population, Area, MGS, Langs\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAnd now:\n\nlanguages\n\n# A tibble: 74 × 5\n   Country      Population  Area   MGS Langs\n   &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Algeria            4.41  6.38  6.6     18\n 2 Angola             4.01  6.1   6.22    42\n 3 Australia          4.24  6.89  6      234\n 4 Bangladesh         5.07  5.16  7.4     37\n 5 Benin              3.69  5.05  7.14    52\n 6 Bolivia            3.88  6.04  6.92    38\n 7 Botswana           3.13  5.76  4.6     27\n 8 Brazil             5.19  6.93  9.71   209\n 9 Burkina Faso       3.97  5.44  5.17    75\n10 CAR                3.5   5.79  8.08    94\n# ℹ 64 more rows\n\n\nWhat’s the difference? As you can see above, the tibble has information about the number of observations (74) and variables (5), the names of the variables: Country, Population, Area, MGS (mean growing season, measured by number of months when crops grow), Langs, and the variable types (character: chr, doubles: dbl, which is a type of numeric vector, and integer: int, also a numeric vector). In addition, the command will display the first ten observations of the variables, lines 1 to 10."
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk3.html#step-2-data-wrangling",
    "href": "Worksheets/Answers/Worksheet_wk3.html#step-2-data-wrangling",
    "title": "3. Exploratory Data Analysis - Answers",
    "section": "Step 2: Data wrangling",
    "text": "Step 2: Data wrangling\nWe will now use tidyverse functions for data wrangling. As discussed in our last session, data wrangling is also referred to as data pre-processing or data cleaning. It simply means preparing your raw data (e.g., the data files from experimental software) for statistical analyses. This entails, for example, dealing with missing values, relabeling variables, changing the variable types, etc.\nIn this part of the handout, we will look at a few tidyverse function that you can use for data wrangling. For more information, I recommend Chapter 3 of Andrews (2021), which provides a comprehensive introduction to data wrangling using tidyverse.\nLet’s look at five useful functions for data wrangling with tibbles: filter, select, rename, mutate, and arrange.\n\nFilter\nThe filter() function can be used, unsurprisingly, to filter rows in your tibble. The filter() function takes the input tibble as its first argument. The second argument is then a logical statement that you can use to filter the data as you please.\n\n\n\n\n\n\nNote\n\n\n\nWe are using pipes, as introduced last week. So here’s some more practice with them coming up. filter() takes the input as the first argument, which means we can pipe the tibble into the filter function.\n\n\nIn the following example, we are reducing the languages tibble to only those rows with countries that have more than 500 languages.\n\nlanguages |&gt; #pipe object languages\n  filter(Langs &gt; 500) # filter rows where variable (column) Langs is greater than 500\n\n# A tibble: 2 × 5\n  Country          Population  Area   MGS Langs\n  &lt;chr&gt;                 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Indonesia              5.27  6.28  10.7   701\n2 Papua New Guinea       3.58  5.67  10.9   862\n\n\nOr if you are interested in the data from a specific country (say, Angola), you could simply run the following command. This will only display the rows for Angola.\n\nlanguages |&gt; \n  filter(Country == \"Angola\")\n\n# A tibble: 1 × 5\n  Country Population  Area   MGS Langs\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Angola        4.01   6.1  6.22    42\n\n\nWe can even start to get a little bit more adventurous and filter on more than one argument. What about finding countries with more than 500 languages and Population greater than 4?\n\n\n\n\n\n\nWarning\n\n\n\nWait! Before running the next line, think about what we are trying to find, look at the previous code we have run in this section, and decide what the expected output should be.\nHow many rows should be returned? What countries are they going to be?\nThis kind of mental engagement is critical for your own development - we should also have some kind of mental representation of the information we are getting. It won’t always be super specific, but even just being aware of the shape of a tibble, or general description of the information is important - it means we can check for issues quicker.\n\n\n\nlanguages |&gt; \n  filter(Langs &gt; 500, Population &gt; 4)\n\n# A tibble: 1 × 5\n  Country   Population  Area   MGS Langs\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Indonesia       5.27  6.28  10.7   701\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nIndonesia. Look at the output for the code below and notice that it is the same code except we filtered it one step further.\n\nlanguages |&gt; #pipe object languages\n  filter(Langs &gt; 500)\n\n# A tibble: 2 × 5\n  Country          Population  Area   MGS Langs\n  &lt;chr&gt;                 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Indonesia              5.27  6.28  10.7   701\n2 Papua New Guinea       3.58  5.67  10.9   862\n\n\n\n\n\n\n\nSelect\nIn contrast, you can use the select() function to select specific columns. To do this, simply add the columns you wish to select, separated by commas, as arguments in the function.\n\nlanguages |&gt; \n  select(Langs, Country)\n\n# A tibble: 74 × 2\n   Langs Country     \n   &lt;dbl&gt; &lt;chr&gt;       \n 1    18 Algeria     \n 2    42 Angola      \n 3   234 Australia   \n 4    37 Bangladesh  \n 5    52 Benin       \n 6    38 Bolivia     \n 7    27 Botswana    \n 8   209 Brazil      \n 9    75 Burkina Faso\n10    94 CAR         \n# ℹ 64 more rows\n\n\nAs you might notice, the select() function can also be used to change the sequence of the columns. (In the original tibble, Country came first, followed by Langs.)\nOn the other hand, if you wish to exclude a column, you can do this by using a minus sign in front of the column in question. The command below will select the four columns Country, Population, Area and MGS, but excluded Langs as requested.\n\nlanguages |&gt; \n  select(-Langs)\n\n# A tibble: 74 × 4\n   Country      Population  Area   MGS\n   &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Algeria            4.41  6.38  6.6 \n 2 Angola             4.01  6.1   6.22\n 3 Australia          4.24  6.89  6   \n 4 Bangladesh         5.07  5.16  7.4 \n 5 Benin              3.69  5.05  7.14\n 6 Bolivia            3.88  6.04  6.92\n 7 Botswana           3.13  5.76  4.6 \n 8 Brazil             5.19  6.93  9.71\n 9 Burkina Faso       3.97  5.44  5.17\n10 CAR                3.5   5.79  8.08\n# ℹ 64 more rows\n\n\nYou can also select consecutive columns using :. Let’s take the columns from and including Population, to and including MGS.\n\nlanguages |&gt; \n  select(Population:MGS)\n\n# A tibble: 74 × 3\n   Population  Area   MGS\n        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1       4.41  6.38  6.6 \n 2       4.01  6.1   6.22\n 3       4.24  6.89  6   \n 4       5.07  5.16  7.4 \n 5       3.69  5.05  7.14\n 6       3.88  6.04  6.92\n 7       3.13  5.76  4.6 \n 8       5.19  6.93  9.71\n 9       3.97  5.44  5.17\n10       3.5   5.79  8.08\n# ℹ 64 more rows\n\n\nIt is worth noting that getting filter() and select() mixed up is pretty common. I personally do it a lot. A vaguely helpful way to remember is that filteR is for Rows and select is not. Or seleCt is for Columns? Or jsut do what I do, and get it wrong 50% of the time, and then just change it when you get the error!\n\n\nRename\nA third useful function is called rename(). This function can be used to change the name of columns. To do this, you first write the new column (here, Population) followed by an equal sign (=) and the old column name (Pop).\nrename() is useful for when we have really messy taking that comes from an online survey, government statistics, or even a PsychoPy study. We could tidy data in excel before reading into R, but that defeats the point of using R and being a successful data scientist (which is you!) - it reduces the reproduciblity and transparency of your analyses from start to finish.\n\nlanguages &lt;- languages |&gt; #here we are manipulating in place the object languages. We are overwriting the existing object\n  rename(Pop = Population)\n\n\n\n\n\n\n\nWarning\n\n\n\nWoah, woah, woah - what did I just do to the object languages? I assigned the object languages to languages? Not quite. I assigned the entire of the right hand side of the assignment operator to overwrite the same object. So I renamed a column and saved it to the existing object. Analogy: like saving over the top of a word document. Or, to see a basic example in action, make sure you have the environment pane selected in the top right and run the next command, see what shows, and then run the second and see what changed. It should be pretty clear what will happen, but just extrapolate that to tibbles and renaming (we could have also reassigned any of the previous actions we took using filter or select).\n\nx &lt;- 32\n\nx &lt;- 64\n\n\n\n\n\nMutate\nThe mutate() function can be used to change the content of a tibble. For example, you can add an additional column, which in the example below will be the Langs column divided by 100.\n\nlanguages &lt;- languages |&gt; \n  mutate(Langs100 = Langs/100)\n\nhead(languages) # show me the top 6 rows\n\n# A tibble: 6 × 6\n  Country      Pop  Area   MGS Langs Langs100\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 Algeria     4.41  6.38  6.6     18     0.18\n2 Angola      4.01  6.1   6.22    42     0.42\n3 Australia   4.24  6.89  6      234     2.34\n4 Bangladesh  5.07  5.16  7.4     37     0.37\n5 Benin       3.69  5.05  7.14    52     0.52\n6 Bolivia     3.88  6.04  6.92    38     0.38\n\n\nWe can also use mutate() to change specific values to something else. Look at this example dataset:\n\n#let's create a 2x3 tibble with two participants, who both took part in a study where we tested their working memory, and one was in the control group (1 for yes, 0 for no) and the other was not\ntest_data &lt;- tibble(name = c(\"Jenny\", \"Jonny\"), \n                    score = c(45, 23),\n                    control_group = c(1,0))\n\ntest_data\n\n# A tibble: 2 × 3\n  name  score control_group\n  &lt;chr&gt; &lt;dbl&gt;         &lt;dbl&gt;\n1 Jenny    45             1\n2 Jonny    23             0\n\n\nLooking at the variable control_group it may not be clear to everyone what 1 or 0 means, so let’s use mutate() to update this variable\n\ntest_data_update &lt;- test_data |&gt; \n  mutate(control_group = str_replace(control_group, \"1\",  \"yes\"),\n         control_group = str_replace(control_group, \"0\",  \"no\"))\n\nLooks pretty complicated! Let’s break it down:\n\nThe first line tells us what the new object will be, and what data we are starting with, we are then piping that into:\nmutate() which then tells us we want to update the control_group variable. and we want to apply a new function called str_replace which replaces strings. We are saying “look in the variable control group, and if we find a ‘1’, replace it with the string ‘yes’. We end the second line with a comma telling R we are doing something more\nline three then repeats line two (still inside mutate if you follow the parentheses), and looks for a ‘0’ and replaces it with ‘no’. And we finish there.\n\nCheck out both obhjects, what’s changed?\n\ntest_data\n\n# A tibble: 2 × 3\n  name  score control_group\n  &lt;chr&gt; &lt;dbl&gt;         &lt;dbl&gt;\n1 Jenny    45             1\n2 Jonny    23             0\n\ntest_data_update\n\n# A tibble: 2 × 3\n  name  score control_group\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;        \n1 Jenny    45 yes          \n2 Jonny    23 no           \n\n\nThe arguments in str_replace() want three things: the column it is checking, the strong it is looking for, and the string to replace it with. What if we wanted 1 to equal “control” and 0 to equal “experiment” instead? Try it out.\nWe will explore mutate() more in later weeks, but know that it is a very useful and powerful tool. It will be a common tool in your data science belt. It can be used in conjunction with other functions to do some really cool things. As a quick teaser, what if we wanted to know the population density per area unit?\n\nlanguages &lt;- languages |&gt; \n  mutate(density = Pop/Area) #We divide the total population by the area\n\nIf you took the time to calculate each row, you will see that for every row mutate() has taken the specific value of Pop and divided it by Area. Neat, that will save us a lot of time! Just know that we can make these even more complex…\n\n\nArrange\nFinally, the arrange() function can be used to order a tibble in ascending or descending order. In the example below, we are use this function to first look at the countries with the smallest number of languages (Cuba, Madagascar, etc.), followed by the countries with the largest numbers of languages (Papua New Guinea, Indonesia, Nigeria, etc.).\n\nlanguages |&gt; \n  arrange(Langs)\n\n# A tibble: 74 × 7\n   Country        Pop  Area   MGS Langs Langs100 density\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1 Cuba          4.03  5.04  7.46     1     0.01   0.800\n 2 Madagascar    4.06  5.77  7.33     4     0.04   0.704\n 3 Yemen         4.09  5.72  0        6     0.06   0.715\n 4 Nicaragua     3.6   5.11  8.13     7     0.07   0.705\n 5 Sri Lanka     4.24  4.82  9.59     7     0.07   0.880\n 6 Mauritania    3.31  6.01  0.75     8     0.08   0.551\n 7 Oman          3.19  5.33  0        8     0.08   0.598\n 8 Saudi Arabia  4.17  6.33  0.4      8     0.08   0.659\n 9 Honduras      3.72  5.05  8.54     9     0.09   0.737\n10 UAE           3.21  4.92  0.83     9     0.09   0.652\n# ℹ 64 more rows\n\n\nor in descending order?\n\nlanguages |&gt; \n  arrange(desc(Langs))\n\n# A tibble: 74 × 7\n   Country            Pop  Area   MGS Langs Langs100 density\n   &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1 Papua New Guinea  3.58  5.67 10.9    862     8.62   0.631\n 2 Indonesia         5.27  6.28 10.7    701     7.01   0.839\n 3 Nigeria           5.05  5.97  7      427     4.27   0.846\n 4 India             5.93  6.52  5.32   405     4.05   0.910\n 5 Cameroon          4.09  5.68  9.17   275     2.75   0.720\n 6 Mexico            4.94  6.29  5.84   243     2.43   0.785\n 7 Australia         4.24  6.89  6      234     2.34   0.615\n 8 Zaire             4.56  6.37  9.44   219     2.19   0.716\n 9 Brazil            5.19  6.93  9.71   209     2.09   0.749\n10 Philippines       4.8   5.48 10.3    168     1.68   0.876\n# ℹ 64 more rows\n\n#which is functionally equivalent to:\n#\n#languages |&gt; \n# arrange(-Langs)\n\nBefore we move on, let’s clean up our environment and we are wanting to start with a fresh one for the rest of the worksheet.\nWe can use the function rm() to remove items from the environment pane. We should have two items, x and languages.\n\nrm(x)\nrm(languages)"
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk3.html#step-3-exploratory-data-analysis",
    "href": "Worksheets/Answers/Worksheet_wk3.html#step-3-exploratory-data-analysis",
    "title": "3. Exploratory Data Analysis - Answers",
    "section": "Step 3: Exploratory Data Analysis",
    "text": "Step 3: Exploratory Data Analysis\nWe are now ready for some exploratory data analysis in R. First, let’s load the three data files as tibbles. To load the data sets as tibbles, we use the read_csv() function\n\n\nRows: 74 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Country\ndbl (4): Population, Area, MGS, Langs\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 1000 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): student_id, age, exam_1, exam_2, exam_3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 21 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): scores\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nOnce you have loaded the data sets and created the new tibbles, it’s good to inspect the data to make sure all was imported properly. This is important before you do any analyses. Remember: “Garbage in, garbage out.”\nYou can use the View(), head(), and str() functions. Personally I suggest using head() to get a first idea.\nWe are now ready to calculate a few summary statistics! We did some of this in prior worksheets, but we will see some more here.\n\nMean\nTo calculate the arithmetic mean, you can use the mean() function.\n\nmean(test_scores$scores)\n\n[1] 612.2381\n\n\nIn the case of language_exams_new, we have three exams for which we might want to know the mean.\n\nmean(language_exams_new$exam_1)\n\n[1] 68.052\n\nmean(language_exams_new$exam_2)\n\n[1] 75.055\n\nmean(language_exams_new$exam_3)\n\n[1] 87.918\n\n\nBut rather than calculating the mean for each column separately (exam_1 to exam_3), we can use the colMeans() function to calculate the mean for all columns.\n\ncolMeans(language_exams_new)\n\nstudent_id        age     exam_1     exam_2     exam_3 \n 16326.612     24.705     68.052     75.055     87.918 \n\n\nNote that for student_id the output is essentially meaningless; it might be worth to filter out this variable (well, unselect the column. We could do this using two pipes:\n\nlanguage_exams_new |&gt; # take the object language_exams_new\n  select(-student_id) |&gt; # deselect the column student_id\n  colMeans() # apply the function colMeans()\n\n   age exam_1 exam_2 exam_3 \n24.705 68.052 75.055 87.918 \n\n\nIf the above looks really complicated, it isn’t - don’t worry! Let us break it down. Look at the first two lines of code: we take the object language_exams_new and pipe it into the select function and remove the column student_id. We’ve done this before, this isn’t new. We are then seeing another pipe! This means that everything on the LHS of that pipe is shifted into the first spot of the RHS, so we are taken the object minus student_id and applying col_means(). That wasn’t so bad, it’s quite clear when we think about it programmatically.\nConsider that we can take this same approach and keep adding pipes for ever and ever, always doing the same thing: evaluate the first line and the bit between two pipes, then all of the LHS goes through the next pipe into the next function, and then so on, ever building in its pipeline, creating something complex at the end. That’s enough of this for now, we can get more practice later on, but it will help you to structure and read complicated code and make you into the great data scientist that you are.\n\n\nTrimmed Mean\nThe trimmed mean is useful when we want to exclude extreme values. Remember though that any data exclusion needs to be justified and it needs to be described in your report. There is a significant degree of trust that you report everything that you carried out in your analysis transparently.\nCompare the two outputs, with and without the extreme values. The trim argument here deletes the bottom and top 10% of scores.\n\nmean(test_scores$scores)\n\n[1] 612.2381\n\nmean(test_scores$scores, trim = 0.1)\n\n[1] 508.5882\n\n\n\n\nMedian\nThe median is calculated by using the following command.\n\nmedian(test_scores$scores)\n\n[1] 465\n\nmedian (language_exams_new$exam_1)\n\n[1] 68\n\n\n\n\nStandard Deviation\n\nsd(test_scores$scores)\n\n[1] 552.2874\n\nsd(language_exams_new$exam_1)\n\n[1] 2.049755\n\n\n\n\nRange\nThe range can provide useful information about our sample data. For example, let’s calculate the age range of participants in language_exams_new.\nThe range() function does not give you the actual range. It only provides the minimum and maximum values.\n\nrange(language_exams_new$age)\n\n[1] 17 38\n\n\nTo calculate the range, we can ask R to give us the different between the two values reported by range\n\nrange(language_exams_new$age) |&gt; \n  diff()\n\n[1] 21\n\n\ndiff() is a new function, and one we probably won’t use too much more, so it doesn’t deserve much space, diff is short for difference, so it calculates the difference between two values. That’s all really, but look how we take one output that wasn’t useful by itself and applied a new function to get something useful!\n\n\n\n\n\n\nNote\n\n\n\nIf you want to know a (not-so) fun fact, before writing this worksheet, I didn’t even know that diff() existed. I guessed it would (which is based off experience, so not helpful for you to know), but I then checked it did what I thought it did by running ?diff (in the console, NOT script) and it confirmed my suspicions. I could also have just googled “R difference between two values” and read the top two results.\n\n\n\n\nQuantiles\nQuantiles can easily be displayed by means of the quantile() function, as you can see in the example below.\n\nquantile(language_exams_new$exam_1)\n\n  0%  25%  50%  75% 100% \n  62   67   68   69   74 \n\n\nIn our exam_1 data, each of the quantiles above tells us how many scores are below the given the value. For example, 0% of scores are below 62, meaning that 62 is the lowest score. 50% of scores are below 68 and 50% above 68. (The 50% quantile is the median.) And 100% of scores are below 74, meaning that 74 is the highest score in the data set.\nWe can confirm that this is true by using the range() function, which confirms 62 and 74 as minimum and maximum values. Can you remember (or look back) to see how we would do this?\n\n\n[1] 62 74\n\n\nSometimes, we want to calculate a specific one, e.g. a percentile. For this, you can add the following arguments to the quantile() function. For example, to display the 10% and 90% quantiles, you would add 0.1 and 0.9 respectively, as in the examples below.\n\nquantile(language_exams_new$exam_1, 0.1)\n\n10% \n 65 \n\nquantile(language_exams_new$exam_1, 0.9)\n\n90% \n 71 \n\n\n\n\nSummary\nThe summary() function is a fast way to get the key summary statistics.\n\nsummary(language_exams_new)\n\n   student_id         age           exam_1          exam_2          exam_3     \n Min.   :10011   Min.   :17.0   Min.   :62.00   Min.   :70.00   Min.   :81.00  \n 1st Qu.:13028   1st Qu.:20.0   1st Qu.:67.00   1st Qu.:74.00   1st Qu.:87.00  \n Median :15988   Median :23.0   Median :68.00   Median :75.00   Median :88.00  \n Mean   :16327   Mean   :24.7   Mean   :68.05   Mean   :75.06   Mean   :87.92  \n 3rd Qu.:19070   3rd Qu.:28.0   3rd Qu.:69.00   3rd Qu.:76.00   3rd Qu.:89.00  \n Max.   :24996   Max.   :38.0   Max.   :74.00   Max.   :81.00   Max.   :97.00  \n\n\nsummary() is actually quite messy, but it is quick and sometimes that’s all we need, just to get a rough idea of the shape of the data.\n\n\nFrequencies\nSometimes it is helpful to observe the frequencies in our sample data. The freq() function is really helpful for this. Note: This function requires the descr package, which we will need to install and then load. Can you remember where we put these two respective commands? One goes in the console, and one at the top of the script.\n\ninstall.packages(\"descr\")\n\nlibrary(descr)\n\n\nlibrary(descr)\n\nWarning: package 'descr' was built under R version 4.2.3\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\ninstall.packages(\"descr\") #in the console because it is temporary\n\nlibrary(descr) #at the top of the script because we need it to show everyone what tools they need\n\n\n\n\nThe following command will create a frequency table. On the left, you will see the score, in the middle the frequency with which the score occurs, and to the right this is express in percentage points. This tells you, for example, that the most frequent score (the mode) is 68, the least frequent score is 62.\n\nfreq(language_exams_new$exam_1, plot = FALSE)\n\nlanguage_exams_new$exam_1 \n      Frequency Percent\n62            2     0.2\n63           13     1.3\n64           27     2.7\n65           68     6.8\n66          109    10.9\n67          168    16.8\n68          196    19.6\n69          175    17.5\n70          139    13.9\n71           56     5.6\n72           32     3.2\n73           12     1.2\n74            3     0.3\nTotal      1000   100.0\n\n\n\n\nFrequencies with a twist\nIf you omit the argument plot = FALSE in the freq() function (which just default the argument plot = TRUE, check it out in the help pane and searching for the function name), R will produce both a table and a visual display of your data. Compare the table and the histogram (the graph). Which one is more informative? What are the relative advantages of either?\n\nfreq(language_exams_new$exam_1)\n\n\n\n\nlanguage_exams_new$exam_1 \n      Frequency Percent\n62            2     0.2\n63           13     1.3\n64           27     2.7\n65           68     6.8\n66          109    10.9\n67          168    16.8\n68          196    19.6\n69          175    17.5\n70          139    13.9\n71           56     5.6\n72           32     3.2\n73           12     1.2\n74            3     0.3\nTotal      1000   100.0\n\n\nFor now, we can keep the colour grey. Next week we can play with colours properly, but for now - just know you can make some great plots, but for basic exploration, let’s keep them fast and basic because we just need to know the basic shape of the data"
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk3.html#step-4-our-first-graphic-explorations",
    "href": "Worksheets/Answers/Worksheet_wk3.html#step-4-our-first-graphic-explorations",
    "title": "3. Exploratory Data Analysis - Answers",
    "section": "Step 4: Our first graphic explorations",
    "text": "Step 4: Our first graphic explorations\nTo conclude, let’s try out a few basic graphics. Next week, we will go into much more detail, but here’s a preview of data visualization. The graphics below are available in the R base package. By and large, this will be your only exposure to base plots, because they aren’t as useful or smart as tidy plots, but for fast visualisation, they’re “just ok”. Don’t get too attached to them, next week we get some real exposure to graphics.\n\nBoxplot\nBoxplots are helpful to inspect the data (Tukey, 1977), as discussed in our lecture today. The following command creates a boxplot, based on exam_1 from the data set language_exams_new.\n\nboxplot(language_exams_new$exam_1)\n\n\n\n\nWhat if we want to compare performance on all three exams (exam_1, exam_2, exam_3) next to each other?\nOne way of doing this (there are other ways) is to first create three new objects exam_1, exam_2, exam_3, and then used the boxplot() function on the three.\nHave a go.\n\nexam_1 &lt;- c(language_exams_new$exam_1) \n\nexam_2 &lt;- c(language_exams_new$exam_2) \n\nexam_3 &lt;- c(language_exams_new$exam_3) \n\nboxplot(exam_1, exam_2, exam_3)\n\n\n\n\nCould you run the above using just one line? What is easier to read? What do you prefer?\n\n\nHistograms\nLast but not least, histograms are helpful ways to visually inspect your data. To create a histogram, simply use the hist() function, as in the following examples.\n\nhist(language_exams_new$exam_3)"
  },
  {
    "objectID": "Worksheets/Answers/Worksheet_wk3.html#take-home-task",
    "href": "Worksheets/Answers/Worksheet_wk3.html#take-home-task",
    "title": "3. Exploratory Data Analysis - Answers",
    "section": "Take home task",
    "text": "Take home task\nNone this week. You’ve worked hard and learnt a lot of complicated aspects of R - your task can be relish in your new found knowledge and be impressed with what you can now do in R.\nNext week we take on graphs, plots, and visualisations!"
  },
  {
    "objectID": "Worksheets/Worksheet_wk10.html",
    "href": "Worksheets/Worksheet_wk10.html",
    "title": "10. Multiple linear regression",
    "section": "",
    "text": "Three important things to remember:"
  },
  {
    "objectID": "Worksheets/Worksheet_wk10.html#task-0-setting-up-our-environment",
    "href": "Worksheets/Worksheet_wk10.html#task-0-setting-up-our-environment",
    "title": "10. Multiple linear regression",
    "section": "Task 0: Setting up our environment",
    "text": "Task 0: Setting up our environment\n\nCreate a new script and call it Week 10.\nLoad in the tidyverse and the performance library at the top of the script"
  },
  {
    "objectID": "Worksheets/Worksheet_wk10.html#the-important-information",
    "href": "Worksheets/Worksheet_wk10.html#the-important-information",
    "title": "10. Multiple linear regression",
    "section": "The important information",
    "text": "The important information\nThis week we are going to explore data from Przybylski and Weinstein (2017)1. They used English adolescents to assess the effect of smartphones on young adult wellbeing.\nThe abstract from this paper is as follows\n“Although the time adolescents spend with digital technologies has sparked widespread concerns that their use might be negatively associated with mental well-being, these potential deleterious influences have not been rigorously studied. Using a preregistered plan for analyzing data collected from a representative sample of English adolescents (n = 120,115), we obtained evidence that the links between digital-screen time and mental well-being are described by quadratic functions. Further, our results showed that these links vary as a function of when digital technologies are used (i.e., weekday vs. weekend), suggesting that a full understanding of the impact of these recreational activities will require examining their functionality among other daily pursuits. Overall, the evidence indicated that moderate use of digital technology is not intrinsically harmful and may be advantageous in a connected world. The findings inform recommendations for limiting adolescents’ technology use and provide a template for conducting rigorous investigations into the relations between digital technology and children’s and adolescents’ health.”\nWhat is really impressive is the size of the sample used, as well as the fact that theymade their data openly available. This is great because we can have a look! In this exercise, we will look at whether the relationship between screen time and well-being is modulated by participants’ (self-reported) gender.\nThe dependent measure used in the study was the Warwick-Edinburgh Mental Well-Being Scale (WEMWBS). This is a 14-item scale with 5 response categories, summed together to form a single score ranging from 14-70 - So this is what we are interested in modelling. The reported wellbeing of the adolescents measured on this 14-item scale.\nWe will be usin preprocessed data for the ease of carrying out a multiple regression, but you can find the raw data in the reference above.\nThe study used multiple measures of screen time, but we will only be looking at smartphone use in these examples, feel free to continue your exploration with other measures too.\nPrzybylski and Weinstein looked at multiple measures of screen time, but we will be focusing on smartphone use. They found that decrements in well-being started to appear when respondents reported more than one hour of weekly smartphone use. Our question: Does the negative association between hours of use and well-being (beyond the one-hour point) differ for boys and girls?\nSo we want to explore how screen time and gender influences self-reported well-being. We have three variables:\n\nour DV: well-being which is a continuouscategorical\nour first IV: screen time, which is a continuouscategorical\nour second IV: gender, which is a continuouscategorical\n\nBased on our previous experiences, this question looks like it should be addressed with a simple regression (to model the effect of screen time on well-being) AND a t-test to compare wellbeing scores between the (boy and girl) gender groups. But wait, multiple regression allows us to do this in combination, offering us a stronger, more robust analysis."
  },
  {
    "objectID": "Worksheets/Worksheet_wk10.html#task-1-reading-in-our-data",
    "href": "Worksheets/Worksheet_wk10.html#task-1-reading-in-our-data",
    "title": "10. Multiple linear regression",
    "section": "Task 1: Reading in our data",
    "text": "Task 1: Reading in our data\nDownload the three files wk10_wellbeing.csv, wk10_participant_info.csv, and wk10_screen_time.csv and read these into your R environment. Call them wellbeing, pinfo, and screen respectively.\nThe wellbeing tibble has information from the WEMWBS questionnaire; screen has information about screen time use on weekends (variables ending with we) and weekdays (variables ending with wk) for four types of activities: using a computer (variables starting with Comph; Q10 on the survey), playing video games (variables starting with Comp; Q9 on the survey), using a smartphone (variables starting with Smart; Q11 on the survey) and watching TV (variables starting with Watch; Q8 on the survey)."
  },
  {
    "objectID": "Worksheets/Worksheet_wk10.html#task-2-data-wrangling",
    "href": "Worksheets/Worksheet_wk10.html#task-2-data-wrangling",
    "title": "10. Multiple linear regression",
    "section": "Task 2: Data Wrangling",
    "text": "Task 2: Data Wrangling\nBefore we can do any analysis, we need to tidy up the data. If we look at wellbeing then we can see that it has participant IDs in Serial and then each column is the response to each question on the WEMWBS scale. We want a single score which is the sum of all scores. This can be done in a number of ways, either using mutate() and select(), or with pivot_longer() and summarise()\nCreate a new object called wemwbs that has just two columns, serial and wellbeing (the sum score) using one of the above methods\n\n\n\n\n\n\nmethod 1\n\n\n\n\n\n\nwemwbs &lt;- wellbeing |&gt; mutate(wellbeing = WBOptimf + WBUseful + WBRelax + WBIntp + WBEnergy + WBDealpr + WBThkclr + WBGoodme + WBClsep + WBConfid + WBMkmind + WBLoved + WBIntthg + WBCheer) |&gt; \n  select(Serial, wellbeing)\n\n\n\n\n\n\n\n\n\n\nmethod 2\n\n\n\n\n\n\nwemwbs &lt;- wellbeing |&gt; pivot_longer(cols = c(WBOptimf:WBCheer)) |&gt;  #this : allows me to say \"use all columns between and including these columns\n  group_by(Serial) |&gt; \n  summarise(wellbeing = sum(value))\n\n\n\n\nBecause these steps were taken by the original authors, we can check we got the right numbers by checking against the mean and SD reported originally, 47.52 (SD = 9.55), as well as thinking about the above “The dependent measure used in the study was the Warwick-Edinburgh Mental Well-Being Scale (WEMWBS). This is a 14-item scale with 5 response categories, summed together to form a single score ranging from 14-70” - what range should the data fall within? What if it is above or below this?\n\nCombining dataframes\nNext we need combine our wellbeing scores with the screen time data. If you look at the screen data, then you’ll see it has data for more than just smartphones, so we need to select the relevant columns before pivoting and joining the data with the wellbeing scores\n\ndata &lt;- screen |&gt; \n  select(Serial, starts_with(\"Smart\")) |&gt;  #what do you think that this does? Use the help or Google to find out what starts_with does for us\n  pivot_longer(cols = Smart_we:Smart_wk, values_to = \"hours\", names_to = \"day\") |&gt; \n  inner_join(wemwbs, by = join_by(Serial)) |&gt; \n  mutate(day = recode(day,\n              \"Smart_wk\" = \"Weekday\",\n              \"Smart_we\" = \"Weekend\"))\n\nNow let’s visualise the effect of screen time on wellbeing. The graph makes it evident that smartphone use of more than 1 hour per day is associated with increasingly negative well-being.\n\ndata |&gt;\n  group_by(day, hours) %&gt;%\n  summarise(mean_wellbeing = mean(wellbeing)) |&gt;  # get a mean for day type and for hours spent\n    ggplot(aes(hours, mean_wellbeing, linetype = day)) + \n  geom_line() +\n  geom_point()\n\n`summarise()` has grouped output by 'day'. You can override using the `.groups`\nargument.\n\n\n\n\n\nWe need to tidy up the data slightly further to only look at time spent over 1 hour, as well as get a mean score for each participant for their hourly usage over both weekdaya dn weekend.\nTo do this you will need to create a new object called totalhours which is created by having to first filter the data for hours over 1, and then group_by participant, before summarising to get a mean hourly average. You should aim to have the data look like the below (if you run head(totalhours))\n\n\n# A tibble: 6 × 2\n   Serial totalhours\n    &lt;dbl&gt;      &lt;dbl&gt;\n1 1000003        2  \n2 1000004        2.5\n3 1000005        3.5\n4 1000006        2  \n5 1000008        2  \n6 1000009        2  \n\n\nThen we need to combine total_hours with the other two datasets using inner_join - we will need to do this twice because we can only combine with one dataframe at a time.\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\ndata &lt;- total_hours |&gt; \n  inner_join(wemwbs, by = join_by(Serial)) |&gt; \n  inner_join(pinfo, by = join_by(Serial))"
  },
  {
    "objectID": "Worksheets/Worksheet_wk10.html#task-3-mean-centring-variables",
    "href": "Worksheets/Worksheet_wk10.html#task-3-mean-centring-variables",
    "title": "10. Multiple linear regression",
    "section": "Task 3: mean centring variables",
    "text": "Task 3: mean centring variables\nWhen you have continuous variables in a regression, it is often sensible to transform them by mean centering. You mean center a predictor X simply by subtracting the mean (X_centered = X - mean(X)). This has two useful consequences:\n\nthe model intercept reflects the prediction for Y at the mean value of the predictor variable, rather than at the zero value of the unscaled variable;\nif there are interactions in the model, any lower-order effects can be given the same interpretation as they receive in ANOVA (main effects, rather than simple effects).\n\nFor categorical predictors with two levels, these become coded as -.5 and .5 (because the mean of these two values is 0).\n\nUse mutate to add a new variable to data: totalhours_c, calculated as a mean-centered version of the tothours predictor\nUse mutate to change gender so that it reads “male” and “female” instead of numbers\nFinally, recode the gender variable as a factor, so that R knows not to treat them as a real numbers. (we can use the code skeleton mutate(x = as.factor(x)) to force a continuous variable into a factor)\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\ndata &lt;- data |&gt; \n  mutate(totalhours_c = totalhours - mean(totalhours),\n         gender = if_else(gender == 1, \"male\", \"female\")) |&gt; \n  mutate(gender = as.factor(gender))"
  },
  {
    "objectID": "Worksheets/Worksheet_wk10.html#task-4-visualising-the-relationship",
    "href": "Worksheets/Worksheet_wk10.html#task-4-visualising-the-relationship",
    "title": "10. Multiple linear regression",
    "section": "Task 4: Visualising the relationship",
    "text": "Task 4: Visualising the relationship\nNow we have the data in the format that we want, let’s visualise it\n\ndata |&gt; \n  group_by(totalhours, gender) |&gt; \n  summarise(mean_wellbeing = mean(wellbeing)) |&gt; \n  ggplot(aes(totalhours, mean_wellbeing, colour = gender)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`summarise()` has grouped output by 'totalhours'. You can override using the\n`.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nHow can we interpret the above plot? How does it translate to the real world?\n\n\n\n\n\n\nmy interpretation\n\n\n\n\n\nFor smartphone use over one hour per day, girls show an overall lower well-being score compared to boys. The effect of usage is more pronounced for girls, as shown by the more negative relationship in the slope. The boys’ slope is flatter, suggesting that wellbeing is negatively impacted more in girls as hourly use increases."
  },
  {
    "objectID": "Worksheets/Worksheet_wk10.html#task-5-running-a-multiple-regression",
    "href": "Worksheets/Worksheet_wk10.html#task-5-running-a-multiple-regression",
    "title": "10. Multiple linear regression",
    "section": "Task 5: Running a multiple regression",
    "text": "Task 5: Running a multiple regression\nFinally, let’s run the model and see whether we find anything significant. Using the data object, create a model to predict wellbeing with two predictors, the centred totalhours and gender.\n\nlm(wellbeing ~ totalhours_c * gender, data) |&gt; \n  summary()\n\n\nCall:\nlm(formula = wellbeing ~ totalhours_c * gender, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.815  -5.695   0.392   6.213  27.305 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             44.86660    0.04474 1002.82   &lt;2e-16 ***\ntotalhours_c            -0.81836    0.02473  -33.09   &lt;2e-16 ***\ngendermale               5.13915    0.07104   72.34   &lt;2e-16 ***\ntotalhours_c:gendermale  0.47327    0.03926   12.05   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.136 on 71145 degrees of freedom\nMultiple R-squared:  0.0938,    Adjusted R-squared:  0.09376 \nF-statistic:  2455 on 3 and 71145 DF,  p-value: &lt; 2.2e-16\n\n\nGreat. Let’s extract the meaningful information from the output we have here, as well as understnad what it all means\n\nIs the overall model significant? yesno\nTo 2dp, what is the \\(R^2\\) value? \nFor a unit increase in totalhours_c, how much does wellbeing change (to 2dp)? Wellbeing increasesdecreasesstays the same by .\n\nBefore the next question, we need to know how to interpret categorical variables. Gender is a categorical variable which means that a “unit increase” is not continuous and instead means the difference between one category and the next. When we run as.factor() above, it automatically chooses a reference level (the first level) because if we look in the model output it shows us “gendermale” which is confusing because what does this mean? gender is the variable name (obviously) and the reference level is whatever is first alphabetically (female is before male, so female is our reference), the output shows us the number between the reference and the other factor level (which is male) - if we revisit the plot with these numbers in mind, we should expect to see that the difference between the two lines is roughly this value. So,\n\nWhat is the difference seen in wellbeing scores for boys compared to girls (to 2dp)?"
  },
  {
    "objectID": "Worksheets/Worksheet_wk10.html#task-6-comparing-models",
    "href": "Worksheets/Worksheet_wk10.html#task-6-comparing-models",
    "title": "10. Multiple linear regression",
    "section": "Task 6: Comparing Models",
    "text": "Task 6: Comparing Models\nSimply put, we can check whether the model with an interaction performs better (explains more) with the interaction by using AIC() for comaprisons, as well as using anova(model1, model2) to see whether they are significantly different in their composition.\nRemember from the lectures that the lower AIC value is usually the better model, and for the anova test, we want to see whether the more complex model is significantly different to the less complex. If it is not significantly different (p &gt; .05) then we might choose the simpler model, but if it is significantly different, then we can keep the more complex model.\n\nlm(wellbeing ~ totalhours_c + gender, data = data) |&gt; AIC()\n\n[1] 516861.7\n\nlm(wellbeing ~ totalhours_c * gender, data = data) |&gt; AIC()\n\n[1] 516718.6\n\nanova(lm(wellbeing ~ totalhours_c + gender, data = data), \n      lm(wellbeing ~ totalhours_c * gender, data = data), test = \"Chisq\")\n\nAnalysis of Variance Table\n\nModel 1: wellbeing ~ totalhours_c + gender\nModel 2: wellbeing ~ totalhours_c * gender\n  Res.Df     RSS Df Sum of Sq  Pr(&gt;Chi)    \n1  71146 5950819                           \n2  71145 5938690  1     12129 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBoth tests point towards keeping the interaction. It is worth noting that the tests above are not the only possible ways of checking model suitability, there are other criterion methods, and the best thing to do is to use a well-validated and trusted method for validating models. Provided you can cite and justify your decisions, you shouldn’t face any major issues.\nFor now, we can be satisfied that the model with the interaction is good - and we can continue."
  },
  {
    "objectID": "Worksheets/Worksheet_wk10.html#task-7-assumptions-checking",
    "href": "Worksheets/Worksheet_wk10.html#task-7-assumptions-checking",
    "title": "10. Multiple linear regression",
    "section": "Task 7: Assumptions Checking",
    "text": "Task 7: Assumptions Checking\nThe assumptions for multiple regression are the same as simple regression but there is one additional assumption, that of multicollinearity, the idea that predictor variables should not be too highly correlated.\n\nThe outcome/DV is a interval/ratio level data\nThe predictor variable is interval/ratio or categorical (with two levels)\nAll values of the outcome variable are independent (i.e., each score should come from a different participant)\nThe predictors have non-zero variance\nThe relationship between outcome and predictor is linear\nThe residuals should be normally distributed\nThere should be homoscedasticity (homogeneity of variance, but for the residuals)\nMulticollinearity: predictor variables should not be too highly correlated\n\nFrom the work we’ve done so far we know that assumptions 1 - 4 are met and we can use the functions from the performance package again to check the rest, like we did with the simple linear regression chapter.\nOne difference from when we used check_model() previously is that rather than just letting it run all the tests it wants, we’re going to specify which tests, to stop it throwing an error. A word of warning - these assumptions tests will take longer than usual to run, because it’s such a big dataset. The first line of code will run the assumption tests and save it to an object, calling the object name will then display the plots.\n\nassumptions &lt;- check_model(lm(wellbeing ~ totalhours_c * gender, data), \n                           check = c(\"vif\", \"qq\", \"normality\", \"linearity\", \"homogeneity\"))\n\nVariable `Component` is not in your data frame :/\n\nassumptions\n\n\n\n\nFor assumption 5, linearity, we already know from looking at the scatterplot that the relationship is linear, but the residual plot also confirms this.\nFor assumption 6, normality of residuals, again the residuals look good in both plots and this provides an excellent example of why it’s often better to visualise than rely on statistics because if we use check_normality() which calls the Shapiro-Wilk test, it tells us that the residuals are not normal, despite the fact that the plots look almost perfect. And that’s because with large sample sizes, any deviation from perfect normality can be flagged as non-normal.\nFor assumption 7, we can plot the assumption using the below:\n\nggplot(assumptions$HOMOGENEITY, aes(x, y)) +\n    geom_point() +\n    stat_smooth(\n      method = \"loess\",\n      se = FALSE,\n      formula = y ~ x,\n    ) +\n    labs(\n      title = \"Homogeneity of Variance\",\n      subtitle = \"Reference line should be flat and horizontal\",\n      y = expression(sqrt(\"|Std. residuals|\")),\n      x = \"Fitted values\"\n    ) \n\n\n\n\nAgain like normality, the plot isn’t perfect but it is pretty good and another example of why visualisation is better than running statistical tests as we see the same significant result if we run:\n\ncheck_homogeneity(lm(wellbeing ~ totalhours_c * gender, data))\n\nWarning: Variances differ between groups (Bartlett Test, p = 0.000).\n\n\nFor assumption 8, linearity, again the plot looks fine, and we could also have used the grouped scatterplots above to look at this.\nFinally, for assumption 9, multicollinearity, the plot also indicates no issues but we can also test this statistically using check_collinearity().\nEssentially, this function estimates how much the variance of a coefficient is “inflated” because of linear dependence with other predictors, i.e., that a predictor isn’t actually adding any unique variance to the model, it’s just really strongly related to other predictors. You can read more about this here. Thankfully, VIF is not affected by large samples like the other tests.\nThere are various rules of thumb, but most converge on a VIF of above 2 - 2.5 for any one predictor being problematic."
  },
  {
    "objectID": "Worksheets/Worksheet_wk10.html#task-8-effect-size.",
    "href": "Worksheets/Worksheet_wk10.html#task-8-effect-size.",
    "title": "10. Multiple linear regression",
    "section": "Task 8: Effect size.",
    "text": "Task 8: Effect size.\nWe report effect sizes the same way that we would for a simple linear regression: \\(R^2/(1-R^2)\\)), which we have to do by hand.\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\n0.09377 / (1-0.09377)\n\n[1] 0.1034726"
  },
  {
    "objectID": "Worksheets/Worksheet_wk10.html#task-9-the-interpretation-and-writeup",
    "href": "Worksheets/Worksheet_wk10.html#task-9-the-interpretation-and-writeup",
    "title": "10. Multiple linear regression",
    "section": "Task 9: The interpretation and writeup",
    "text": "Task 9: The interpretation and writeup\nFinally, we need to take all our tests and produce a writeup of our findings. The format is similar to last week’s, so look back for help:\nTo test whether wellbeing was affected by smartphone use and tv useagegender, a multiple linear regression was used with an interaction between smartphone use and gender. The results of the regression indicated that the model significantly predicted wellbeing scores, F(, ) = , p &gt;=&lt; , \\(R_{adjusted}^2\\) = , \\(f^2\\) = . Total screen time was a significant negative predictor of well-being scores (\\(\\beta\\) = , p &lt; .001, as was gender (\\(\\beta\\) = , p &lt; .001, with girls having lower well-being scores than boys. Importantly, there was a significant interaction between screen time and gender (\\(\\beta\\) = , p &lt; .001), smartphone use was more negatively associated with well-being for girls than for boys."
  },
  {
    "objectID": "Worksheets/Worksheet_wk10.html#footnotes",
    "href": "Worksheets/Worksheet_wk10.html#footnotes",
    "title": "10. Multiple linear regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPrzybylski, A. & Weinstein, N. (2017). A Large-Scale Test of the Goldilocks Hypothesis. Psychological Science, 28, 204–215.↩︎"
  },
  {
    "objectID": "Worksheets/Worksheet_wk5.html",
    "href": "Worksheets/Worksheet_wk5.html",
    "title": "5. Live Data Wrangling Walkthrough",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.4     ✔ purrr   1.0.2\n✔ tibble  3.2.1     ✔ dplyr   1.1.2\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n# anything else goes here"
  },
  {
    "objectID": "Worksheets/Worksheet_wk5.html#set-up-environment",
    "href": "Worksheets/Worksheet_wk5.html#set-up-environment",
    "title": "5. Live Data Wrangling Walkthrough",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.4     ✔ purrr   1.0.2\n✔ tibble  3.2.1     ✔ dplyr   1.1.2\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n# anything else goes here"
  },
  {
    "objectID": "Worksheets/Worksheet_wk5.html#import-data",
    "href": "Worksheets/Worksheet_wk5.html#import-data",
    "title": "5. Live Data Wrangling Walkthrough",
    "section": "Import Data",
    "text": "Import Data\nRead in key datafiles\n\ndata_raw &lt;- read_csv(\"swiss_crime_2022.csv\")\n\nRows: 19 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): crime, crime_type\ndbl (17): male, female, age18_19, age20_24, age25_29, age30_34, age35_39, ag...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCheck out the data\n\nhead(data_raw)\n\n# A tibble: 6 × 19\n  crime     crime_type  male female age18_19 age20_24 age25_29 age30_34 age35_39\n  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Intentio… Severe vi…    82     20        0       16       17       15       13\n2 Grievous… Severe vi…   204      6        9       53       42       32       28\n3 Female g… Severe vi…     0      0        0        0        0        0        0\n4 Hostage … Severe vi…     0      0        0        0        0        0        0\n5 Rape      Severe vi…    71      0        0        7       13       11       12\n6 Violent … Severe vi…     6      0        0        3        1        0        1\n# ℹ 10 more variables: age40_44 &lt;dbl&gt;, age45_49 &lt;dbl&gt;, age50_59 &lt;dbl&gt;,\n#   age60_69 &lt;dbl&gt;, age70plus &lt;dbl&gt;, nat_swiss &lt;dbl&gt;, nat_foreign &lt;dbl&gt;,\n#   foreign_permit &lt;dbl&gt;, foreign_other &lt;dbl&gt;, foreign_unknown &lt;dbl&gt;\n\ncolnames(data_raw)\n\n [1] \"crime\"           \"crime_type\"      \"male\"            \"female\"         \n [5] \"age18_19\"        \"age20_24\"        \"age25_29\"        \"age30_34\"       \n [9] \"age35_39\"        \"age40_44\"        \"age45_49\"        \"age50_59\"       \n[13] \"age60_69\"        \"age70plus\"       \"nat_swiss\"       \"nat_foreign\"    \n[17] \"foreign_permit\"  \"foreign_other\"   \"foreign_unknown\""
  },
  {
    "objectID": "Worksheets/Worksheet_wk5.html#tidying-data-to-minimum-required",
    "href": "Worksheets/Worksheet_wk5.html#tidying-data-to-minimum-required",
    "title": "5. Live Data Wrangling Walkthrough",
    "section": "Tidying data to minimum required",
    "text": "Tidying data to minimum required\nWhat do we need to keep if we are wanting to visualise the differences in type of crime convicted in the Swiss Adult population according to age?\n\ndata &lt;- data_raw |&gt; \n  select(crime, crime_type, contains(\"age\"))"
  },
  {
    "objectID": "Worksheets/Worksheet_wk5.html#how-do-i-condense-down-all-the-specific-crimes-into-one-value",
    "href": "Worksheets/Worksheet_wk5.html#how-do-i-condense-down-all-the-specific-crimes-into-one-value",
    "title": "5. Live Data Wrangling Walkthrough",
    "section": "How do I condense down all the specific crimes into one value?",
    "text": "How do I condense down all the specific crimes into one value?\n\ndata_tidy &lt;- data |&gt;\n  group_by(crime_type) |&gt; \n  mutate(age18_19 = sum(age18_19),\n         age20_24 = sum(age20_24),\n         age25_29 = sum(age25_29),\n         age30_34 = sum(age30_34),\n         age35_39 = sum(age35_39),\n         age40_44 = sum(age40_44),\n         age45_49 = sum(age45_49),\n         age50_59 = sum(age50_59),\n         age60_69 = sum(age60_69),\n         age70plus = sum(age70plus)\n         ) |&gt; \n  select(-crime) |&gt; \n  distinct()\n\n# factor the group data\ndata_tidy &lt;- data_tidy |&gt; \n  mutate(crime_type = factor(crime_type, levels = c(\"Moderate violence (threat of violence)\", \n                                                    \"Moderate violence (exercise of violence evt. threat of violence)\",\n                                                    \"Severe violence (exercise of violence)\")))"
  },
  {
    "objectID": "Worksheets/Worksheet_wk5.html#pivot-so-its-ready-for-plotting",
    "href": "Worksheets/Worksheet_wk5.html#pivot-so-its-ready-for-plotting",
    "title": "5. Live Data Wrangling Walkthrough",
    "section": "pivot so it’s ready for plotting",
    "text": "pivot so it’s ready for plotting\n\ndata_tidy_long &lt;- data_tidy |&gt; \n  pivot_longer(cols = age18_19:age70plus, \n               names_to = \"age_group\", \n               values_to = \"convictions\")"
  },
  {
    "objectID": "Worksheets/Worksheet_wk5.html#plotting",
    "href": "Worksheets/Worksheet_wk5.html#plotting",
    "title": "5. Live Data Wrangling Walkthrough",
    "section": "Plotting",
    "text": "Plotting\n\ndata_tidy_long |&gt; \n  filter(crime_type == \"Severe violence (exercise of violence)\") |&gt; \n  ggplot(aes(age_group, convictions)) +\n  geom_col()\n\n\n\n#try colour first, and then go to fill\ndata_tidy_long |&gt; \n  ggplot(aes(age_group, convictions, fill = crime_type)) +\n  geom_col(position = \"dodge\")\n\n\n\n\nWhat can we see in the dataset (as well as the x-axis) that indicates this is a misleadingly organised dataset?\nThe age ranges are not consistent - so the above plot is misleading, just like the examples in week 4.\n\ndata_tidy_ages &lt;- data_tidy |&gt; mutate(\n  age20_29 = sum(age20_24, age25_29),\n  age30_39 = sum(age30_34, age35_39),\n  age40_49 = sum(age40_44, age45_49),\n  .before = age50_59\n) |&gt; \n  select(-c(age20_24, age25_29, age30_34, age35_39, age40_44, age45_49))\n\ndata_tidy_ages_long &lt;- data_tidy_ages |&gt; \n  pivot_longer(cols = age18_19:age70plus, \n               names_to = \"age_group\", \n               values_to = \"convictions\") \n\ndata_tidy_ages_long |&gt; \n  ggplot(aes(age_group, convictions, fill = crime_type)) +\n  geom_col(position = \"dodge\")\n\n\n\n\n\ndata_tidy_ages_long_sum &lt;- data_tidy_ages_long |&gt; \n  group_by(age_group) |&gt; \n  summarise(total = sum(convictions))\n\ndata_tidy_ages_long_percent &lt;- data_tidy_ages_long |&gt; \n  left_join(data_tidy_ages_long_sum, by = join_by(age_group)) |&gt; \n  mutate(percentage = convictions/total*100)\n\n##sanity check\n#data_tidy_ages_long_percent |&gt; \n#  filter(age_group == \"age18_19\") |&gt; \n#  pull(percentage) |&gt; sum()\n\n\ndata_tidy_ages_long_percent |&gt; \n  ggplot(aes(age_group, percentage, colour = crime_type)) +\n  geom_col(position = \"dodge\")\n\n\n\n\nWhat inferences can we draw from this? What else could we explore?"
  },
  {
    "objectID": "Worksheets/Worksheet_wk6.html",
    "href": "Worksheets/Worksheet_wk6.html",
    "title": "6. Probability and distributions",
    "section": "",
    "text": "Three important things to remember:"
  },
  {
    "objectID": "Worksheets/Worksheet_wk6.html#task-1-sampling-distributions",
    "href": "Worksheets/Worksheet_wk6.html#task-1-sampling-distributions",
    "title": "6. Probability and distributions",
    "section": "Task 1: Sampling Distributions",
    "text": "Task 1: Sampling Distributions\nThe psychologist Geoff Cumming wrote an excellent textbook on The New Statistics (confidence intervals, effect sizes, meta-analyses), in which he advocates a significant reform of the way we use statistics in the social sciences.\nThe following article summarizes his ideas neatly.\nCumming, G. (2014). The New Statistics: Why and How. Psychological Science, 25(1), 7–29. https://doi.org/10.1177/0956797613504966\nThe following recordings of one of Cumming’s workshop are also worth watching:\nhttps://www.psychologicalscience.org/members/new-statistics\nCumming developed a very helpful software package (ESCI, Exploratory Software for Confidence Intervals) that we will use over the coming weeks to illustrate important ideas in statistics. You can access the software and the companion website here:\nhttps://www.esci.thenewstatistics.com/\nLet’s use the Dance tools of the ESCI website to illustrate a few key ideas\nFirst, go to the website above and click on Dances. Below is a screenshot of the Dances tool.\n\n\n\na screenshot of the ESCI website Dance tool\n\n\nThis tool allows you to draw samples from a population and to manipulate many interesting variables. We can easily visualize the effect of our manipulation.\nFor this task, we will use the default settings, with the following exception.\nIn section 5, make sure that Mean heap and Sampling distribution curve are turned on (clicked).\nNow go to section 2 and press Take sample. This will draw one sample (n = 20, see section 4 to confirm) from our population distribution (\\(\\mu\\)= 50, \\(\\sigma\\) = 20). Your screen will look similar to this (though not identical, as we are drawing randomly).\n\n\n\na screenshot of the ESCI website Dance tool\n\n\nThe empty bubbles underneath the population distribution are individual observations. There should be 20 as we chose sample size n = 20 (see section 4). The green bubble represents the mean of this particular sample (M = 54.74, SD = 24.16, see section 4 last sample).\nLet’s now draw another random 200 samples from the same distribution. This is like doing 200 experiments in sequence, each with the same number of observations (n = 20), but each time with different ones.\nYou can press Take sample in section 3 another 200 times. Or you can click Run stop until the the number of samples in section 3 is close to 200. The slider to the right of Run stop allows you to control the speed at which samples are drawn.\nYour screen should look roughly like this now. The emerging distribution at the bottom is your sampling distribution.\n\n\n\na screenshot of the ESCI website Dance tool"
  },
  {
    "objectID": "Worksheets/Worksheet_wk6.html#task-2-playing-around-with-the-normal-distribution",
    "href": "Worksheets/Worksheet_wk6.html#task-2-playing-around-with-the-normal-distribution",
    "title": "6. Probability and distributions",
    "section": "Task 2: Playing around with the normal distribution",
    "text": "Task 2: Playing around with the normal distribution\nThe following task is adapted from Winter (2019). The purpose is to familiarize you with the normal distribution.\nTo complete this section, we first need to load the tidyverse package.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.4     ✔ purrr   1.0.2\n✔ tibble  3.2.1     ✔ dplyr   1.1.2\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nFirst, we use a random number generation function called runif() to create a set of 50 uniformly distributed numbers. The name stands for “random uniform”, so this specific random number generator will generate datasets with normally distributed data.\nThis command creates a variable x and assigns 50 random uniformly distributed numbers.\n\nx &lt;- runif(50)\n\nYou can now look at the numbers in variable x. Remember that my random numbers will look different to yours.\n\nx\n\n [1] 0.09256004 0.63578704 0.80243763 0.14941430 0.68837848 0.52802393\n [7] 0.81375063 0.37252244 0.40453444 0.01735944 0.84816314 0.83430405\n[13] 0.19550930 0.02666873 0.41393242 0.72581020 0.73453548 0.81317698\n[19] 0.89281177 0.34016034 0.05938471 0.23668245 0.84088203 0.12261608\n[25] 0.02365220 0.64421924 0.51720891 0.47606525 0.89150871 0.07530694\n[31] 0.34122391 0.11144758 0.52622591 0.47446060 0.47656226 0.15075901\n[37] 0.98103597 0.49800762 0.89613028 0.49972487 0.48865003 0.98624614\n[43] 0.31022067 0.92791627 0.86428797 0.88908498 0.40252126 0.05506960\n[49] 0.30473844 0.70401966\n\n\nBy default, the runif() function will generate random values between 0 and 1. We can override the default setting by adding the min and max arguments. The following command will generate uniformly distributed random numbers between 2 and 6.\n\ny &lt;- runif(50, min = 2, max = 6)\n\nLet’s confirm that this is really the case.\n\nhead(y)\n\n[1] 2.980621 3.616929 4.043631 4.366569 4.092333 3.516112\n\n\nIt’s good to visualize the data. The following histogram does the trick. Remember that my plot will look different from yours as the dataset is randomly generated for each of us.\n\nhist(y)\n\n\n\n\nNote that the histogram doesn’t necessarily look like a normal distribution, despite the fact that the data are drawn from a normally-distributed population. (The mean of the above data is 4.1196849, right in the middle, as you would expect from a normal distribution.)\nThe following command assigns 50 random numbers from a normal distribution, produces a histogram and adds a dashed line (lty = 2) with width 2 (lwd = 2) to indicate the mean.\n\nz &lt;- rnorm(50)\nhist(z)\nabline(v = mean(z), lty = 2, lwd = 2)\n\n\n\n\nWe can also create random data with specific means and standard deviations. All we need to do is add the arguments mean and sd to the rnorm() function.\n\na &lt;- rnorm(50, mean = 5, sd = 2)\n\nWe can verify if the mean and standard deviation are really what we specified.\nFor example, the following command creates a histogram with the dashed line representing the mean. This shows that the mean of the data set is really 5, as specified in the argument mean above.\n\nhist(a)\nabline(v = mean(a), lty = 2, lwd = 2)\n\n\n\n\nAnd we can also verify the means and standard deviation by typing the following commands. (My actual values will be different from yours, but for all of us the rounded mean should be 5 and the standard deviation 2.\n\n#|eval: true\nmean(a)\n\n[1] 4.929551\n\nsd(a)\n\n[1] 2.069571\n\n\nThe quantile() function allows us to compute percentiles. If we just run the quantile() function on our data set a without supplying additional arguments, the output will show the minimum (0th percentile) and maximum (100th percentile) as well as the first quantile (Q1, 25% percentile), the second quantile (Q2, 50% percentile, which is the median), and the third quantile (Q3, 75% percentile).\n\nquantile(a)\n\n      0%      25%      50%      75%     100% \n1.231914 3.267396 4.906419 6.324437 9.313363 \n\n\nLet’s use the quantile() function to check the 68-95-99.7 rule.\nFirst, we check the values that span the 68% interval. The interval corresponds to the 16th and 84th percentiles.\n\nquantile(a, 0.16)\n\n     16% \n2.613814 \n\nquantile(a, 0.84)\n\n    84% \n7.14376 \n\n\nIf the 68% rule works, the resulting numbers should be close to the interval by M – SD and M + SD. We can confirm this by writing the following commands. And yes, it looks like the numbers are fairly close to the 16th and 84th percentile.\n\nmean(a) - sd(a)\n\n[1] 2.85998\n\nmean(a) + sd(a)\n\n[1] 6.999122\n\n\nWe now do the same for the 95% interval, which corresponds to the interval between the 2.5th percentile and the 97.5th percentile.\n\nquantile(a, 0.025)\n\n    2.5% \n1.268643 \n\nquantile(a, 0.975)\n\n   97.5% \n9.021888 \n\n\nThe 2.5th and 97.5th percentile should correspond roughly to +/- 2 SD around the mean.\n\nmean(a) - 2 * sd(a)\n\n[1] 0.7904094\n\nmean(a) + 2 * sd(a)\n\n[1] 9.068694\n\n\nAs you can see when you compare the two previous calculations, the 95% rule is a little off. This is because we are dealing with a small data set (n = 50).\nLast but not least. Let’s develop a better intuition for how normally-distributed data might look like. As you play around with the following commands, observe how they affect the distribution.\nFirst, let’s run the following five commands. Each draws random data from a normal distribution, and each time, we have 20 observations. Observe and compare the shapes of the five histograms. What do you notice?\n\nhist(rnorm(n = 20)) # Observe the shape of the histogram\n\n\n\nhist(rnorm(n = 20)) # Observe the shape of the histogram\n\n\n\nhist(rnorm(n = 20)) # Observe the shape of the histogram\n\n\n\nhist(rnorm(n = 20)) # Observe the shape of the histogram\n\n\n\nhist(rnorm(n = 20)) # Observe the shape of the histogram\n\n\n\n\nYou might have noticed that often the distribution might not look normal at all, despite the fact that an underlying normal distribution was used to generate the data.\nLet’s now observe the effect of changing the sample size n. Run each of the following commands and observe the shape of the distribution. Remember: They are all drawn from the normal distribution. What do you notice?\n\nhist(rnorm(n = 5))\n\n\n\nhist(rnorm(n = 10))\n\n\n\nhist(rnorm(n = 20))\n\n\n\nhist(rnorm(n = 30))\n\n\n\nhist(rnorm(n = 40))\n\n\n\nhist(rnorm(n = 50))\n\n\n\nhist(rnorm(n = 100))\n\n\n\nhist(rnorm(n = 500))\n\n\n\nhist(rnorm(n = 1000))"
  },
  {
    "objectID": "Worksheets/Worksheet_wk6.html#task-3-the-law-of-large-numbers-in-action",
    "href": "Worksheets/Worksheet_wk6.html#task-3-the-law-of-large-numbers-in-action",
    "title": "6. Probability and distributions",
    "section": "Task 3: The Law of Large Numbers in Action",
    "text": "Task 3: The Law of Large Numbers in Action\nThe following task is adapted from Brown (2021). It’s a nice way to visualize the Law of Large Numbers in action. (Some chunks of code are a bit more advanced, don’t worry if you don’t understand all of it yet. Most of the code should be clear.)\nFirst, we need to install the reshape package. (We already installed tidyverse above.)\n\n#|eval: false\n\n#install.packages('reshape')\n\n\nlibrary(reshape)\n\n\nAttaching package: 'reshape'\n\n\nThe following object is masked from 'package:dplyr':\n\n    rename\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, smiths\n\n\nWe use the set.seed() function to create “reproducible randomness”. Without getting into the technicality of random numbers in R, the short of it is: randomness is fake but is sufficient for what we want it to do in almost every situation. It approximates randomness, but true randomness is not easily computed. Interestingly, to avoid this issue of computer-generated randomness, one UK cybersecurity company uses lava lamps to generate true randomness…\nIf we run the following command, and then do not deviate in the following steps, we will all end up with the same “random” output.\n\nset.seed(3376)\n\nWe can check this by running this command below. Your numbers should be the same as mine. Confidently, I reckon you will get 99, 74, and 19. If not, set the seed above again, and then the code below.\n\nsample(100, 3)\n\n[1] 99 74 19\n\n\nJust out of interest, the chances of you getting the same numbers as me completely randomly (if we didn’t set the seed) would be 1/100 * 1/99 * 1/98 or 0.000001%.\nThe following line of code creates three numbers in our environment with a population mean \\(\\mu\\) = 100 and population standard deviation \\(\\sigma\\) = 10. We will use these numbers to draw random samples next\n\nmu=100; sigma=10; n=10\n\nNow we tell R that we want to create a variable where we can put the mean from each sample we draw. Each variable (xbar1, xbar2, xbar3) will hold five observations.\nWe use rep() function to do create placeholders. The rep() function simply replicates the values in the first argument position. In the three examples below, the digit is 0. So, writing xbar1 = rep(0,5) means the variable xbar1 should consist of five 0 0 0 0 0, xbar2 should contain 10 zeros, and xbar 3 should contain 100.\nFor each variable (xbar1, xbar2, xbar 3), we then ask R to conduct an operation five times by means of the for() function. This is a loop that says: For each element in xbar 1, xbar2, and xbar3, replace it with something else. And the something else if specified in braces {}, namely the means of, respectively, 5, 10 and 100 randomly drawn samples from our population.\n\nxbar1=rep(0,5)\nfor (i in 1:5) {xbar1[i]= mean(rnorm(n, mean=mu, sd=sigma))}\n\nxbar2=rep(0,10)\nfor (i in 1:10) {xbar2[i]=mean(rnorm(n, mean=mu, sd=sigma))}\n\nxbar3=rep(0,100)\nfor (i in 1:100) {xbar3[i]=mean(rnorm(n, mean=mu, sd=sigma))}\n\nThe above commands have calculated the means for 5, 10, 100 samples and placed the results in three variables xbar1, xbar2, xbar 3.\nThe following commands reorder the structure of the data so that we can place density plots over each. This is done by means of the list() and melt() functions.\n\nx &lt;- list(v1=xbar1, v2=xbar2, v3=xbar3)\ndata &lt;- melt(x)\n\nWe are now ready to produce a density plot with three sampling distributions. One representing the mean with 5 random samples from the population, one with 10 draws, and one with 50 draws. If we were using the ESCI Dances tools, this would correspond to pressing the Take sample button 5, 10 and 50 times, respectively.\nIf you run the following commands, you will get the density plot at the bottom (Figure 8-2 from Brown, 2021).\n\nxb1 &lt;- mean(xbar1)\nxb2 &lt;- mean(xbar2)\nxb3 &lt;- mean(xbar3)\n\nxb1\n\n[1] 99.72584\n\nxb2\n\n[1] 99.39641\n\nxb3\n\n[1] 99.91599\n\n\n\nggplot(data, aes(x = value, fill = L1)) +\n  geom_density(alpha = .50) +\n  ggtitle(\"Figure 8-2: Increased Draws, Increased Accuracy\") +\n  xlab(\"Value\") +\n  ylab(\"Density\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 8, face = \"bold\")) +\n  theme(axis.title = element_text(size = 8, face = \"bold\")) +\n  geom_vline(xintercept = xb1, col = \"#bf0000\") +\n  geom_vline(xintercept = xb2, col = \"#008b00\") +\n  geom_vline(xintercept = xb3, col = \"#0000ff\") +\n  annotate(\n    \"text\",\n    x = 95,\n    y = .15,\n    label =\n      \"Mean with 5 draws = 98.895\",\n    col = \"#bf0000\"\n  ) +\n  annotate(\n    \"text\",\n    x = 95,\n    y = .175,\n    label =\n      \"Mean with 10 draws = 100.481\",\n    col = \"#008b00\"\n  ) +\n  annotate(\n    \"text\",\n    x = 95,\n    y = .20,\n    label =\n      \"Mean with 100 draws = 100.071\",\n    col = \"#0000ff\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nInspect the density plots. Remember the true population mean is  = 100. How does the number of randomly-drawn samples (5, 10, 100 samples drawn from the population) affect the three sampling distributions and the mean of each?"
  },
  {
    "objectID": "Worksheets/Worksheet_wk6.html#task-4-population-distributions-and-sampling-distributions",
    "href": "Worksheets/Worksheet_wk6.html#task-4-population-distributions-and-sampling-distributions",
    "title": "6. Probability and distributions",
    "section": "Task 4: Population distributions and sampling distributions",
    "text": "Task 4: Population distributions and sampling distributions\nLet’s return to Cumming’s ESCI website. In this task, we want to explore what happens when we change the distribution of the population from which draw samples.\nIn section 1, you can change the shape of the distribution: normal, rectangular, skew.\n ### Sampling distribution of a normally-distributed population\nFirst, let’s use the normal distribution (the default).\nRemember to select Mean heap and Sampling distribution curve in section 5. This will be important so that we can visualize the shape of the sampling distribution.\nThen, go to section 2 and click Run stop until you have drawn 200 samples from the normal population distribution. Again, you can accelerate by using the slider.\nNote down what shape the sampling distribution has.\n\nSampling distribution of a uniformly-distributed population\nLet’s now see what happens if we change the population distribution from normal to rectangular. Now, the population distribution has a rectangular distribution.\n\n\n\na screenshot of the ESCI website Dance tool\n\n\nAgain, check that Mean heap and Sampling distribution curve are selected.\nThen, go to section 2 and click Run stop until you have drawn 200 samples from the rectangular population distribution.\nNote down the shape of the new sampling distribution.\n\n\nSampling distribution of populations with skewed distributions\nThis time, go to section 1 and select skew, value –1. This produces a negatively-skewed population distribution.\n\n\n\na screenshot of the ESCI website Dance tool\n\n\nAgain, draw 200 samples. What does the sampling distribution look like when you draw samples from the a negatively-skewed population distribution?\nFinally, go to section 1 and select skew, value +0.5. This produces a positively-skewed population distribution.\n\n\n\na screenshot of the ESCI website Dance tool\n\n\nDraw 200 samples. What does the sampling distribution look like when you draw samples from the a positively-skewed population distribution?"
  },
  {
    "objectID": "Worksheets/Worksheet_wk6.html#task-5-the-central-limit-theorem-in-action",
    "href": "Worksheets/Worksheet_wk6.html#task-5-the-central-limit-theorem-in-action",
    "title": "6. Probability and distributions",
    "section": "Task 5: The Central Limit Theorem in Action",
    "text": "Task 5: The Central Limit Theorem in Action\nFinally, let’s see the Central Limit Theorem in action. The code and figure are adapted from Brown (2021).\nSet the seed to 3376 as we did last time so that we all get the same answers.\n\n\n\n\n\n\nNeed a reminder?\n\n\n\n\n\n\nset.seed(3376)\n\n\n\n\nThe population mean \\(\\mu\\) (mu) is set to 100, the population standard deviation is \\(\\sigma\\) (sigma) = 10. What changes is the number of observations in each sample. The variables xbar1, xbar2, and xbar3 will have n = 5, n = 10, and n = 100, respectively. We ran this code above already, but we do it again to ensure everything is consistent.\n\nmu=100; sigma=10; n=5\nxbar1=rep(0,10)\nfor (i in 1:10) {xbar1[i]=mean(rnorm(n, mean=mu, sd=sigma))}\n\nmu=100; sigma=10; n=10\nxbar2=rep(0,10)\nfor (i in 1:10) {xbar2[i]=mean(rnorm(n, mean=mu, sd=sigma))}\n\nmu=100; sigma=10; n=100\nxbar3=rep(0,10)\nfor (i in 1:10) {xbar3[i]=mean(rnorm(n, mean=mu, sd=sigma))}\n\nWe are now ready to reorder the data to produce the density plot.\n\nx &lt;- list(v1=xbar1,v2=xbar2,v3=xbar3)\ndata &lt;- melt(x)\n\nggplot(data, aes(x = value, fill = L1)) +\n  geom_density(alpha = .70) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 8, face = \"bold\")) +\n  theme(axis.title = element_text(size = 8, face = \"bold\")) +\n  ggtitle(\"Brown (2021, Figure 8-3): Increasing Observations Increases Normality\") +\n  annotate(\n    \"text\",\n    x = 95,\n    y = .15,\n    label =\n      \"5 Observations per Draw\",\n    col = \"#bf0000\"\n  ) +\n  annotate(\n    \"text\",\n    x = 95,\n    y = .175,\n    label =\n      \"10 Observations per Draw\",\n    col = \"#008b00\"\n  ) +\n  annotate(\n    \"text\",\n    x = 95,\n    y = .20,\n    label =\n      \"100 Observations per Draw\",\n    col = \"#0000ff\"\n  ) +\n  theme(legend.position = \"none\") +\n  xlab(\"Mean of Sample Means\") +\n  ylab(\"Density\")\n\n\n\n\nInspect the density plots with the three sampling distributions. Remember that the population mean and standard deviation is the same for each sampling distribution. What changes is the number of observations in each sample, n = 5, n = 10, and n = 100, respectively.\nHow does changing the number of observations affect the three sampling distributions? Observe, for example, the means (you calculated them) and the shapes of the sampling distributions?"
  },
  {
    "objectID": "Worksheets/Worksheet_wk3.html",
    "href": "Worksheets/Worksheet_wk3.html",
    "title": "3. Exploratory Data Analysis",
    "section": "",
    "text": "Three important things to remember:"
  },
  {
    "objectID": "Worksheets/Worksheet_wk3.html#step-0-preparing-your-environment",
    "href": "Worksheets/Worksheet_wk3.html#step-0-preparing-your-environment",
    "title": "3. Exploratory Data Analysis",
    "section": "Step 0: Preparing your Environment",
    "text": "Step 0: Preparing your Environment\nFirt things first, open up a new R script and load in the Tidyverse library\n\n\n\n\n\n\nNeed a reminder?\n\n\n\n\n\n\nlibrary(tidyverse)\n\n\n\n\nIn addition, please download the following data files from Moodle (see folder for session 3) and place them in your working directory.\n\nnettle_1999_climate.csv\nlanguage_exams_new.csv\nscores.csv"
  },
  {
    "objectID": "Worksheets/Worksheet_wk3.html#step-1-tibbles",
    "href": "Worksheets/Worksheet_wk3.html#step-1-tibbles",
    "title": "3. Exploratory Data Analysis",
    "section": "Step 1: Tibbles?",
    "text": "Step 1: Tibbles?\nIn the last session, you have learned how to install the tidyverse package (Wickham, 2017).\ntidyverse is a collection of packages that great facilitates data handling in R. In our session on data visualization, you will encounter the ggplot2 package (Wickham, 2016), which is part of tidyverse. Today, we will use functions from other important packages of the tidyverse, namely tibble (Müller & Wickham, 2018), readr (Wickham et al., 2022), and dplyr (Wickham et al., 2018). These are all automatically installed when you install the tidyverse.\nWe have actually used tibbles last week. When we use the function read_csv(), this reads dataframes and gives them a class of tibble as well as dataframe. We can interpret this as meaning we can apply functions or operations that can apply to both classes.\nTibbles are like the data frames but better. For example, they load much faster, which is important when you are dealing with lots of data.\nFirst, let us load in a tibble/dataframe called languages. The data set comes from Winter’s (2019) textbook.\n\nlanguages &lt;- read_csv('nettle_1999_climate.csv')\n\nclass(languages)\n\n\nWhat classes are associated with the object languages? Which one means it has been read in as a tibble?\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n“tbl” is short for tibble!\n\n\n\nRealistically, in future steps/worksheets/discussions, provided we are all on the same page and using read_csv() not read.csv(), then the terms tibble and dataframe are interchangeable.\nFor completeness sake, and so we can see why working with tibbles is easier, let us read the data again using read.csv(), have a look and compare it against the stored langages object.\nFor this, let us just read the csv without saving it to an object, you may recall that in week 1, we played about getting R to output various things, we told it to print 2 + 2 and it did just that. It gave us the number 4 in the output. And notice that when we ran languages &lt;- read_csv('nettle_1999_climate.csv') above, we did not get any output. This is because R did what we asked, which was to assign the values of the file to the object called “languages”. Got it? Cool, we can get rid of the left hand side of the assignment operator and just have R read (and print) the values of a file to us. In practice, this is not a useful exercise, but as an educational step, it’s convenient.\nLet us run the following command again to have it stored in the environment properly, notice the function is read_csv.\n\nlanguages &lt;- read_csv('nettle_1999_climate.csv')\n\nAnd now:\n\nlanguages\n\nWhat’s the difference? As you can see above, the tibble has information about the number of observations (74) and variables (5), the names of the variables: Country, Population, Area, MGS (mean growing season, measured by number of months when crops grow), Langs, and the variable types (character: chr, doubles: dbl, which is a type of numeric vector, and integer: int, also a numeric vector). In addition, the command will display the first ten observations of the variables, lines 1 to 10."
  },
  {
    "objectID": "Worksheets/Worksheet_wk3.html#step-2-data-wrangling",
    "href": "Worksheets/Worksheet_wk3.html#step-2-data-wrangling",
    "title": "3. Exploratory Data Analysis",
    "section": "Step 2: Data wrangling",
    "text": "Step 2: Data wrangling\nWe will now use tidyverse functions for data wrangling. As discussed in our last session, data wrangling is also referred to as data pre-processing or data cleaning. It simply means preparing your raw data (e.g., the data files from experimental software) for statistical analyses. This entails, for example, dealing with missing values, relabeling variables, changing the variable types, etc.\nIn this part of the handout, we will look at a few tidyverse function that you can use for data wrangling. For more information, I recommend Chapter 3 of Andrews (2021), which provides a comprehensive introduction to data wrangling using tidyverse.\nLet’s look at five useful functions for data wrangling with tibbles: filter, select, rename, mutate, and arrange.\n\nFilter\nThe filter() function can be used, unsurprisingly, to filter rows in your tibble. The filter() function takes the input tibble as its first argument. The second argument is then a logical statement that you can use to filter the data as you please.\n\n\n\n\n\n\nNote\n\n\n\nWe are using pipes, as introduced last week. So here’s some more practice with them coming up. filter() takes the input as the first argument, which means we can pipe the tibble into the filter function.\n\n\nIn the following example, we are reducing the languages tibble to only those rows with countries that have more than 500 languages.\n\nlanguages |&gt; #pipe object languages\n  filter(Langs &gt; 500) # filter rows where variable (column) Langs is greater than 500\n\nOr if you are interested in the data from a specific country (say, Angola), you could simply run the following command. This will only display the rows for Angola.\n\nlanguages |&gt; \n  filter(Country == \"Angola\")\n\nWe can even start to get a little bit more adventurous and filter on more than one argument. What about finding countries with more than 500 languages and Population greater than 4?\n\n\n\n\n\n\nWarning\n\n\n\nWait! Before running the next line, think about what we are trying to find, look at the previous code we have run in this section, and decide what the expected output should be.\nHow many rows should be returned? What countries are they going to be?\nThis kind of mental engagement is critical for your own development - we should also have some kind of mental representation of the information we are getting. It won’t always be super specific, but even just being aware of the shape of a tibble, or general description of the information is important - it means we can check for issues quicker.\n\n\n\nlanguages |&gt; \n  filter(Langs &gt; 500, Population &gt; 4)\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nIndonesia. Look at the output for the code below and notice that it is the same code except we filtered it one step further.\n\nlanguages |&gt; #pipe object languages\n  filter(Langs &gt; 500)\n\n\n\n\n\n\nSelect\nIn contrast, you can use the select() function to select specific columns. To do this, simply add the columns you wish to select, separated by commas, as arguments in the function.\n\nlanguages |&gt; \n  select(Langs, Country)\n\nAs you might notice, the select() function can also be used to change the sequence of the columns. (In the original tibble, Country came first, followed by Langs.)\nOn the other hand, if you wish to exclude a column, you can do this by using a minus sign in front of the column in question. The command below will select the four columns Country, Population, Area and MGS, but excluded Langs as requested.\n\nlanguages |&gt; \n  select(-Langs)\n\nYou can also select consecutive columns using :. Let’s take the columns from and including Population, to and including MGS.\n\nlanguages |&gt; \n  select(Population:MGS)\n\nIt is worth noting that getting filter() and select() mixed up is pretty common. I personally do it a lot. A vaguely helpful way to remember is that filteR is for Rows and select is not. Or seleCt is for Columns? Or jsut do what I do, and get it wrong 50% of the time, and then just change it when you get the error!\n\n\nRename\nA third useful function is called rename(). This function can be used to change the name of columns. To do this, you first write the new column (here, Population) followed by an equal sign (=) and the old column name (Pop).\nrename() is useful for when we have really messy taking that comes from an online survey, government statistics, or even a PsychoPy study. We could tidy data in excel before reading into R, but that defeats the point of using R and being a successful data scientist (which is you!) - it reduces the reproduciblity and transparency of your analyses from start to finish.\n\nlanguages &lt;- languages |&gt; #here we are manipulating in place the object languages. We are overwriting the existing object\n  rename(Population = Pop)\n\n\n\n\n\n\n\nWarning\n\n\n\nWoah, woah, woah - what did I just do to the object languages? I assigned the object languages to languages? Not quite. I assigned the entire of the right hand side of the assignment operator to overwrite the same object. So I renamed a column and saved it to the existing object. Analogy: like saving over the top of a word document. Or, to see a basic example in action, make sure you have the environment pane selected in the top right and run the next command, see what shows, and then run the second and see what changed. It should be pretty clear what will happen, but just extrapolate that to tibbles and renaming (we could have also reassigned any of the previous actions we took using filter or select).\n\nx &lt;- 32\n\nx &lt;- 64\n\n\n\n\n\nMutate\nThe mutate() function can be used to change the content of a tibble. For example, you can add an additional column, which in the example below will be the Langs column divided by 100.\n\nlanguages &lt;- languages |&gt; \n  mutate(Langs100 = Langs/100)\n\nhead(languages) # show me the top 6 rows\n\nWe can also use mutate() to change specific values to something else. Look at this example dataset:\n\n#let's create a 2x3 tibble with two participants, who both took part in a study where we tested their working memory, and one was in the control group (1 for yes, 0 for no) and the other was not\ntest_data &lt;- tibble(name = c(\"Jenny\", \"Jonny\"), \n                    score = c(45, 23),\n                    control_group = c(1,0))\n\ntest_data\n\nLooking at the variable control_group it may not be clear to everyone what 1 or 0 means, so let’s use mutate() to update this variable\n\ntest_data_update &lt;- test_data |&gt; \n  mutate(control_group = str_replace(control_group, \"1\",  \"yes\"),\n         control_group = str_replace(control_group, \"0\",  \"no\"))\n\nLooks pretty complicated! Let’s break it down:\n\nThe first line tells us what the new object will be, and what data we are starting with, we are then piping that into:\nmutate() which then tells us we want to update the control_group variable. and we want to apply a new function called str_replace which replaces strings. We are saying “look in the variable control group, and if we find a ‘1’, replace it with the string ‘yes’. We end the second line with a comma telling R we are doing something more\nline three then repeats line two (still inside mutate if you follow the parentheses), and looks for a ‘0’ and replaces it with ‘no’. And we finish there.\n\nCheck out both obhjects, what’s changed?\n\ntest_data\n\ntest_data_update\n\nThe arguments in str_replace() want three things: the column it is checking, the strong it is looking for, and the string to replace it with. What if we wanted 1 to equal “control” and 0 to equal “experiment” instead? Try it out.\nWe will explore mutate() more in later weeks, but know that it is a very useful and powerful tool. It will be a common tool in your data science belt. It can be used in conjunction with other functions to do some really cool things. As a quick teaser, what if we wanted to know the population density per area unit?\n\nlanguages &lt;- languages |&gt; \n  mutate(density = Pop/Area) #We divide the total population by the area\n\nIf you took the time to calculate each row, you will see that for every row mutate() has taken the specific value of Pop and divided it by Area. Neat, that will save us a lot of time! Just know that we can make these even more complex…\n\n\nArrange\nFinally, the arrange() function can be used to order a tibble in ascending or descending order. In the example below, we are use this function to first look at the countries with the smallest number of languages (Cuba, Madagascar, etc.), followed by the countries with the largest numbers of languages (Papua New Guinea, Indonesia, Nigeria, etc.).\n\nlanguages |&gt; \n  arrange(Langs)\n\nor in descending order?\n\nlanguages |&gt; \n  arrange(desc(Langs))\n\n#which is functionally equivalent to:\n#\n#languages |&gt; \n# arrange(-Langs)\n\nBefore we move on, let’s clean up our environment and we are wanting to start with a fresh one for the rest of the worksheet.\nWe can use the function rm() to remove items from the environment pane. We should have two items, x and languages.\n\nrm(x)\nrm(languages)"
  },
  {
    "objectID": "Worksheets/Worksheet_wk3.html#step-3-exploratory-data-analysis",
    "href": "Worksheets/Worksheet_wk3.html#step-3-exploratory-data-analysis",
    "title": "3. Exploratory Data Analysis",
    "section": "Step 3: Exploratory Data Analysis",
    "text": "Step 3: Exploratory Data Analysis\nWe are now ready for some exploratory data analysis in R. First, let’s load the three data files as tibbles. To load the data sets as tibbles, we use the read_csv() function\nOnce you have loaded the data sets and created the new tibbles, it’s good to inspect the data to make sure all was imported properly. This is important before you do any analyses. Remember: “Garbage in, garbage out.”\nYou can use the View(), head(), and str() functions. Personally I suggest using head() to get a first idea.\nWe are now ready to calculate a few summary statistics! We did some of this in prior worksheets, but we will see some more here.\n\nMean\nTo calculate the arithmetic mean, you can use the mean() function.\n\nmean(test_scores$scores)\n\nIn the case of language_exams_new, we have three exams for which we might want to know the mean.\n\nmean(language_exams_new$exam_1)\n\nmean(language_exams_new$exam_2)\n\nmean(language_exams_new$exam_3)\n\nBut rather than calculating the mean for each column separately (exam_1 to exam_3), we can use the colMeans() function to calculate the mean for all columns.\n\ncolMeans(language_exams_new)\n\nNote that for student_id the output is essentially meaningless; it might be worth to filter out this variable (well, unselect the column. We could do this using two pipes:\n\nlanguage_exams_new |&gt; # take the object language_exams_new\n  select(-student_id) |&gt; # deselect the column student_id\n  colMeans() # apply the function colMeans()\n\nIf the above looks really complicated, it isn’t - don’t worry! Let us break it down. Look at the first two lines of code: we take the object language_exams_new and pipe it into the select function and remove the column student_id. We’ve done this before, this isn’t new. We are then seeing another pipe! This means that everything on the LHS of that pipe is shifted into the first spot of the RHS, so we are taken the object minus student_id and applying col_means(). That wasn’t so bad, it’s quite clear when we think about it programmatically.\nConsider that we can take this same approach and keep adding pipes for ever and ever, always doing the same thing: evaluate the first line and the bit between two pipes, then all of the LHS goes through the next pipe into the next function, and then so on, ever building in its pipeline, creating something complex at the end. That’s enough of this for now, we can get more practice later on, but it will help you to structure and read complicated code and make you into the great data scientist that you are.\n\n\nTrimmed Mean\nThe trimmed mean is useful when we want to exclude extreme values. Remember though that any data exclusion needs to be justified and it needs to be described in your report. There is a significant degree of trust that you report everything that you carried out in your analysis transparently.\nCompare the two outputs, with and without the extreme values. The trim argument here deletes the bottom and top 10% of scores.\n\nmean(test_scores$scores)\n\n\nmean(test_scores$scores, trim = 0.1)\n\n\n\nMedian\nThe median is calculated by using the following command.\n\nmedian(test_scores$scores)\n\nmedian (language_exams_new$exam_1)\n\n\n\nStandard Deviation\n\nsd(test_scores$scores)\n\nsd(language_exams_new$exam_1)\n\n\n\nRange\nThe range can provide useful information about our sample data. For example, let’s calculate the age range of participants in language_exams_new.\nThe range() function does not give you the actual range. It only provides the minimum and maximum values.\n\nrange(language_exams_new$age)\n\nTo calculate the range, we can ask R to give us the different between the two values reported by range\n\nrange(language_exams_new$age) |&gt; \n  diff()\n\ndiff() is a new function, and one we probably won’t use too much more, so it doesn’t deserve much space, diff is short for difference, so it calculates the difference between two values. That’s all really, but look how we take one output that wasn’t useful by itself and applied a new function to get something useful!\n\n\n\n\n\n\nNote\n\n\n\nIf you want to know a (not-so) fun fact, before writing this worksheet, I didn’t even know that diff() existed. I guessed it would (which is based off experience, so not helpful for you to know), but I then checked it did what I thought it did by running ?diff (in the console, NOT script) and it confirmed my suspicions. I could also have just googled “R difference between two values” and read the top two results.\n\n\n\n\nQuantiles\nQuantiles can easily be displayed by means of the quantile() function, as you can see in the example below.\n\nquantile(language_exams_new$exam_1)\n\nIn our exam_1 data, each of the quantiles above tells us how many scores are below the given the value. For example, 0% of scores are below 62, meaning that 62 is the lowest score. 50% of scores are below 68 and 50% above 68. (The 50% quantile is the median.) And 100% of scores are below 74, meaning that 74 is the highest score in the data set.\nWe can confirm that this is true by using the range() function, which confirms 62 and 74 as minimum and maximum values. Can you remember (or look back) to see how we would do this?\nSometimes, we want to calculate a specific one, e.g. a percentile. For this, you can add the following arguments to the quantile() function. For example, to display the 10% and 90% quantiles, you would add 0.1 and 0.9 respectively, as in the examples below.\n\nquantile(language_exams_new$exam_1, 0.1)\n\nquantile(language_exams_new$exam_1, 0.9)\n\n\n\nSummary\nThe summary() function is a fast way to get the key summary statistics.\n\nsummary(language_exams_new)\n\nsummary() is actually quite messy, but it is quick and sometimes that’s all we need, just to get a rough idea of the shape of the data.\n\n\nFrequencies\nSometimes it is helpful to observe the frequencies in our sample data. The freq() function is really helpful for this. Note: This function requires the descr package, which we will need to install and then load. Can you remember where we put these two respective commands? One goes in the console, and one at the top of the script.\n\ninstall.packages(\"descr\")\n\nlibrary(descr)\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\ninstall.packages(\"descr\") #in the console because it is temporary\n\nlibrary(descr) #at the top of the script because we need it to show everyone what tools they need\n\n\n\n\nThe following command will create a frequency table. On the left, you will see the score, in the middle the frequency with which the score occurs, and to the right this is express in percentage points. This tells you, for example, that the most frequent score (the mode) is 68, the least frequent score is 62.\n\nfreq(language_exams_new$exam_1, plot = FALSE)\n\n\n\nFrequencies with a twist\nIf you omit the argument plot = FALSE in the freq() function (which just default the argument plot = TRUE, check it out in the help pane and searching for the function name), R will produce both a table and a visual display of your data. Compare the table and the histogram (the graph). Which one is more informative? What are the relative advantages of either?\n\nfreq(language_exams_new$exam_1)\n\nFor now, we can keep the colour grey. Next week we can play with colours properly, but for now - just know you can make some great plots, but for basic exploration, let’s keep them fast and basic because we just need to know the basic shape of the data"
  },
  {
    "objectID": "Worksheets/Worksheet_wk3.html#step-4-our-first-graphic-explorations",
    "href": "Worksheets/Worksheet_wk3.html#step-4-our-first-graphic-explorations",
    "title": "3. Exploratory Data Analysis",
    "section": "Step 4: Our first graphic explorations",
    "text": "Step 4: Our first graphic explorations\nTo conclude, let’s try out a few basic graphics. Next week, we will go into much more detail, but here’s a preview of data visualization. The graphics below are available in the R base package. By and large, this will be your only exposure to base plots, because they aren’t as useful or smart as tidy plots, but for fast visualisation, they’re “just ok”. Don’t get too attached to them, next week we get some real exposure to graphics.\n\nBoxplot\nBoxplots are helpful to inspect the data (Tukey, 1977), as discussed in our lecture today. The following command creates a boxplot, based on exam_1 from the data set language_exams_new.\n\nboxplot(language_exams_new$exam_1)\n\nWhat if we want to compare performance on all three exams (exam_1, exam_2, exam_3) next to each other?\nOne way of doing this (there are other ways) is to first create three new objects exam_1, exam_2, exam_3, and then used the boxplot() function on the three.\nHave a go.\n\nexam_1 &lt;- c(language_exams_new$exam_1) \n\nexam_2 &lt;- c(language_exams_new$exam_2) \n\nexam_3 &lt;- c(language_exams_new$exam_3) \n\nboxplot(exam_1, exam_2, exam_3)\n\nCould you run the above using just one line? What is easier to read? What do you prefer?\n\n\nHistograms\nLast but not least, histograms are helpful ways to visually inspect your data. To create a histogram, simply use the hist() function, as in the following examples.\n\nhist(language_exams_new$exam_3)"
  },
  {
    "objectID": "Worksheets/Worksheet_wk3.html#take-home-task",
    "href": "Worksheets/Worksheet_wk3.html#take-home-task",
    "title": "3. Exploratory Data Analysis",
    "section": "Take home task",
    "text": "Take home task\nNone this week. You’ve worked hard and learnt a lot of complicated aspects of R - your task can be relish in your new found knowledge and be impressed with what you can now do in R.\nNext week we take on graphs, plots, and visualisations!"
  },
  {
    "objectID": "Week5/Live_code_resources/example.html",
    "href": "Week5/Live_code_resources/example.html",
    "title": "Data Wrangling Walkthrough",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.4     ✔ purrr   1.0.2\n✔ tibble  3.2.1     ✔ dplyr   1.1.2\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n# anything else goes here\n\n\n\n\nRead in key datafiles\n\ndata_raw &lt;- read_csv(\"swiss_crime_2022.csv\")\n\nRows: 19 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): crime, crime_type\ndbl (17): male, female, age18_19, age20_24, age25_29, age30_34, age35_39, ag...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCheck out the data\n\nhead(data_raw)\n\n# A tibble: 6 × 19\n  crime     crime_type  male female age18_19 age20_24 age25_29 age30_34 age35_39\n  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Intentio… Severe vi…    82     20        0       16       17       15       13\n2 Grievous… Severe vi…   204      6        9       53       42       32       28\n3 Female g… Severe vi…     0      0        0        0        0        0        0\n4 Hostage … Severe vi…     0      0        0        0        0        0        0\n5 Rape      Severe vi…    71      0        0        7       13       11       12\n6 Violent … Severe vi…     6      0        0        3        1        0        1\n# ℹ 10 more variables: age40_44 &lt;dbl&gt;, age45_49 &lt;dbl&gt;, age50_59 &lt;dbl&gt;,\n#   age60_69 &lt;dbl&gt;, age70plus &lt;dbl&gt;, nat_swiss &lt;dbl&gt;, nat_foreign &lt;dbl&gt;,\n#   foreign_permit &lt;dbl&gt;, foreign_other &lt;dbl&gt;, foreign_unknown &lt;dbl&gt;\n\ncolnames(data_raw)\n\n [1] \"crime\"           \"crime_type\"      \"male\"            \"female\"         \n [5] \"age18_19\"        \"age20_24\"        \"age25_29\"        \"age30_34\"       \n [9] \"age35_39\"        \"age40_44\"        \"age45_49\"        \"age50_59\"       \n[13] \"age60_69\"        \"age70plus\"       \"nat_swiss\"       \"nat_foreign\"    \n[17] \"foreign_permit\"  \"foreign_other\"   \"foreign_unknown\"\n\n\n\n\n\nWhat do we need to keep if we are wanting to visualise the differences in type of crime convicted in the Swiss Adult population?\n\ndata &lt;- data_raw |&gt; \n  select(crime, crime_type, contains(\"age\"))\n\n\n\n\n\ndata_tidy &lt;- data |&gt;\n  group_by(crime_type) |&gt; \n  mutate(age18_19 = sum(age18_19),\n         age20_24 = sum(age20_24),\n         age25_29 = sum(age25_29),\n         age30_34 = sum(age30_34),\n         age35_39 = sum(age35_39),\n         age40_44 = sum(age40_44),\n         age45_49 = sum(age45_49),\n         age50_59 = sum(age50_59),\n         age60_69 = sum(age60_69),\n         age70plus = sum(age70plus)\n         ) |&gt; \n  select(-crime) |&gt; \n  distinct()\n\n# factor the group data\ndata_tidy &lt;- data_tidy |&gt; \n  mutate(crime_type = factor(crime_type, levels = c(\"Moderate violence (threat of violence)\", \n                                                    \"Moderate violence (exercise of violence evt. threat of violence)\",\n                                                    \"Severe violence (exercise of violence)\")))\n\n\n\n\n\ndata_tidy_long &lt;- data_tidy |&gt; \n  pivot_longer(cols = age18_19:age70plus, \n               names_to = \"age_group\", \n               values_to = \"convictions\")\n\n\n\n\n\ndata_tidy_long |&gt; \n  filter(crime_type == \"Severe violence (exercise of violence)\") |&gt; \n  ggplot(aes(age_group, convictions)) +\n  geom_col()\n\n\n\n#try colour first, and then go to fill\ndata_tidy_long |&gt; \n  ggplot(aes(age_group, convictions, fill = crime_type)) +\n  geom_col(position = \"dodge\")\n\n\n\n\nWhat can we see in the dataset (as well as the x-axis) that indicates this is a misleadingly organised dataset?\nThe age ranges are not consistent - so the above plot is misleading, just like the examples in week 4.\n\ndata_tidy_ages &lt;- data_tidy |&gt; mutate(\n  age20_29 = sum(age20_24, age25_29),\n  age30_39 = sum(age30_34, age35_39),\n  age40_49 = sum(age40_44, age45_49),\n  .before = age50_59\n) |&gt; \n  select(-c(age20_24, age25_29, age30_34, age35_39, age40_44, age45_49))\n\ndata_tidy_ages_long &lt;- data_tidy_ages |&gt; \n  pivot_longer(cols = age18_19:age70plus, \n               names_to = \"age_group\", \n               values_to = \"convictions\") \n\ndata_tidy_ages_long |&gt; \n  ggplot(aes(age_group, convictions, fill = crime_type)) +\n  geom_col(position = \"dodge\")\n\n\n\n\n\ndata_tidy_ages_long_sum &lt;- data_tidy_ages_long |&gt; \n  group_by(age_group) |&gt; \n  summarise(total = sum(convictions))\n\ndata_tidy_ages_long_percent &lt;- data_tidy_ages_long |&gt; \n  left_join(data_tidy_ages_long_sum, by = join_by(age_group)) |&gt; \n  mutate(percentage = convictions/total*100)\n\n##sanity check\n#data_tidy_ages_long_percent |&gt; \n#  filter(age_group == \"age18_19\") |&gt; \n#  pull(percentage) |&gt; sum()\n\n\ndata_tidy_ages_long_percent |&gt; \n  ggplot(aes(age_group, percentage, colour = crime_type)) +\n  geom_col(position = \"dodge\")\n\n\n\n\nWhat inferences can we draw from this? What else could we explore?"
  },
  {
    "objectID": "Week5/Live_code_resources/example.html#set-up-environment",
    "href": "Week5/Live_code_resources/example.html#set-up-environment",
    "title": "Data Wrangling Walkthrough",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.4     ✔ purrr   1.0.2\n✔ tibble  3.2.1     ✔ dplyr   1.1.2\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n# anything else goes here"
  },
  {
    "objectID": "Week5/Live_code_resources/example.html#import-data",
    "href": "Week5/Live_code_resources/example.html#import-data",
    "title": "Data Wrangling Walkthrough",
    "section": "",
    "text": "Read in key datafiles\n\ndata_raw &lt;- read_csv(\"swiss_crime_2022.csv\")\n\nRows: 19 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): crime, crime_type\ndbl (17): male, female, age18_19, age20_24, age25_29, age30_34, age35_39, ag...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCheck out the data\n\nhead(data_raw)\n\n# A tibble: 6 × 19\n  crime     crime_type  male female age18_19 age20_24 age25_29 age30_34 age35_39\n  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Intentio… Severe vi…    82     20        0       16       17       15       13\n2 Grievous… Severe vi…   204      6        9       53       42       32       28\n3 Female g… Severe vi…     0      0        0        0        0        0        0\n4 Hostage … Severe vi…     0      0        0        0        0        0        0\n5 Rape      Severe vi…    71      0        0        7       13       11       12\n6 Violent … Severe vi…     6      0        0        3        1        0        1\n# ℹ 10 more variables: age40_44 &lt;dbl&gt;, age45_49 &lt;dbl&gt;, age50_59 &lt;dbl&gt;,\n#   age60_69 &lt;dbl&gt;, age70plus &lt;dbl&gt;, nat_swiss &lt;dbl&gt;, nat_foreign &lt;dbl&gt;,\n#   foreign_permit &lt;dbl&gt;, foreign_other &lt;dbl&gt;, foreign_unknown &lt;dbl&gt;\n\ncolnames(data_raw)\n\n [1] \"crime\"           \"crime_type\"      \"male\"            \"female\"         \n [5] \"age18_19\"        \"age20_24\"        \"age25_29\"        \"age30_34\"       \n [9] \"age35_39\"        \"age40_44\"        \"age45_49\"        \"age50_59\"       \n[13] \"age60_69\"        \"age70plus\"       \"nat_swiss\"       \"nat_foreign\"    \n[17] \"foreign_permit\"  \"foreign_other\"   \"foreign_unknown\""
  },
  {
    "objectID": "Week5/Live_code_resources/example.html#tidying-data-to-minimum-required",
    "href": "Week5/Live_code_resources/example.html#tidying-data-to-minimum-required",
    "title": "Data Wrangling Walkthrough",
    "section": "",
    "text": "What do we need to keep if we are wanting to visualise the differences in type of crime convicted in the Swiss Adult population?\n\ndata &lt;- data_raw |&gt; \n  select(crime, crime_type, contains(\"age\"))"
  },
  {
    "objectID": "Week5/Live_code_resources/example.html#how-do-i-condense-down-all-the-specific-crimes-into-one-value",
    "href": "Week5/Live_code_resources/example.html#how-do-i-condense-down-all-the-specific-crimes-into-one-value",
    "title": "Data Wrangling Walkthrough",
    "section": "",
    "text": "data_tidy &lt;- data |&gt;\n  group_by(crime_type) |&gt; \n  mutate(age18_19 = sum(age18_19),\n         age20_24 = sum(age20_24),\n         age25_29 = sum(age25_29),\n         age30_34 = sum(age30_34),\n         age35_39 = sum(age35_39),\n         age40_44 = sum(age40_44),\n         age45_49 = sum(age45_49),\n         age50_59 = sum(age50_59),\n         age60_69 = sum(age60_69),\n         age70plus = sum(age70plus)\n         ) |&gt; \n  select(-crime) |&gt; \n  distinct()\n\n# factor the group data\ndata_tidy &lt;- data_tidy |&gt; \n  mutate(crime_type = factor(crime_type, levels = c(\"Moderate violence (threat of violence)\", \n                                                    \"Moderate violence (exercise of violence evt. threat of violence)\",\n                                                    \"Severe violence (exercise of violence)\")))"
  },
  {
    "objectID": "Week5/Live_code_resources/example.html#pivot-so-its-ready-for-plotting",
    "href": "Week5/Live_code_resources/example.html#pivot-so-its-ready-for-plotting",
    "title": "Data Wrangling Walkthrough",
    "section": "",
    "text": "data_tidy_long &lt;- data_tidy |&gt; \n  pivot_longer(cols = age18_19:age70plus, \n               names_to = \"age_group\", \n               values_to = \"convictions\")"
  },
  {
    "objectID": "Week5/Live_code_resources/example.html#plotting",
    "href": "Week5/Live_code_resources/example.html#plotting",
    "title": "Data Wrangling Walkthrough",
    "section": "",
    "text": "data_tidy_long |&gt; \n  filter(crime_type == \"Severe violence (exercise of violence)\") |&gt; \n  ggplot(aes(age_group, convictions)) +\n  geom_col()\n\n\n\n#try colour first, and then go to fill\ndata_tidy_long |&gt; \n  ggplot(aes(age_group, convictions, fill = crime_type)) +\n  geom_col(position = \"dodge\")\n\n\n\n\nWhat can we see in the dataset (as well as the x-axis) that indicates this is a misleadingly organised dataset?\nThe age ranges are not consistent - so the above plot is misleading, just like the examples in week 4.\n\ndata_tidy_ages &lt;- data_tidy |&gt; mutate(\n  age20_29 = sum(age20_24, age25_29),\n  age30_39 = sum(age30_34, age35_39),\n  age40_49 = sum(age40_44, age45_49),\n  .before = age50_59\n) |&gt; \n  select(-c(age20_24, age25_29, age30_34, age35_39, age40_44, age45_49))\n\ndata_tidy_ages_long &lt;- data_tidy_ages |&gt; \n  pivot_longer(cols = age18_19:age70plus, \n               names_to = \"age_group\", \n               values_to = \"convictions\") \n\ndata_tidy_ages_long |&gt; \n  ggplot(aes(age_group, convictions, fill = crime_type)) +\n  geom_col(position = \"dodge\")\n\n\n\n\n\ndata_tidy_ages_long_sum &lt;- data_tidy_ages_long |&gt; \n  group_by(age_group) |&gt; \n  summarise(total = sum(convictions))\n\ndata_tidy_ages_long_percent &lt;- data_tidy_ages_long |&gt; \n  left_join(data_tidy_ages_long_sum, by = join_by(age_group)) |&gt; \n  mutate(percentage = convictions/total*100)\n\n##sanity check\n#data_tidy_ages_long_percent |&gt; \n#  filter(age_group == \"age18_19\") |&gt; \n#  pull(percentage) |&gt; sum()\n\n\ndata_tidy_ages_long_percent |&gt; \n  ggplot(aes(age_group, percentage, colour = crime_type)) +\n  geom_col(position = \"dodge\")\n\n\n\n\nWhat inferences can we draw from this? What else could we explore?"
  }
]